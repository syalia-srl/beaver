[
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "This is where the fun begins. Let’s get BeaverDB installed and run your first multi-modal script. You’ll be up and running in about 30 seconds.\n\n\nBeaverDB is a Python library, so you can install it right from your terminal using uv or pip.\nThe Core Install\nThe core beaver-db package includes everything you need for local development, including the CLI, vector search, and all data structures.\n# This includes the core library, vector search, and the CLI\npip install beaver-db\nInstalling Optional Features\nThe only optional feature is the REST API server, which allows you to run BeaverDB as a networked service.\n\nbeaver-db[remote]: Adds the fastapi-based REST server, which you can run with the beaver serve command.\nbeaver-db[full]: A convenience extra that includes remote and all future optional features.\n\n# To install all features, including the REST server\npip install \"beaver-db[full]\"\n\n\n\nYou can also run the BeaverDB REST API server using Docker. This is the recommended way to deploy BeaverDB as a service.\n# Pull the latest image from the GitHub Container Registry\ndocker pull ghcr.io/syalia-srl/beaver:latest\n\n# Run the server, mounting a local directory to persist data\ndocker run -p 8000:8000 \\\n  -v $(pwd)/my-beaver-data:/app/data \\\n  -e \"DATABASE=data/production.db\" \\\n  ghcr.io/syalia-srl/beaver:latest\nThis command:\n\nRuns the server on port 8000.\nMounts a local folder named my-beaver-data into the container.\nTells the server to create its database file at /app/data/production.db (which will persist in your my-beaver-data folder).\n\n\n\n\nLet’s create a single Python script that shows off BeaverDB’s “multi-modal” power.\nCreate a new file named quickstart.py.\n\n\nFirst, import BeaverDB and Document. The BeaverDB class is your main entry point, and Document is the object we’ll use for storing rich data. This line creates a single file, my_data.db, if it doesn’t already exist.\nfrom beaver import BeaverDB, Document\n\n# This creates a single file \"my_data.db\" if it doesn't exist\n# and sets it up for safe, concurrent access.\ndb = BeaverDB(\"my_data.db\")\n\n\n\nNow let’s use a namespaced dictionary. This is perfect for storing app configuration or user settings. We get it by calling db.dict(\"app_config\"). The object it returns behaves just like a standard Python dict.\n# This is perfect for storing app configuration or user settings.\nconfig = db.dict(\"app_config\")\n\n# Assigning a value saves it instantly to the database file.\nconfig[\"theme\"] = \"dark\"\nconfig[\"user_id\"] = 123\n\n# You can read the value back just as easily:\nprint(f\"App theme is: {config['theme']}\")\n\n\n\nNext, let’s store some ordered data. A persistent list (db.list()) is great for a to-do list, a job queue, or a chat history. It supports methods like push, pop, and standard index access.\n# This is great for a to-do list, a job queue, or a chat history.\ntasks = db.list(\"daily_tasks\")\n\n# Use .push() to append items\ntasks.push({\"id\": \"task-001\", \"desc\": \"Write project report\"})\ntasks.push({\"id\": \"task-002\", \"desc\": \"Deploy new feature\"})\n\n# You can access items by index, just like a normal list:\nprint(f\"First task is: {tasks[0]['desc']}\")\n\n\n\nThe “collection” is the most powerful feature. It stores rich Document objects and allows you to search them using vectors, text, or graph relationships.\nLet’s get a collection and create a Document to store. The body field holds all of our text and metadata.\n# This is the most powerful feature, combining data and search.\narticles = db.collection(\"articles\")\n\n# Create a Document to store.\n# We give it a unique ID and some text content in the 'body'.\ndoc = Document(\n    id=\"sqlite-001\",\n    body=\"SQLite is a powerful embedded database ideal for local apps.\"\n)\n\n\n\nNow, we’ll save the document using .index(). By setting fts=True, we tell BeaverDB to also automatically add the text in body to a Full-Text Search index. We’ll add fuzzy=True to tolerate typos.\n# This not only saves the document but also automatically\n# makes its text content searchable via a Full-Text Search (FTS) index\n# with optional fuzzy matching.\narticles.index(doc, fts=True, fuzzy=True)\n\n\n\nFinally, let’s query our collection. We’ll use .match() to perform a text search. Notice the intentional typo in “datbase”. Because we indexed with fuzziness=1, BeaverDB finds the correct document anyway!\n# This isn't a simple string find; it's a real search engine!\n# Note the typo in \"datbase\"\nresults = articles.match(query=\"datbase\", fuzziness=1)\n\n# The result is a list of tuples: (document, score)\ntop_doc, rank = results[0]\nprint(f\"Search found: '{top_doc.body}' (Score: {rank:.2f})\")\nWhen you run your quickstart.py script, you’ll have a single my_data.db file containing your config, your task list, and your searchable articles.\n\n\n\n\nThe beaver CLI is included in the core installation and is a great way to inspect or manage your database from the terminal.\nLet’s interact with the my_data.db file we just created.\n# Get the 'theme' key from the 'app_config' dictionary\n$ beaver --database my_data.db dict app_config get theme\n\"dark\"\n\n# Run the same fuzzy search from our script\n$ beaver --database my_data.db collection articles match \"datbase\" --fuzziness 1\n[\n  {\n    \"document\": {\n      \"id\": \"sqlite-001\",\n      \"embedding\": null,\n      \"body\": \"SQLite is a powerful embedded database ideal for local apps.\"\n    },\n    \"score\": 0.0\n  }\n]\n\n\n\nYou can start a REST API server for your database using the beaver serve command. This allows you to interact with your BeaverDB instance over HTTP, making it easy to build web or mobile applications.\n$ beaver serve --database my_data.db --port 8000\nINFO:     Started server process [12345]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nNow you can access your database via HTTP requests. For example, to get the theme from the app_config dictionary:\n$ curl http://localhost:8000/dict/app_config/theme\n\"dark\"\nThat’s it! You’ve successfully installed BeaverDB, created a multi-modal database, and interacted with it both programmatically and via the CLI and REST API.\nHappy coding!"
  },
  {
    "objectID": "quickstart.html#installation",
    "href": "quickstart.html#installation",
    "title": "Quickstart Guide",
    "section": "",
    "text": "BeaverDB is a Python library, so you can install it right from your terminal using uv or pip.\nThe Core Install\nThe core beaver-db package includes everything you need for local development, including the CLI, vector search, and all data structures.\n# This includes the core library, vector search, and the CLI\npip install beaver-db\nInstalling Optional Features\nThe only optional feature is the REST API server, which allows you to run BeaverDB as a networked service.\n\nbeaver-db[remote]: Adds the fastapi-based REST server, which you can run with the beaver serve command.\nbeaver-db[full]: A convenience extra that includes remote and all future optional features.\n\n# To install all features, including the REST server\npip install \"beaver-db[full]\""
  },
  {
    "objectID": "quickstart.html#docker",
    "href": "quickstart.html#docker",
    "title": "Quickstart Guide",
    "section": "",
    "text": "You can also run the BeaverDB REST API server using Docker. This is the recommended way to deploy BeaverDB as a service.\n# Pull the latest image from the GitHub Container Registry\ndocker pull ghcr.io/syalia-srl/beaver:latest\n\n# Run the server, mounting a local directory to persist data\ndocker run -p 8000:8000 \\\n  -v $(pwd)/my-beaver-data:/app/data \\\n  -e \"DATABASE=data/production.db\" \\\n  ghcr.io/syalia-srl/beaver:latest\nThis command:\n\nRuns the server on port 8000.\nMounts a local folder named my-beaver-data into the container.\nTells the server to create its database file at /app/data/production.db (which will persist in your my-beaver-data folder)."
  },
  {
    "objectID": "quickstart.html#a-step-by-step-example",
    "href": "quickstart.html#a-step-by-step-example",
    "title": "Quickstart Guide",
    "section": "",
    "text": "Let’s create a single Python script that shows off BeaverDB’s “multi-modal” power.\nCreate a new file named quickstart.py.\n\n\nFirst, import BeaverDB and Document. The BeaverDB class is your main entry point, and Document is the object we’ll use for storing rich data. This line creates a single file, my_data.db, if it doesn’t already exist.\nfrom beaver import BeaverDB, Document\n\n# This creates a single file \"my_data.db\" if it doesn't exist\n# and sets it up for safe, concurrent access.\ndb = BeaverDB(\"my_data.db\")\n\n\n\nNow let’s use a namespaced dictionary. This is perfect for storing app configuration or user settings. We get it by calling db.dict(\"app_config\"). The object it returns behaves just like a standard Python dict.\n# This is perfect for storing app configuration or user settings.\nconfig = db.dict(\"app_config\")\n\n# Assigning a value saves it instantly to the database file.\nconfig[\"theme\"] = \"dark\"\nconfig[\"user_id\"] = 123\n\n# You can read the value back just as easily:\nprint(f\"App theme is: {config['theme']}\")\n\n\n\nNext, let’s store some ordered data. A persistent list (db.list()) is great for a to-do list, a job queue, or a chat history. It supports methods like push, pop, and standard index access.\n# This is great for a to-do list, a job queue, or a chat history.\ntasks = db.list(\"daily_tasks\")\n\n# Use .push() to append items\ntasks.push({\"id\": \"task-001\", \"desc\": \"Write project report\"})\ntasks.push({\"id\": \"task-002\", \"desc\": \"Deploy new feature\"})\n\n# You can access items by index, just like a normal list:\nprint(f\"First task is: {tasks[0]['desc']}\")\n\n\n\nThe “collection” is the most powerful feature. It stores rich Document objects and allows you to search them using vectors, text, or graph relationships.\nLet’s get a collection and create a Document to store. The body field holds all of our text and metadata.\n# This is the most powerful feature, combining data and search.\narticles = db.collection(\"articles\")\n\n# Create a Document to store.\n# We give it a unique ID and some text content in the 'body'.\ndoc = Document(\n    id=\"sqlite-001\",\n    body=\"SQLite is a powerful embedded database ideal for local apps.\"\n)\n\n\n\nNow, we’ll save the document using .index(). By setting fts=True, we tell BeaverDB to also automatically add the text in body to a Full-Text Search index. We’ll add fuzzy=True to tolerate typos.\n# This not only saves the document but also automatically\n# makes its text content searchable via a Full-Text Search (FTS) index\n# with optional fuzzy matching.\narticles.index(doc, fts=True, fuzzy=True)\n\n\n\nFinally, let’s query our collection. We’ll use .match() to perform a text search. Notice the intentional typo in “datbase”. Because we indexed with fuzziness=1, BeaverDB finds the correct document anyway!\n# This isn't a simple string find; it's a real search engine!\n# Note the typo in \"datbase\"\nresults = articles.match(query=\"datbase\", fuzziness=1)\n\n# The result is a list of tuples: (document, score)\ntop_doc, rank = results[0]\nprint(f\"Search found: '{top_doc.body}' (Score: {rank:.2f})\")\nWhen you run your quickstart.py script, you’ll have a single my_data.db file containing your config, your task list, and your searchable articles."
  },
  {
    "objectID": "quickstart.html#using-the-cli",
    "href": "quickstart.html#using-the-cli",
    "title": "Quickstart Guide",
    "section": "",
    "text": "The beaver CLI is included in the core installation and is a great way to inspect or manage your database from the terminal.\nLet’s interact with the my_data.db file we just created.\n# Get the 'theme' key from the 'app_config' dictionary\n$ beaver --database my_data.db dict app_config get theme\n\"dark\"\n\n# Run the same fuzzy search from our script\n$ beaver --database my_data.db collection articles match \"datbase\" --fuzziness 1\n[\n  {\n    \"document\": {\n      \"id\": \"sqlite-001\",\n      \"embedding\": null,\n      \"body\": \"SQLite is a powerful embedded database ideal for local apps.\"\n    },\n    \"score\": 0.0\n  }\n]"
  },
  {
    "objectID": "quickstart.html#running-the-rest-api-server",
    "href": "quickstart.html#running-the-rest-api-server",
    "title": "Quickstart Guide",
    "section": "",
    "text": "You can start a REST API server for your database using the beaver serve command. This allows you to interact with your BeaverDB instance over HTTP, making it easy to build web or mobile applications.\n$ beaver serve --database my_data.db --port 8000\nINFO:     Started server process [12345]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nNow you can access your database via HTTP requests. For example, to get the theme from the app_config dictionary:\n$ curl http://localhost:8000/dict/app_config/theme\n\"dark\"\nThat’s it! You’ve successfully installed BeaverDB, created a multi-modal database, and interacted with it both programmatically and via the CLI and REST API.\nHappy coding!"
  },
  {
    "objectID": "guide-collections.html",
    "href": "guide-collections.html",
    "title": "The Document Collection (db.collection)",
    "section": "",
    "text": "The Document Collection (db.collection)\nChapter Outline:\n\n5.1. Documents & Indexing\n\nThe Document class: id, embedding, and metadata.\nIndexing and Upserting: .index(doc) performs an atomic insert-or-replace.\nRemoving data: .drop(doc).\n\n5.2. Vector Search (ANN)\n\nAdding vectors via the Document(embedding=...) field.\nQuerying: .search(vector, top_k=N).\nUse Case: Building a RAG system by combining text and vector search.\nHelper: The rerank() function for hybrid search results.\n\n5.3. Full-Text & Fuzzy Search\n\nFull-Text Search (FTS): .match(query, on=[\"field.path\"]).\nFuzzy Search: .match(query, fuzziness=2) for typo-tolerance.\n\n5.4. Knowledge Graph\n\nCreating relationships: .connect(source, target, label, metadata).\nSingle-hop traversal: .neighbors(doc, label).\nMulti-hop (BFS) traversal: .walk(source, labels, depth, direction)."
  },
  {
    "objectID": "dev-architecture.html",
    "href": "dev-architecture.html",
    "title": "Core Architecture & Design",
    "section": "",
    "text": "Core Architecture & Design\nChapter Outline:\n\n9.1. Guiding Principles (Developer Focus)\n\nA deeper dive into the “Why” from design.md.\nStandard SQLite Compatibility as a “no-magic” rule.\nConvention over Configuration.\n\n9.2. The Manager Delegation Pattern\n\nHow BeaverDB acts as a factory.\nHow managers (e.g., DictManager) are initialized with a reference to the core BeaverDB connection pool.\nHow all tables are prefixed with beaver_ to avoid user-space conflicts.\n\n9.3. Type-Safe Models (beaver.Model)\n\nUsing the model=... parameter for automatic serialization and deserialization.\nInheriting from beaver.Model for a lightweight, Pydantic-compatible solution."
  },
  {
    "objectID": "dev-concurrency.html",
    "href": "dev-concurrency.html",
    "title": "Concurrency Model",
    "section": "",
    "text": "Concurrency Model\nChapter Outline:\n\n10.1. Thread Safety (threading.local)\n\nHow BeaverDB provides a unique sqlite3.Connection for every thread.\nWhy this is the key to preventing thread-related errors.\nEnabling WAL (Write-Ahead Logging) for concurrent reads.\n\n10.2. Inter-Process Locking (The Implementation)\n\nHow db.lock() works under the hood.\nThe beaver_lock_waiters table as a fair (FIFO) queue.\nThe expires_at column as a deadlock-prevention (TTL) mechanism.\n\n10.3. The Asynchronous .as_async() Pattern\n\nHow the Async...Manager wrappers are implemented.\nUsing asyncio.to_thread to run blocking I/O without blocking the event loop."
  },
  {
    "objectID": "guide-lists-queues.html",
    "href": "guide-lists-queues.html",
    "title": "Lists and Queues",
    "section": "",
    "text": "Lists and Queues\nChapter Outline:\n\n4.1. Persistent Lists (db.list)\n\nA full-featured, persistent Python list.\nFull support for: push, pop, prepend, deque, slicing my_list[1:5], and in-place updates my_list[0] = ....\n\n4.2. Priority Queues (db.queue)\n\nCreating a persistent, multi-process task queue.\nAdding tasks: .put(data, priority=N) (lower number is higher priority).\nConsuming tasks: The blocking .get(timeout=N) method.\nUse Case: A multi-process producer/consumer pattern."
  },
  {
    "objectID": "guide.html",
    "href": "guide.html",
    "title": "Feature Summary",
    "section": "",
    "text": "This guide provides a comprehensive set of practical, code-first examples for every major feature of BeaverDB.\n\n\n\n\nUse a namespaced dictionary for storing simple key-value data like application configuration or user profiles.\nFirst, let’s import BeaverDB and get a handle to a namespaced dictionary called \"app_config\".\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\n\n# Get a handle to a namespaced dictionary\nconfig = db.dict(\"app_config\")\nNow you can use the config object just like a standard Python dictionary. Operations are saved to the database file instantly.\n# --- 1. Setting Values ---\nconfig[\"theme\"] = \"dark\"\nconfig[\"retries\"] = 3\n\n# You can also use the .set() method\nconfig.set(\"user_ids\", [101, 205, 301])\n\nprint(f\"Configuration dictionary has {len(config)} items.\")\n\n# --- 2. Retrieving Values ---\n# Use standard dict access\ntheme = config[\"theme\"]\nprint(f\"Retrieved theme: {theme}\")\n\n# Or use .get() with a default value\nnon_existent = config.get(\"non_existent_key\", \"default_value\")\nprint(f\"Result for non_existent_key: {non_existent}\")\nIterating and deleting items also follows standard Python syntax.\n# --- 3. Iterating Over the Dictionary ---\nprint(\"\\nIterating over config items:\")\nfor key, value in config.items():\n    print(f\"  - {key}: {value}\")\n\n# --- 4. Deleting an Item ---\ndel config[\"retries\"]\nprint(f\"\\nAfter deleting 'retries', config has {len(config)} items.\")\n\ndb.close()\n\n\n\nLeverage a dictionary with a Time-To-Live (TTL) to cache the results of slow network requests.\nLet’s define a mock function that simulates a slow API call and get a handle to a dictionary we’ll use as our cache.\nimport time\nfrom beaver import BeaverDB\n\ndef expensive_api_call(prompt: str):\n    \"\"\"A mock function that simulates a slow API call.\"\"\"\n    print(f\"--- Making expensive API call for: '{prompt}' ---\")\n    time.sleep(2)  # Simulate network latency\n    return \"Quito\"\n\ndb = BeaverDB(\"demo.db\")\napi_cache = db.dict(\"api_cache\")\nprompt = \"capital of Ecuador\"\nOn our first attempt, the cache is empty, so we get a “cache miss.” We call our expensive function and then use .set() with ttl_seconds=10 to store the result for 10 seconds.\n# --- 1. First Call (Cache Miss) ---\nprint(\"\\nAttempt 1: Key is not in cache.\")\nresponse = api_cache.get(prompt)\nif response is None:\n    print(\"Cache miss.\")\n    response = expensive_api_call(prompt)\n    # Set the value in the cache with a 10-second TTL\n    api_cache.set(prompt, response, ttl_seconds=10)\n\nprint(f\"Response: {response}\")\nNow, if we request the same data within the 10-second window, we get a “cache hit.” The data is returned instantly from the cache, and our expensive function is never called.\n# --- 2. Second Call (Cache Hit) ---\nprint(\"\\nAttempt 2: Making the same request within 5 seconds.\")\ntime.sleep(5)\nresponse = api_cache.get(prompt)\nif response is None:\n    # ... (this won't be called) ...\n    pass\nelse:\n    print(\"Cache hit!\")\n\nprint(f\"Response: {response}\")\nFinally, we wait 12 seconds, which is longer than our TTL. The cache key has automatically expired. When we request the data, it’s a “cache miss” again, forcing us to re-run the expensive function and update the cache for another 10 seconds.\n# --- 3. Third Call (Cache Expired) ---\nprint(\"\\nAttempt 3: Waiting for 12 seconds for the cache to expire.\")\ntime.sleep(12)\nresponse = api_cache.get(prompt)\nif response is None:\n    print(\"Cache miss (key expired).\")\n    response = expensive_api_call(prompt)\n    api_cache.set(prompt, response, ttl_seconds=10)\nelse:\n    print(\"Cache hit!\")\n\nprint(f\"Response: {response}\")\ndb.close()\n\n\n\nA persistent list is perfect for storing ordered data, like the history of a conversation. It supports a full Python list API.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\ntasks = db.list(\"project_tasks\")\nYou can add items to the beginning or end using push (like append) and prepend.\n# --- 1. Pushing and Prepending Items ---\ntasks.push({\"id\": \"task-002\", \"desc\": \"Write documentation\"})\ntasks.push({\"id\": \"task-003\", \"desc\": \"Deploy to production\"})\ntasks.prepend({\"id\": \"task-001\", \"desc\": \"Design the feature\"})\nAccessing, iterating, and slicing work exactly as you’d expect.\n# --- 2. Iterating Over the List ---\nprint(\"\\nCurrent tasks in order:\")\nfor task in tasks:\n    print(f\"  - {task['id']}: {task['desc']}\")\n\n# --- 3. Accessing and Slicing ---\nprint(f\"\\nThe first task is: {tasks[0]}\")\nprint(f\"The last task is: {tasks[-1]}\")\nprint(f\"A slice of the first two tasks: {tasks[0:2]}\")\nYou can also modify the list in place with standard __setitem__ and __delitem__ syntax, or use stack/queue methods like pop and deque.\n# --- 4. Updating an Item in Place ---\nprint(\"\\nUpdating the second task...\")\ntasks[1] = {\"id\": \"task-002\", \"desc\": \"Write and review documentation\"}\nprint(f\"Updated second task: {tasks[1]}\")\n\n# --- 5. Deleting an Item by Index ---\nprint(\"\\nDeleting the first task ('task-001')...\")\ndel tasks[0]\nprint(f\"List length after deletion: {len(tasks)}\")\n\n# --- 6. Popping the Last Item ---\nlast_item = tasks.pop()\nprint(f\"\\nPopped the last task: {last_item}\")\n\ndb.close()\n\n\n\nUse a persistent priority queue to manage tasks for an AI agent or any worker system. This ensures the most important tasks are always processed first.\nItems are added with a priority (lower numbers are processed first). The .get() method is atomic and blocks until an item is available, making it perfect for worker processes.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\ntasks = db.queue(\"agent_tasks\")\n\n# Tasks are added with a priority (lower is higher)\ntasks.put({\"action\": \"summarize_news\", \"topic\": \"AI\"}, priority=10)\ntasks.put({\"action\": \"respond_to_user\", \"user_id\": \"alice\"}, priority=1)\ntasks.put({\"action\": \"run_backup\", \"target\": \"all\"}, priority=20)\ntasks.put({\"action\": \"send_alert\", \"message\": \"CPU at 90%\"}, priority=1)\n\n# The agent retrieves the highest-priority task\n# This is a blocking call that waits for an item\nitem1 = tasks.get() # -&gt; Returns \"respond_to_user\" (priority 1, added first)\nitem2 = tasks.get() # -&gt; Returns \"send_alert\" (priority 1, added second)\nitem3 = tasks.get() # -&gt; Returns \"summarize_news\" (priority 10)\n\nprint(f\"Agent's first task: {item1.data['action']}\")\nprint(f\"Agent's second task: {item2.data['action']}\")\n\ndb.close()\n\n\n\nUse the blob store to save binary data like user avatars, attachments, or generated reports directly in the database.\nFirst, we’ll get a handle to our blob store and prepare some sample binary data.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\nattachments = db.blobs(\"user_uploads\")\n\n# Create some sample binary data\nfile_content = \"This is the content of a virtual text file.\"\nfile_bytes = file_content.encode(\"utf-8\")\nfile_key = \"emails/user123/attachment_01.txt\"\nNow, we store the data using .put() with a unique key and some JSON metadata.\n# Store a user's avatar with metadata\nattachments.put(\n    key=file_key,\n    data=file_bytes,\n    metadata={\"mimetype\": \"text/plain\", \"sender\": \"alice@example.com\"}\n)\nWe can retrieve the blob using .get(), which returns a Blob object containing the key, data, and metadata.\n# Retrieve it later\nblob = attachments.get(file_key)\nif blob:\n    print(f\"Retrieved Blob: {blob.key}\")\n    print(f\"Metadata: {blob.metadata}\")\n    print(f\"Data (decoded): '{blob.data.decode('utf-8')}'\")\nFinally, we can clean up the blob using .delete().\n# Delete the blob\nattachments.delete(file_key)\nprint(f\"\\nVerified deletion: {file_key not in attachments}\")\n\ndb.close()\n\n\n\n\nThe collection is the most powerful data structure, combining document storage with vector, text, and graph search.\n\n\nThis example shows how to combine keyword and vector search for a simple RAG pipeline.\nfrom beaver import BeaverDB, Document\nfrom beaver.collections import rerank\n\ndb = BeaverDB(\"rag_demo.db\")\narticles = db.collection(\"articles\")\nFirst, we index our documents. Each Document contains text for FTS (in body) and a vector (embedding). We enable FTS by setting fts=True.\n# Index documents with both text and embeddings\ndocs_to_index = [\n    Document(\n        id=\"py-fast\",\n        embedding=[0.1, 0.9, 0.2],  # Vector leans towards \"speed\"\n        body=\"Python is a great language for fast prototyping.\",\n    ),\n    Document(\n        id=\"py-data\",\n        embedding=[0.8, 0.2, 0.9],  # Vector leans towards \"data science\"\n        body=\"The Python ecosystem is ideal for data science.\",\n    ),\n    Document(\n        id=\"js-fast\",\n        embedding=[0.2, 0.8, 0.1],  # Vector similar to \"py-fast\"\n        body=\"JavaScript engines are optimized for fast execution.\",\n    ),\n]\nfor doc in docs_to_index:\n    articles.index(doc, fts=True) # Enable FTS\nNow, we perform two separate searches. One is a keyword search for “python,” and the other is a semantic vector search for a query representing “high-performance code.”\n# 1. Vector Search for \"high-performance code\"\nquery_vector = [0.15, 0.85, 0.15] # A vector close to \"fast\"\nvector_results = [doc for doc, _ in articles.search(vector=query_vector)]\n\n# 2. Full-Text Search for \"python\"\ntext_results = [doc for doc, _ in articles.match(query=\"python\")]\nFinally, we use the rerank helper to merge these two lists. It promotes documents that appear high in both result sets, giving us the most relevant answer.\n# 3. Combine and rerank to get the best context\nbest_context = rerank(text_results, vector_results)\n\nprint(\"--- Final Reranked Results ---\")\nfor doc in best_context:\n    print(f\"  - {doc.id}: {doc.body}\")\n\ndb.close()\n\n\n\nA collection also serves as a graph. We can connect documents to build a network of relationships.\nfrom beaver import BeaverDB, Document\n\ndb = BeaverDB(\"graph_demo.db\")\nnet = db.collection(\"social_network\")\nFirst, we index our documents, which will act as the nodes in our graph.\n# 1. Create Documents (nodes)\nalice = Document(id=\"alice\", body={\"name\": \"Alice\"})\nbob = Document(id=\"bob\", body={\"name\": \"Bob\"})\ncharlie = Document(id=\"charlie\", body={\"name\": \"Charlie\"})\ndiana = Document(id=\"diana\", body={\"name\": \"Diana\"})\nnet.index(alice); net.index(bob); net.index(charlie); net.index(diana)\nNext, we create relationships (or edges) between them using the .connect() method. We can give each connection a label to define the relationship type.\n# 2. Create Edges (relationships)\nnet.connect(alice, bob, label=\"FOLLOWS\")\nnet.connect(alice, charlie, label=\"FOLLOWS\")\nnet.connect(bob, diana, label=\"FOLLOWS\")\nnet.connect(charlie, bob, label=\"FOLLOWS\")\nWe can now query these relationships. .neighbors() finds all nodes one hop away.\n# 3. Find 1-hop neighbors\nprint(\"--- Testing `neighbors` (1-hop) ---\")\nfollowing = net.neighbors(alice, label=\"FOLLOWS\")\nprint(f\"Alice follows: {[p.id for p in following]}\")\nFor multi-hop traversals (like “friends of friends”), we use the .walk() method.\n# 4. Find multi-hop connections (e.g., \"friends of friends\")\nprint(\"\\n--- Testing `walk` (multi-hop) ---\")\nfoaf = net.walk(\n    source=alice,\n    labels=[\"FOLLOWS\"],\n    depth=2,\n)\nprint(f\"Alice's extended network (friends of friends): {[p.id for p in foaf]}\")\n\ndb.close()\n\n\n\n\n\n\nRun multiple scripts in parallel and use db.lock() to coordinate them. This example simulates two scrapers trying to refresh a shared sitemap. The db.lock() ensures that only one process can enter the critical section, preventing a “thundering herd” race condition. The timeout=1 causes other processes to skip rather than wait.\nimport time\nimport os\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"scraper_state.db\")\nscrapers_state = db.dict(\"scraper_state\")\n\nlast_refresh = scrapers_state.get(\"last_sitemap_refresh\", 0)\nif time.time() - last_refresh &gt; 3600: # Only refresh once per hour\n    try:\n        # Try to get a lock, but don't wait long\n        with db.lock(\"refresh_sitemap\", timeout=1):\n            # We got the lock.\n            print(f\"PID {os.getpid()} is refreshing the sitemap...\")\n            scrapers_state[\"sitemap\"] = [\"/page1\", \"/page2\"] # Your fetch_sitemap()\n            scrapers_state[\"last_sitemap_refresh\"] = time.time()\n\n    except TimeoutError:\n        # Another process is already refreshing\n        print(f\"PID {os.getpid()} letting other process handle refresh.\")\n\nsitemap = scrapers_state.get(\"sitemap\")\nprint(f\"PID {os.getpid()} proceeding with sitemap: {sitemap}\")\ndb.close()\n\n\n\nEnsure a worker process can safely pull a batch of items from a queue without another worker interfering, using the built-in manager lock. By wrapping the operation in with db.queue(...).acquire(), we guarantee that this process can pull 10 items from the queue atomically, without another worker process stealing items in the middle of the batch.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"tasks.db\")\ntasks_to_process = []\ntry:\n    # This lock guarantees no other process can access 'agent_tasks'\n    # while this block is running.\n    with db.queue('agent_tasks').acquire(timeout=5) as q:\n        for _ in range(10): # Get a batch of 10\n            item = q.get(block=False)\n            tasks_to_process.append(item.data)\nexcept (TimeoutError, IndexError):\n    # Lock timed out or queue was empty\n    print(\"Could not get 10 items.\")\n    pass\n\n# Now process the batch outside the lock\nprint(f\"Processing batch of {len(tasks_to_process)} items.\")\ndb.close()\n\n\n\nBeaverDB’s pub/sub system allows for real-time, multi-process communication. Here is how two separate processes can communicate.\nIn one process (or thread), a publisher sends a message to a named channel. This is a fast, single INSERT operation.\n# --- In one process or thread (e.g., a monitoring service) ---\n#\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\nsystem_events = db.channel(\"system_events\")\nprint(\"[Publisher] Publishing message...\")\nsystem_events.publish({\"event\": \"user_login\", \"user_id\": \"alice\"})\ndb.close()\nIn another process, a subscriber “listens” to that same channel. The with ...subscribe() block handles registration, and listener.listen() blocks until a message arrives.\n# --- In another process or thread (e.g., a UI updater or logger) ---\n#\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\n# The 'with' block handles the subscription lifecycle.\nwith db.channel(\"system_events\").subscribe() as listener:\n    for message in listener.listen():\n        print(f\"Event received: {message}\")\n        # &gt;&gt; Event received: {'event': 'user_login', 'user_id': 'alice'}\n        break # Exit after one message for this example\ndb.close()\n\n\n\nMonitor your application’s health in real-time. The .live() method provides a continuously updating, aggregated view of your log data, perfect for terminal dashboards.\nFirst, we define an ‘aggregator’ function that takes a list of log entries and returns a single summary dictionary.\nfrom datetime import timedelta\nimport statistics\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"live_log_demo.db\")\nlogs = db.log(\"system_metrics\")\n\ndef summarize(window: list[dict]) -&gt; dict:\n    \"\"\"Aggregator that calculates stats from a window of log data.\"\"\"\n    if not window:\n        return {\"mean\": 0.0, \"count\": 0}\n    values = [log.get(\"value\", 0) for log in window]\n    return {\"mean\": statistics.mean(values), \"count\": len(values)}\n\n# Start a background thread to write logs (see examples/logs.py for full code)\n# ...\nNext, we get the live iterator. We tell it to maintain a 5-second rolling window of data and to compute a new summary every 1 second using our function.\n# Get the live iterator over a 5-second rolling window, updating every 1 sec\nlive_summary = logs.live(\n    window=timedelta(seconds=5),\n    period=timedelta(seconds=1),\n    aggregator=summarize\n)\nFinally, we just iterate over live_summary. This loop will block and yield a new summary object every 1 second, giving us a real-time view of our data.\nprint(\"[Main Thread] Starting live view. Press Ctrl+C to stop.\")\ntry:\n    for summary in live_summary:\n        print(f\"Live Stats (5s window): Count={summary['count']}, Mean={summary['mean']:.2f}\")\n        time.sleep(1) # In a real app, this loop just blocks\nexcept KeyboardInterrupt:\n    print(\"\\nShutting down.\")\nfinally:\n    db.close()\n\n\n\nListen for database changes in real-time. You can subscribe to events on specific managers (e.g., db.dict(\"config\").on(\"set\", ...) to trigger workflows or update UIs).\nFirst, let’s define a callback function that will be triggered when a change occurs.\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"events_demo.db\")\nconfig = db.dict(\"app_config\")\nconfig.clear()\n\ndef on_config_change(payload):\n    \"\"\"This callback is triggered when a key is set or deleted.\"\"\"\n    print(f\"EVENT RECEIVED: Key '{payload['key']}' was changed!\")\nNow, we subscribe to the ‘set’ and ‘del’ events on our config dictionary using the .on() method. This returns a handle that we can use to unsubscribe later.\n# Subscribe to 'set' events on this specific dict\nset_handle = config.on(\"set\", on_config_change)\ndel_handle = config.on(\"del\", on_config_change)\n\n# Give the listener thread time to start\ntime.sleep(0.1)\nWhen we modify the dictionary, the callback is triggered automatically in a background thread.\nprint(\"Setting 'theme'...\")\nconfig[\"theme\"] = \"dark\"  # Triggers the 'on_config_change' callback\ntime.sleep(0.1) # Wait for event to process\n\nprint(\"Deleting 'theme'...\")\ndel config[\"theme\"] # Triggers the 'on_config_change' callback\ntime.sleep(0.1)\nTo stop listening, we call .off() on the handles. Subsequent changes will be silent.\n# Clean up the listeners\nset_handle.off()\ndel_handle.off()\n\nprint(\"Listeners are off. This change will be silent.\")\nconfig[\"theme\"] = \"light\" # No event will be printed\ntime.sleep(0.1)\n\ndb.close()\n\n\n\n\n\n\nBeaverDB has first-class support for Pydantic. By associating a BaseModel with a data structure, you get automatic, recursive (de)serialization and data validation.\nBy passing model=User to the db.dict() factory, BeaverDB will automatically validate data on write and, more importantly, deserialize the stored JSON back into a full User object on read, giving you type safety and editor autocompletion.\nfrom pydantic import BaseModel\nfrom beaver import BeaverDB\n\n# Define your Pydantic model\nclass User(BaseModel):\n    name: str\n    email: str\n    permissions: list[str]\n\ndb = BeaverDB(\"user_data.db\")\n\n# Associate the User model with a dictionary\nusers = db.dict(\"user_profiles\", model=User)\n\n# BeaverDB now handles serialization automatically\nusers[\"alice\"] = User(\n    name=\"Alice\",\n    email=\"alice@example.com\",\n    permissions=[\"read\", \"write\"]\n)\n\n# The retrieved object is a proper, validated User instance\nretrieved_user = users[\"alice\"]\n\n# Your editor will provide autocompletion here\nprint(f\"Retrieved: {retrieved_user.name}\")\nprint(f\"Permissions: {retrieved_user.permissions}\")\n\ndb.close()\n\n\n\nYou can instantly expose your database over a RESTful API and interact with it from the command line.\n1. Start the Server\n# Start the server for your database file\nbeaver serve --database data.db --port 8000\n2. Interact with the API (e.g., from curl)\n# Set a value in the 'app_config' dictionary\ncurl -X PUT [http://127.0.0.1:8000/dicts/app_config/api_key](http://127.0.0.1:8000/dicts/app_config/api_key) \\\n     -H \"Content-Type: application/json\" \\\n     -d '\"your-secret-api-key\"'\n\n# Get the value back\ncurl [http://127.0.0.1:8000/dicts/app_config/api_key](http://127.0.0.1:8000/dicts/app_config/api_key)\n# Output: \"your-secret-api-key\"\n3. Interact with the CLI Client\nThe beaver CLI lets you call any method directly from your terminal.\n# Get a value from a dictionary\nbeaver --database data.db dict app_config get theme\n\n# Set a value (JSON is automatically parsed)\nbeaver --database data.db dict app_config set user '{\"name\": \"Alice\", \"id\": 123}'\n\n# Push an item to a list\nbeaver --database data.db list daily_tasks push \"Review PRs\"\n\n# Run a script protected by a distributed lock\nbeaver --database data.db lock my-cron-job run bash -c 'run_daily_report.sh'\n\n\n\nAll data structures support a .dump() method for easy backups and migration to a JSON file.\nimport json\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"my_app.db\")\nconfig = db.dict(\"app_config\")\nconfig[\"theme\"] = \"dark\"\nconfig[\"user_id\"] = 456\n\n# Dump the dictionary's contents to a JSON file\nwith open(\"config_backup.json\", \"w\") as f:\n    config.dump(f)\n\n# You can also get the dump as a Python object\ndump_data = config.dump()\nprint(dump_data['metadata'])\n\ndb.close()\nYou can also use the CLI to dump data:\nbeaver --database data.db collection my_documents dump &gt; my_documents.json\n\n\n\n\nKeep exploring BeaverDB with these in-depth guides:\n\nKey-Value Dictionaries and Blob Storage\nPersistent Lists and Priority Queues\nDocument Collections (Vectors, FTS, Graphs)\nReal-Time Data (Pub/Sub and Live Logs)\nConcurrency and Inter-Process Locking\nDeployment with the REST Server and CLI"
  },
  {
    "objectID": "guide.html#core-data-structures",
    "href": "guide.html#core-data-structures",
    "title": "Feature Summary",
    "section": "",
    "text": "Use a namespaced dictionary for storing simple key-value data like application configuration or user profiles.\nFirst, let’s import BeaverDB and get a handle to a namespaced dictionary called \"app_config\".\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\n\n# Get a handle to a namespaced dictionary\nconfig = db.dict(\"app_config\")\nNow you can use the config object just like a standard Python dictionary. Operations are saved to the database file instantly.\n# --- 1. Setting Values ---\nconfig[\"theme\"] = \"dark\"\nconfig[\"retries\"] = 3\n\n# You can also use the .set() method\nconfig.set(\"user_ids\", [101, 205, 301])\n\nprint(f\"Configuration dictionary has {len(config)} items.\")\n\n# --- 2. Retrieving Values ---\n# Use standard dict access\ntheme = config[\"theme\"]\nprint(f\"Retrieved theme: {theme}\")\n\n# Or use .get() with a default value\nnon_existent = config.get(\"non_existent_key\", \"default_value\")\nprint(f\"Result for non_existent_key: {non_existent}\")\nIterating and deleting items also follows standard Python syntax.\n# --- 3. Iterating Over the Dictionary ---\nprint(\"\\nIterating over config items:\")\nfor key, value in config.items():\n    print(f\"  - {key}: {value}\")\n\n# --- 4. Deleting an Item ---\ndel config[\"retries\"]\nprint(f\"\\nAfter deleting 'retries', config has {len(config)} items.\")\n\ndb.close()\n\n\n\nLeverage a dictionary with a Time-To-Live (TTL) to cache the results of slow network requests.\nLet’s define a mock function that simulates a slow API call and get a handle to a dictionary we’ll use as our cache.\nimport time\nfrom beaver import BeaverDB\n\ndef expensive_api_call(prompt: str):\n    \"\"\"A mock function that simulates a slow API call.\"\"\"\n    print(f\"--- Making expensive API call for: '{prompt}' ---\")\n    time.sleep(2)  # Simulate network latency\n    return \"Quito\"\n\ndb = BeaverDB(\"demo.db\")\napi_cache = db.dict(\"api_cache\")\nprompt = \"capital of Ecuador\"\nOn our first attempt, the cache is empty, so we get a “cache miss.” We call our expensive function and then use .set() with ttl_seconds=10 to store the result for 10 seconds.\n# --- 1. First Call (Cache Miss) ---\nprint(\"\\nAttempt 1: Key is not in cache.\")\nresponse = api_cache.get(prompt)\nif response is None:\n    print(\"Cache miss.\")\n    response = expensive_api_call(prompt)\n    # Set the value in the cache with a 10-second TTL\n    api_cache.set(prompt, response, ttl_seconds=10)\n\nprint(f\"Response: {response}\")\nNow, if we request the same data within the 10-second window, we get a “cache hit.” The data is returned instantly from the cache, and our expensive function is never called.\n# --- 2. Second Call (Cache Hit) ---\nprint(\"\\nAttempt 2: Making the same request within 5 seconds.\")\ntime.sleep(5)\nresponse = api_cache.get(prompt)\nif response is None:\n    # ... (this won't be called) ...\n    pass\nelse:\n    print(\"Cache hit!\")\n\nprint(f\"Response: {response}\")\nFinally, we wait 12 seconds, which is longer than our TTL. The cache key has automatically expired. When we request the data, it’s a “cache miss” again, forcing us to re-run the expensive function and update the cache for another 10 seconds.\n# --- 3. Third Call (Cache Expired) ---\nprint(\"\\nAttempt 3: Waiting for 12 seconds for the cache to expire.\")\ntime.sleep(12)\nresponse = api_cache.get(prompt)\nif response is None:\n    print(\"Cache miss (key expired).\")\n    response = expensive_api_call(prompt)\n    api_cache.set(prompt, response, ttl_seconds=10)\nelse:\n    print(\"Cache hit!\")\n\nprint(f\"Response: {response}\")\ndb.close()\n\n\n\nA persistent list is perfect for storing ordered data, like the history of a conversation. It supports a full Python list API.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\ntasks = db.list(\"project_tasks\")\nYou can add items to the beginning or end using push (like append) and prepend.\n# --- 1. Pushing and Prepending Items ---\ntasks.push({\"id\": \"task-002\", \"desc\": \"Write documentation\"})\ntasks.push({\"id\": \"task-003\", \"desc\": \"Deploy to production\"})\ntasks.prepend({\"id\": \"task-001\", \"desc\": \"Design the feature\"})\nAccessing, iterating, and slicing work exactly as you’d expect.\n# --- 2. Iterating Over the List ---\nprint(\"\\nCurrent tasks in order:\")\nfor task in tasks:\n    print(f\"  - {task['id']}: {task['desc']}\")\n\n# --- 3. Accessing and Slicing ---\nprint(f\"\\nThe first task is: {tasks[0]}\")\nprint(f\"The last task is: {tasks[-1]}\")\nprint(f\"A slice of the first two tasks: {tasks[0:2]}\")\nYou can also modify the list in place with standard __setitem__ and __delitem__ syntax, or use stack/queue methods like pop and deque.\n# --- 4. Updating an Item in Place ---\nprint(\"\\nUpdating the second task...\")\ntasks[1] = {\"id\": \"task-002\", \"desc\": \"Write and review documentation\"}\nprint(f\"Updated second task: {tasks[1]}\")\n\n# --- 5. Deleting an Item by Index ---\nprint(\"\\nDeleting the first task ('task-001')...\")\ndel tasks[0]\nprint(f\"List length after deletion: {len(tasks)}\")\n\n# --- 6. Popping the Last Item ---\nlast_item = tasks.pop()\nprint(f\"\\nPopped the last task: {last_item}\")\n\ndb.close()\n\n\n\nUse a persistent priority queue to manage tasks for an AI agent or any worker system. This ensures the most important tasks are always processed first.\nItems are added with a priority (lower numbers are processed first). The .get() method is atomic and blocks until an item is available, making it perfect for worker processes.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\ntasks = db.queue(\"agent_tasks\")\n\n# Tasks are added with a priority (lower is higher)\ntasks.put({\"action\": \"summarize_news\", \"topic\": \"AI\"}, priority=10)\ntasks.put({\"action\": \"respond_to_user\", \"user_id\": \"alice\"}, priority=1)\ntasks.put({\"action\": \"run_backup\", \"target\": \"all\"}, priority=20)\ntasks.put({\"action\": \"send_alert\", \"message\": \"CPU at 90%\"}, priority=1)\n\n# The agent retrieves the highest-priority task\n# This is a blocking call that waits for an item\nitem1 = tasks.get() # -&gt; Returns \"respond_to_user\" (priority 1, added first)\nitem2 = tasks.get() # -&gt; Returns \"send_alert\" (priority 1, added second)\nitem3 = tasks.get() # -&gt; Returns \"summarize_news\" (priority 10)\n\nprint(f\"Agent's first task: {item1.data['action']}\")\nprint(f\"Agent's second task: {item2.data['action']}\")\n\ndb.close()\n\n\n\nUse the blob store to save binary data like user avatars, attachments, or generated reports directly in the database.\nFirst, we’ll get a handle to our blob store and prepare some sample binary data.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\nattachments = db.blobs(\"user_uploads\")\n\n# Create some sample binary data\nfile_content = \"This is the content of a virtual text file.\"\nfile_bytes = file_content.encode(\"utf-8\")\nfile_key = \"emails/user123/attachment_01.txt\"\nNow, we store the data using .put() with a unique key and some JSON metadata.\n# Store a user's avatar with metadata\nattachments.put(\n    key=file_key,\n    data=file_bytes,\n    metadata={\"mimetype\": \"text/plain\", \"sender\": \"alice@example.com\"}\n)\nWe can retrieve the blob using .get(), which returns a Blob object containing the key, data, and metadata.\n# Retrieve it later\nblob = attachments.get(file_key)\nif blob:\n    print(f\"Retrieved Blob: {blob.key}\")\n    print(f\"Metadata: {blob.metadata}\")\n    print(f\"Data (decoded): '{blob.data.decode('utf-8')}'\")\nFinally, we can clean up the blob using .delete().\n# Delete the blob\nattachments.delete(file_key)\nprint(f\"\\nVerified deletion: {file_key not in attachments}\")\n\ndb.close()"
  },
  {
    "objectID": "guide.html#the-document-collection-db.collection",
    "href": "guide.html#the-document-collection-db.collection",
    "title": "Feature Summary",
    "section": "",
    "text": "The collection is the most powerful data structure, combining document storage with vector, text, and graph search.\n\n\nThis example shows how to combine keyword and vector search for a simple RAG pipeline.\nfrom beaver import BeaverDB, Document\nfrom beaver.collections import rerank\n\ndb = BeaverDB(\"rag_demo.db\")\narticles = db.collection(\"articles\")\nFirst, we index our documents. Each Document contains text for FTS (in body) and a vector (embedding). We enable FTS by setting fts=True.\n# Index documents with both text and embeddings\ndocs_to_index = [\n    Document(\n        id=\"py-fast\",\n        embedding=[0.1, 0.9, 0.2],  # Vector leans towards \"speed\"\n        body=\"Python is a great language for fast prototyping.\",\n    ),\n    Document(\n        id=\"py-data\",\n        embedding=[0.8, 0.2, 0.9],  # Vector leans towards \"data science\"\n        body=\"The Python ecosystem is ideal for data science.\",\n    ),\n    Document(\n        id=\"js-fast\",\n        embedding=[0.2, 0.8, 0.1],  # Vector similar to \"py-fast\"\n        body=\"JavaScript engines are optimized for fast execution.\",\n    ),\n]\nfor doc in docs_to_index:\n    articles.index(doc, fts=True) # Enable FTS\nNow, we perform two separate searches. One is a keyword search for “python,” and the other is a semantic vector search for a query representing “high-performance code.”\n# 1. Vector Search for \"high-performance code\"\nquery_vector = [0.15, 0.85, 0.15] # A vector close to \"fast\"\nvector_results = [doc for doc, _ in articles.search(vector=query_vector)]\n\n# 2. Full-Text Search for \"python\"\ntext_results = [doc for doc, _ in articles.match(query=\"python\")]\nFinally, we use the rerank helper to merge these two lists. It promotes documents that appear high in both result sets, giving us the most relevant answer.\n# 3. Combine and rerank to get the best context\nbest_context = rerank(text_results, vector_results)\n\nprint(\"--- Final Reranked Results ---\")\nfor doc in best_context:\n    print(f\"  - {doc.id}: {doc.body}\")\n\ndb.close()\n\n\n\nA collection also serves as a graph. We can connect documents to build a network of relationships.\nfrom beaver import BeaverDB, Document\n\ndb = BeaverDB(\"graph_demo.db\")\nnet = db.collection(\"social_network\")\nFirst, we index our documents, which will act as the nodes in our graph.\n# 1. Create Documents (nodes)\nalice = Document(id=\"alice\", body={\"name\": \"Alice\"})\nbob = Document(id=\"bob\", body={\"name\": \"Bob\"})\ncharlie = Document(id=\"charlie\", body={\"name\": \"Charlie\"})\ndiana = Document(id=\"diana\", body={\"name\": \"Diana\"})\nnet.index(alice); net.index(bob); net.index(charlie); net.index(diana)\nNext, we create relationships (or edges) between them using the .connect() method. We can give each connection a label to define the relationship type.\n# 2. Create Edges (relationships)\nnet.connect(alice, bob, label=\"FOLLOWS\")\nnet.connect(alice, charlie, label=\"FOLLOWS\")\nnet.connect(bob, diana, label=\"FOLLOWS\")\nnet.connect(charlie, bob, label=\"FOLLOWS\")\nWe can now query these relationships. .neighbors() finds all nodes one hop away.\n# 3. Find 1-hop neighbors\nprint(\"--- Testing `neighbors` (1-hop) ---\")\nfollowing = net.neighbors(alice, label=\"FOLLOWS\")\nprint(f\"Alice follows: {[p.id for p in following]}\")\nFor multi-hop traversals (like “friends of friends”), we use the .walk() method.\n# 4. Find multi-hop connections (e.g., \"friends of friends\")\nprint(\"\\n--- Testing `walk` (multi-hop) ---\")\nfoaf = net.walk(\n    source=alice,\n    labels=[\"FOLLOWS\"],\n    depth=2,\n)\nprint(f\"Alice's extended network (friends of friends): {[p.id for p in foaf]}\")\n\ndb.close()"
  },
  {
    "objectID": "guide.html#real-time-concurrency",
    "href": "guide.html#real-time-concurrency",
    "title": "Feature Summary",
    "section": "",
    "text": "Run multiple scripts in parallel and use db.lock() to coordinate them. This example simulates two scrapers trying to refresh a shared sitemap. The db.lock() ensures that only one process can enter the critical section, preventing a “thundering herd” race condition. The timeout=1 causes other processes to skip rather than wait.\nimport time\nimport os\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"scraper_state.db\")\nscrapers_state = db.dict(\"scraper_state\")\n\nlast_refresh = scrapers_state.get(\"last_sitemap_refresh\", 0)\nif time.time() - last_refresh &gt; 3600: # Only refresh once per hour\n    try:\n        # Try to get a lock, but don't wait long\n        with db.lock(\"refresh_sitemap\", timeout=1):\n            # We got the lock.\n            print(f\"PID {os.getpid()} is refreshing the sitemap...\")\n            scrapers_state[\"sitemap\"] = [\"/page1\", \"/page2\"] # Your fetch_sitemap()\n            scrapers_state[\"last_sitemap_refresh\"] = time.time()\n\n    except TimeoutError:\n        # Another process is already refreshing\n        print(f\"PID {os.getpid()} letting other process handle refresh.\")\n\nsitemap = scrapers_state.get(\"sitemap\")\nprint(f\"PID {os.getpid()} proceeding with sitemap: {sitemap}\")\ndb.close()\n\n\n\nEnsure a worker process can safely pull a batch of items from a queue without another worker interfering, using the built-in manager lock. By wrapping the operation in with db.queue(...).acquire(), we guarantee that this process can pull 10 items from the queue atomically, without another worker process stealing items in the middle of the batch.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"tasks.db\")\ntasks_to_process = []\ntry:\n    # This lock guarantees no other process can access 'agent_tasks'\n    # while this block is running.\n    with db.queue('agent_tasks').acquire(timeout=5) as q:\n        for _ in range(10): # Get a batch of 10\n            item = q.get(block=False)\n            tasks_to_process.append(item.data)\nexcept (TimeoutError, IndexError):\n    # Lock timed out or queue was empty\n    print(\"Could not get 10 items.\")\n    pass\n\n# Now process the batch outside the lock\nprint(f\"Processing batch of {len(tasks_to_process)} items.\")\ndb.close()\n\n\n\nBeaverDB’s pub/sub system allows for real-time, multi-process communication. Here is how two separate processes can communicate.\nIn one process (or thread), a publisher sends a message to a named channel. This is a fast, single INSERT operation.\n# --- In one process or thread (e.g., a monitoring service) ---\n#\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\nsystem_events = db.channel(\"system_events\")\nprint(\"[Publisher] Publishing message...\")\nsystem_events.publish({\"event\": \"user_login\", \"user_id\": \"alice\"})\ndb.close()\nIn another process, a subscriber “listens” to that same channel. The with ...subscribe() block handles registration, and listener.listen() blocks until a message arrives.\n# --- In another process or thread (e.g., a UI updater or logger) ---\n#\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\n# The 'with' block handles the subscription lifecycle.\nwith db.channel(\"system_events\").subscribe() as listener:\n    for message in listener.listen():\n        print(f\"Event received: {message}\")\n        # &gt;&gt; Event received: {'event': 'user_login', 'user_id': 'alice'}\n        break # Exit after one message for this example\ndb.close()\n\n\n\nMonitor your application’s health in real-time. The .live() method provides a continuously updating, aggregated view of your log data, perfect for terminal dashboards.\nFirst, we define an ‘aggregator’ function that takes a list of log entries and returns a single summary dictionary.\nfrom datetime import timedelta\nimport statistics\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"live_log_demo.db\")\nlogs = db.log(\"system_metrics\")\n\ndef summarize(window: list[dict]) -&gt; dict:\n    \"\"\"Aggregator that calculates stats from a window of log data.\"\"\"\n    if not window:\n        return {\"mean\": 0.0, \"count\": 0}\n    values = [log.get(\"value\", 0) for log in window]\n    return {\"mean\": statistics.mean(values), \"count\": len(values)}\n\n# Start a background thread to write logs (see examples/logs.py for full code)\n# ...\nNext, we get the live iterator. We tell it to maintain a 5-second rolling window of data and to compute a new summary every 1 second using our function.\n# Get the live iterator over a 5-second rolling window, updating every 1 sec\nlive_summary = logs.live(\n    window=timedelta(seconds=5),\n    period=timedelta(seconds=1),\n    aggregator=summarize\n)\nFinally, we just iterate over live_summary. This loop will block and yield a new summary object every 1 second, giving us a real-time view of our data.\nprint(\"[Main Thread] Starting live view. Press Ctrl+C to stop.\")\ntry:\n    for summary in live_summary:\n        print(f\"Live Stats (5s window): Count={summary['count']}, Mean={summary['mean']:.2f}\")\n        time.sleep(1) # In a real app, this loop just blocks\nexcept KeyboardInterrupt:\n    print(\"\\nShutting down.\")\nfinally:\n    db.close()\n\n\n\nListen for database changes in real-time. You can subscribe to events on specific managers (e.g., db.dict(\"config\").on(\"set\", ...) to trigger workflows or update UIs).\nFirst, let’s define a callback function that will be triggered when a change occurs.\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"events_demo.db\")\nconfig = db.dict(\"app_config\")\nconfig.clear()\n\ndef on_config_change(payload):\n    \"\"\"This callback is triggered when a key is set or deleted.\"\"\"\n    print(f\"EVENT RECEIVED: Key '{payload['key']}' was changed!\")\nNow, we subscribe to the ‘set’ and ‘del’ events on our config dictionary using the .on() method. This returns a handle that we can use to unsubscribe later.\n# Subscribe to 'set' events on this specific dict\nset_handle = config.on(\"set\", on_config_change)\ndel_handle = config.on(\"del\", on_config_change)\n\n# Give the listener thread time to start\ntime.sleep(0.1)\nWhen we modify the dictionary, the callback is triggered automatically in a background thread.\nprint(\"Setting 'theme'...\")\nconfig[\"theme\"] = \"dark\"  # Triggers the 'on_config_change' callback\ntime.sleep(0.1) # Wait for event to process\n\nprint(\"Deleting 'theme'...\")\ndel config[\"theme\"] # Triggers the 'on_config_change' callback\ntime.sleep(0.1)\nTo stop listening, we call .off() on the handles. Subsequent changes will be silent.\n# Clean up the listeners\nset_handle.off()\ndel_handle.off()\n\nprint(\"Listeners are off. This change will be silent.\")\nconfig[\"theme\"] = \"light\" # No event will be printed\ntime.sleep(0.1)\n\ndb.close()"
  },
  {
    "objectID": "guide.html#advanced-features",
    "href": "guide.html#advanced-features",
    "title": "Feature Summary",
    "section": "",
    "text": "BeaverDB has first-class support for Pydantic. By associating a BaseModel with a data structure, you get automatic, recursive (de)serialization and data validation.\nBy passing model=User to the db.dict() factory, BeaverDB will automatically validate data on write and, more importantly, deserialize the stored JSON back into a full User object on read, giving you type safety and editor autocompletion.\nfrom pydantic import BaseModel\nfrom beaver import BeaverDB\n\n# Define your Pydantic model\nclass User(BaseModel):\n    name: str\n    email: str\n    permissions: list[str]\n\ndb = BeaverDB(\"user_data.db\")\n\n# Associate the User model with a dictionary\nusers = db.dict(\"user_profiles\", model=User)\n\n# BeaverDB now handles serialization automatically\nusers[\"alice\"] = User(\n    name=\"Alice\",\n    email=\"alice@example.com\",\n    permissions=[\"read\", \"write\"]\n)\n\n# The retrieved object is a proper, validated User instance\nretrieved_user = users[\"alice\"]\n\n# Your editor will provide autocompletion here\nprint(f\"Retrieved: {retrieved_user.name}\")\nprint(f\"Permissions: {retrieved_user.permissions}\")\n\ndb.close()\n\n\n\nYou can instantly expose your database over a RESTful API and interact with it from the command line.\n1. Start the Server\n# Start the server for your database file\nbeaver serve --database data.db --port 8000\n2. Interact with the API (e.g., from curl)\n# Set a value in the 'app_config' dictionary\ncurl -X PUT [http://127.0.0.1:8000/dicts/app_config/api_key](http://127.0.0.1:8000/dicts/app_config/api_key) \\\n     -H \"Content-Type: application/json\" \\\n     -d '\"your-secret-api-key\"'\n\n# Get the value back\ncurl [http://127.0.0.1:8000/dicts/app_config/api_key](http://127.0.0.1:8000/dicts/app_config/api_key)\n# Output: \"your-secret-api-key\"\n3. Interact with the CLI Client\nThe beaver CLI lets you call any method directly from your terminal.\n# Get a value from a dictionary\nbeaver --database data.db dict app_config get theme\n\n# Set a value (JSON is automatically parsed)\nbeaver --database data.db dict app_config set user '{\"name\": \"Alice\", \"id\": 123}'\n\n# Push an item to a list\nbeaver --database data.db list daily_tasks push \"Review PRs\"\n\n# Run a script protected by a distributed lock\nbeaver --database data.db lock my-cron-job run bash -c 'run_daily_report.sh'\n\n\n\nAll data structures support a .dump() method for easy backups and migration to a JSON file.\nimport json\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"my_app.db\")\nconfig = db.dict(\"app_config\")\nconfig[\"theme\"] = \"dark\"\nconfig[\"user_id\"] = 456\n\n# Dump the dictionary's contents to a JSON file\nwith open(\"config_backup.json\", \"w\") as f:\n    config.dump(f)\n\n# You can also get the dump as a Python object\ndump_data = config.dump()\nprint(dump_data['metadata'])\n\ndb.close()\nYou can also use the CLI to dump data:\nbeaver --database data.db collection my_documents dump &gt; my_documents.json"
  },
  {
    "objectID": "guide.html#further-reading",
    "href": "guide.html#further-reading",
    "title": "Feature Summary",
    "section": "",
    "text": "Keep exploring BeaverDB with these in-depth guides:\n\nKey-Value Dictionaries and Blob Storage\nPersistent Lists and Priority Queues\nDocument Collections (Vectors, FTS, Graphs)\nReal-Time Data (Pub/Sub and Live Logs)\nConcurrency and Inter-Process Locking\nDeployment with the REST Server and CLI"
  },
  {
    "objectID": "dev-search.html",
    "href": "dev-search.html",
    "title": "Search Architecture",
    "section": "",
    "text": "Search Architecture\nChapter Outline:\n\n11.1. Vector Search (ANN) Internals\n\nThe “Hybrid Index System”: Base Index and Delta Index.\nCrash-Safe Logging: How additions and deletions are written to SQLite logs (_beaver_ann_... tables).\nBackground Compaction: The compact() process.\n\n11.2. Text Search (FTS & Fuzzy) Internals\n\nFTS: How beaver_fts_index is a fts5 virtual table.\nFuzzy Search: How BeaverDB builds a custom trigram index (beaver_trigrams table) and uses Levenshtein distance."
  },
  {
    "objectID": "dev-contributing.html",
    "href": "dev-contributing.html",
    "title": "Future Roadmap & Contributing",
    "section": "",
    "text": "Future Roadmap & Contributing\nChapter Outline:\n\n12.1. The Future of BeaverDB\n\nThe BeaverClient as a drop-in network client.\nReplacing faiss with a simpler, pure-numpy linear search.\n\n12.2. How to Contribute\n\n(Standard contribution guidelines, linking to Makefile, etc.)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to BeaverDB!\nIf you’ve ever found yourself building a Python application that needs to save some data, but setting up a full-blown database server felt like massive overkill, you’re in the right place. BeaverDB is designed to be the “Swiss Army knife” for embedded Python data. It’s built for those exact “just right” scenarios: more powerful than a simple pickle file, but far less complex than a networked server like PostgreSQL or MySQL.\n\n\nAt its heart, BeaverDB is a multi-modal database in a single SQLite file.\nIt’s a Python library that lets you manage modern, complex data types without ever leaving the comfort of a local file. The name itself tells the story:\nB.E.A.V.E.R. stands for Backend for mbedded, All-in-one Vector, Entity, and Relationship storage.\nThis means that inside that one .db file, you can seamlessly store and query:\n\nKey-Value Pairs (like a dictionary for your app’s configuration)\nLists (like a persistent to-do list)\nVector Embeddings (for AI and semantic search)\nDocuments & Text (for full-text search)\nGraph Relationships (to connect your data together)\n…and much more.\n\nAll this power comes from building on top of the world’s most deployed database engine: SQLite.\n\n\n\nBeaverDB is built on a few core principles. Understanding these will help you know when and why to choose it for your project.\nRobust, Safe, and Durable\nYour data should be safe, period. BeaverDB is built to be resilient. Thanks to SQLite’s atomic transactions and Write-Ahead Logging (WAL) mode, your database is crash-safe. If your program crashes mid-operation, your data is never lost or corrupted; the database simply rolls back the incomplete transaction.\nFurthermore, it’s designed for concurrency. It’s both thread-safe (different threads can share one BeaverDB object) and process-safe (multiple, independent Python scripts can read from and write to the same database file at the same time). For tasks that require true coordination, it even provides a simple, built-in distributed lock.\nPerformant by Default\nBeaverDB is fast. It’s not just “fast for a small database”–it’s genuinely fast for the vast majority of medium-sized projects. Because it’s an embedded library, there is zero network latency for any query.\nLet’s be clear: if you’re building the next X (formerly Twitter) and need to handle millions of documents and thousands of queries per second, you’ll need a distributed, networked database. But for almost everything else? BeaverDB is more than fast enough. If your project is in the thousand to tens-of-thousands of documents range, you’ll find it’s incredibly responsive.\nLocal-First & Embedded\nThe default, primary way to use BeaverDB is as a single file right next to your code. This means your entire database—users, vectors, chat logs, and all—is contained in one portable file (e.g., my_app.db). You can copy it, email it, or back it up. This “local-first” approach is what makes it so fast and simple to deploy.\nMinimal & Optional Dependencies\nThe core BeaverDB library has zero external dependencies. You can get started with key-value stores, lists, and queues right away.\nWant to add vector search? Great! Install the [vector] extra, and BeaverDB will activate its faiss integration. Need a web server? Install the [server] extra, and it unlocks a fastapi-based REST API. This “pay-as-you-go” approach keeps your project lightweight.\nPythonic API\nBeaverDB is designed to feel like you’re just using standard Python data structures. You shouldn’t have to write complex SQL queries just to save a Python dict or list. The goal is to make the database feel like a natural extension of your code.\nStandard SQLite Compatibility\nThis is the “no-magic” rule. The my_app.db file that BeaverDB creates is a 100% standard, valid SQLite file. You can open it with any database tool (like DB Browser for SQLite) and see your data in regular tables. This ensures your data is never locked into a proprietary format.\nSynchronous Core with Async Potential\nThe core library is synchronous, which makes it simple and robust for multi-threaded applications. However, BeaverDB is fully aware of the modern asyncio world. For every data structure, you can call .as_async() to get a fully awaitable version that runs its blocking operations in a background thread, keeping your asyncio event loop from getting blocked.\n\n\n\nBeaverDB shines in scenarios where simplicity, robustness, and local performance are more important than massive, web-scale concurrency.\n\nLocal AI & RAG: Perfect for building Retrieval-Augmented Generation (RAG) applications that run on your local machine. You can store your vector embeddings and their corresponding text right next to each other.\nDesktop Utilities & CLI Tools: The ideal companion for a custom tool that needs to remember user preferences, manage a history, or cache results.\nChatbots: A persistent list is a perfect, simple way to store a chatbot’s conversation history for a user.\nRapid Prototyping: Get your idea up and running in minutes. Start with a local .db file, and if your project grows, you can deploy it as a REST API without changing your application logic.\n\n\n\n\nWe’ve designed this documentation to get you the information you need, whether you’re building your first script or contributing to the core.\nThis guide is split into two main parts:\n\nPart 1: The User Guide\nThis is your starting point. After the Quickstart, this is where you’ll find an in-depth guide that walks you through how to use every single feature of BeaverDB. We’ll explore each “modality” one by one with practical examples.\nPart 2: The Developer Guide\nThis part is for power users and contributors. We’ll go under the hood to look at the why behind the design. This is where we do deep dives into the core architecture, the concurrency model (threading and locking), and the internals of how features like vector search are implemented."
  },
  {
    "objectID": "intro.html#what-is-beaverdb",
    "href": "intro.html#what-is-beaverdb",
    "title": "Introduction",
    "section": "",
    "text": "At its heart, BeaverDB is a multi-modal database in a single SQLite file.\nIt’s a Python library that lets you manage modern, complex data types without ever leaving the comfort of a local file. The name itself tells the story:\nB.E.A.V.E.R. stands for Backend for mbedded, All-in-one Vector, Entity, and Relationship storage.\nThis means that inside that one .db file, you can seamlessly store and query:\n\nKey-Value Pairs (like a dictionary for your app’s configuration)\nLists (like a persistent to-do list)\nVector Embeddings (for AI and semantic search)\nDocuments & Text (for full-text search)\nGraph Relationships (to connect your data together)\n…and much more.\n\nAll this power comes from building on top of the world’s most deployed database engine: SQLite."
  },
  {
    "objectID": "intro.html#the-beaverdb-philosophy",
    "href": "intro.html#the-beaverdb-philosophy",
    "title": "Introduction",
    "section": "",
    "text": "BeaverDB is built on a few core principles. Understanding these will help you know when and why to choose it for your project.\nRobust, Safe, and Durable\nYour data should be safe, period. BeaverDB is built to be resilient. Thanks to SQLite’s atomic transactions and Write-Ahead Logging (WAL) mode, your database is crash-safe. If your program crashes mid-operation, your data is never lost or corrupted; the database simply rolls back the incomplete transaction.\nFurthermore, it’s designed for concurrency. It’s both thread-safe (different threads can share one BeaverDB object) and process-safe (multiple, independent Python scripts can read from and write to the same database file at the same time). For tasks that require true coordination, it even provides a simple, built-in distributed lock.\nPerformant by Default\nBeaverDB is fast. It’s not just “fast for a small database”–it’s genuinely fast for the vast majority of medium-sized projects. Because it’s an embedded library, there is zero network latency for any query.\nLet’s be clear: if you’re building the next X (formerly Twitter) and need to handle millions of documents and thousands of queries per second, you’ll need a distributed, networked database. But for almost everything else? BeaverDB is more than fast enough. If your project is in the thousand to tens-of-thousands of documents range, you’ll find it’s incredibly responsive.\nLocal-First & Embedded\nThe default, primary way to use BeaverDB is as a single file right next to your code. This means your entire database—users, vectors, chat logs, and all—is contained in one portable file (e.g., my_app.db). You can copy it, email it, or back it up. This “local-first” approach is what makes it so fast and simple to deploy.\nMinimal & Optional Dependencies\nThe core BeaverDB library has zero external dependencies. You can get started with key-value stores, lists, and queues right away.\nWant to add vector search? Great! Install the [vector] extra, and BeaverDB will activate its faiss integration. Need a web server? Install the [server] extra, and it unlocks a fastapi-based REST API. This “pay-as-you-go” approach keeps your project lightweight.\nPythonic API\nBeaverDB is designed to feel like you’re just using standard Python data structures. You shouldn’t have to write complex SQL queries just to save a Python dict or list. The goal is to make the database feel like a natural extension of your code.\nStandard SQLite Compatibility\nThis is the “no-magic” rule. The my_app.db file that BeaverDB creates is a 100% standard, valid SQLite file. You can open it with any database tool (like DB Browser for SQLite) and see your data in regular tables. This ensures your data is never locked into a proprietary format.\nSynchronous Core with Async Potential\nThe core library is synchronous, which makes it simple and robust for multi-threaded applications. However, BeaverDB is fully aware of the modern asyncio world. For every data structure, you can call .as_async() to get a fully awaitable version that runs its blocking operations in a background thread, keeping your asyncio event loop from getting blocked."
  },
  {
    "objectID": "intro.html#ideal-use-cases",
    "href": "intro.html#ideal-use-cases",
    "title": "Introduction",
    "section": "",
    "text": "BeaverDB shines in scenarios where simplicity, robustness, and local performance are more important than massive, web-scale concurrency.\n\nLocal AI & RAG: Perfect for building Retrieval-Augmented Generation (RAG) applications that run on your local machine. You can store your vector embeddings and their corresponding text right next to each other.\nDesktop Utilities & CLI Tools: The ideal companion for a custom tool that needs to remember user preferences, manage a history, or cache results.\nChatbots: A persistent list is a perfect, simple way to store a chatbot’s conversation history for a user.\nRapid Prototyping: Get your idea up and running in minutes. Start with a local .db file, and if your project grows, you can deploy it as a REST API without changing your application logic."
  },
  {
    "objectID": "intro.html#how-this-guide-is-structured",
    "href": "intro.html#how-this-guide-is-structured",
    "title": "Introduction",
    "section": "",
    "text": "We’ve designed this documentation to get you the information you need, whether you’re building your first script or contributing to the core.\nThis guide is split into two main parts:\n\nPart 1: The User Guide\nThis is your starting point. After the Quickstart, this is where you’ll find an in-depth guide that walks you through how to use every single feature of BeaverDB. We’ll explore each “modality” one by one with practical examples.\nPart 2: The Developer Guide\nThis part is for power users and contributors. We’ll go under the hood to look at the why behind the design. This is where we do deep dives into the core architecture, the concurrency model (threading and locking), and the internals of how features like vector search are implemented."
  },
  {
    "objectID": "guide-concurrency.html",
    "href": "guide-concurrency.html",
    "title": "Concurrency",
    "section": "",
    "text": "Concurrency\nChapter Outline:\n\n7.1. Inter-Process Locks (db.lock)\n\nCreating a critical section: with db.lock(\"my_task\", timeout=10): ...\nGuarantees: Fair (FIFO) and Deadlock-Proof (via TTL).\n\n7.2. Atomic Operations on Data Structures\n\nLocking a specific manager: with db.dict(\"config\") as config: ...\nUse Case: Atomically getting and processing a batch of items from a queue."
  },
  {
    "objectID": "guide-dicts-blobs.html",
    "href": "guide-dicts-blobs.html",
    "title": "Key-Value and Blob Storage",
    "section": "",
    "text": "Key-Value and Blob Storage\nChapter Outline:\n\n3.1. Dictionaries & Caching (db.dict)\n\nA Python-like dictionary interface: config[\"api_key\"] = ...\nStandard methods: .get(), del, len(), iterating with .items().\nUse Case: Caching with TTL: Using .set(key, value, ttl_seconds=3600).\n\n3.2. Blob Storage (db.blobs)\n\nStoring binary data (images, PDFs, files) with metadata.\nAPI: .put(key, data, metadata), .get(key).\nThe Blob object: Accessing .data and .metadata."
  },
  {
    "objectID": "guide-realtime.html",
    "href": "guide-realtime.html",
    "title": "Real-Time Data",
    "section": "",
    "text": "Real-Time Data\nChapter Outline:\n\n6.1. Publish/Subscribe (db.channel)\n\nHigh-efficiency, multi-process messaging.\nPublishing: channel.publish(payload).\nSubscribing: with channel.subscribe() as listener: for msg in listener.listen(): ...\n\n6.2. Time-Indexed Logs (db.log)\n\nCreating structured, time-series logs: logs.log(data).\nQuerying history: .range(start_time, end_time).\nFeature: Creating a live dashboard with .live(window, period, aggregator)."
  },
  {
    "objectID": "guide-deployment.html",
    "href": "guide-deployment.html",
    "title": "Deployment & Access",
    "section": "",
    "text": "Deployment & Access\nChapter Outline:\n\n8.1. The REST API Server (beaver serve)\n\nExposing your database as a FastAPI application.\nCommand: beaver serve --database my.db --port 8000\nAccessing the interactive OpenAPI docs at /docs.\n\n8.2. The Command-Line Client (beaver client)\n\nInteracting with your database from the terminal for admin and debugging.\nExample: beaver client --database my.db dict config get theme\n\n8.3. Docker Deployment\n\nRunning the server in a container for stable deployment.\ndocker run -p 8000:8000 -v $(pwd)/data:/app apiad/beaverdb"
  }
]