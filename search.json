[
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "This is where the fun begins. Let’s get BeaverDB installed and run your first multi-modal script. You’ll be up and running in about 30 seconds.\n\n\nBeaverDB is a Python library, so you can install it right from your terminal using uv or pip.\nThe Core Install\nThe core beaver-db package includes everything you need for local development, including the CLI, vector search, and all data structures.\n# This includes the core library, vector search, and the CLI\npip install beaver-db\nInstalling Optional Features\nThe only optional feature is the REST API server, which allows you to run BeaverDB as a networked service.\n\nbeaver-db[remote]: Adds the fastapi-based REST server, which you can run with the beaver serve command.\nbeaver-db[full]: A convenience extra that includes remote and all future optional features.\n\n# To install all features, including the REST server\npip install \"beaver-db[full]\"\n\n\n\nYou can also run the BeaverDB REST API server using Docker. This is the recommended way to deploy BeaverDB as a service.\n# Pull the latest image from the GitHub Container Registry\ndocker pull ghcr.io/syalia-srl/beaver:latest\n\n# Run the server, mounting a local directory to persist data\ndocker run -p 8000:8000 \\\n  -v $(pwd)/my-beaver-data:/app/data \\\n  -e \"DATABASE=data/production.db\" \\\n  ghcr.io/syalia-srl/beaver:latest\nThis command:\n\nRuns the server on port 8000.\nMounts a local folder named my-beaver-data into the container.\nTells the server to create its database file at /app/data/production.db (which will persist in your my-beaver-data folder).\n\n\n\n\nLet’s create a single Python script that shows off BeaverDB’s “multi-modal” power.\nCreate a new file named quickstart.py.\n\n\nFirst, import BeaverDB and Document. The BeaverDB class is your main entry point, and Document is the object we’ll use for storing rich data. This line creates a single file, my_data.db, if it doesn’t already exist.\nfrom beaver import BeaverDB, Document\n\n# This creates a single file \"my_data.db\" if it doesn't exist\n# and sets it up for safe, concurrent access.\ndb = BeaverDB(\"my_data.db\")\n\n\n\nNow let’s use a namespaced dictionary. This is perfect for storing app configuration or user settings. We get it by calling db.dict(\"app_config\"). The object it returns behaves just like a standard Python dict.\n# This is perfect for storing app configuration or user settings.\nconfig = db.dict(\"app_config\")\n\n# Assigning a value saves it instantly to the database file.\nconfig[\"theme\"] = \"dark\"\nconfig[\"user_id\"] = 123\n\n# You can read the value back just as easily:\nprint(f\"App theme is: {config['theme']}\")\n\n\n\nNext, let’s store some ordered data. A persistent list (db.list()) is great for a to-do list, a job queue, or a chat history. It supports methods like push, pop, and standard index access.\n# This is great for a to-do list, a job queue, or a chat history.\ntasks = db.list(\"daily_tasks\")\n\n# Use .push() to append items\ntasks.push({\"id\": \"task-001\", \"desc\": \"Write project report\"})\ntasks.push({\"id\": \"task-002\", \"desc\": \"Deploy new feature\"})\n\n# You can access items by index, just like a normal list:\nprint(f\"First task is: {tasks[0]['desc']}\")\n\n\n\nThe “collection” is the most powerful feature. It stores rich Document objects and allows you to search them using vectors, text, or graph relationships.\nLet’s get a collection and create a Document to store. The body field holds all of our text and metadata.\n# This is the most powerful feature, combining data and search.\narticles = db.collection(\"articles\")\n\n# Create a Document to store.\n# We give it a unique ID and some text content in the 'body'.\ndoc = Document(\n    id=\"sqlite-001\",\n    body=\"SQLite is a powerful embedded database ideal for local apps.\"\n)\n\n\n\nNow, we’ll save the document using .index(). By setting fts=True, we tell BeaverDB to also automatically add the text in body to a Full-Text Search index. We’ll add fuzzy=True to tolerate typos.\n# This not only saves the document but also automatically\n# makes its text content searchable via a Full-Text Search (FTS) index\n# with optional fuzzy matching.\narticles.index(doc, fts=True, fuzzy=True)\n\n\n\nFinally, let’s query our collection. We’ll use .match() to perform a text search. Notice the intentional typo in “datbase”. Because we indexed with fuzziness=1, BeaverDB finds the correct document anyway!\n# This isn't a simple string find; it's a real search engine!\n# Note the typo in \"datbase\"\nresults = articles.match(query=\"datbase\", fuzziness=1)\n\n# The result is a list of tuples: (document, score)\ntop_doc, rank = results[0]\nprint(f\"Search found: '{top_doc.body}' (Score: {rank:.2f})\")\nWhen you run your quickstart.py script, you’ll have a single my_data.db file containing your config, your task list, and your searchable articles.\n\n\n\n\nThe beaver CLI is included in the core installation and is a great way to inspect or manage your database from the terminal.\nLet’s interact with the my_data.db file we just created.\n# Get the 'theme' key from the 'app_config' dictionary\n$ beaver --database my_data.db dict app_config get theme\n\"dark\"\n\n# Run the same fuzzy search from our script\n$ beaver --database my_data.db collection articles match \"datbase\" --fuzziness 1\n[\n  {\n    \"document\": {\n      \"id\": \"sqlite-001\",\n      \"embedding\": null,\n      \"body\": \"SQLite is a powerful embedded database ideal for local apps.\"\n    },\n    \"score\": 0.0\n  }\n]\n\n\n\nYou can start a REST API server for your database using the beaver serve command. This allows you to interact with your BeaverDB instance over HTTP, making it easy to build web or mobile applications.\n$ beaver serve --database my_data.db --port 8000\nINFO:     Started server process [12345]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nNow you can access your database via HTTP requests. For example, to get the theme from the app_config dictionary:\n$ curl http://localhost:8000/dict/app_config/theme\n\"dark\"\nThat’s it! You’ve successfully installed BeaverDB, created a multi-modal database, and interacted with it both programmatically and via the CLI and REST API.\nHappy coding!"
  },
  {
    "objectID": "quickstart.html#installation",
    "href": "quickstart.html#installation",
    "title": "Quickstart Guide",
    "section": "",
    "text": "BeaverDB is a Python library, so you can install it right from your terminal using uv or pip.\nThe Core Install\nThe core beaver-db package includes everything you need for local development, including the CLI, vector search, and all data structures.\n# This includes the core library, vector search, and the CLI\npip install beaver-db\nInstalling Optional Features\nThe only optional feature is the REST API server, which allows you to run BeaverDB as a networked service.\n\nbeaver-db[remote]: Adds the fastapi-based REST server, which you can run with the beaver serve command.\nbeaver-db[full]: A convenience extra that includes remote and all future optional features.\n\n# To install all features, including the REST server\npip install \"beaver-db[full]\""
  },
  {
    "objectID": "quickstart.html#docker",
    "href": "quickstart.html#docker",
    "title": "Quickstart Guide",
    "section": "",
    "text": "You can also run the BeaverDB REST API server using Docker. This is the recommended way to deploy BeaverDB as a service.\n# Pull the latest image from the GitHub Container Registry\ndocker pull ghcr.io/syalia-srl/beaver:latest\n\n# Run the server, mounting a local directory to persist data\ndocker run -p 8000:8000 \\\n  -v $(pwd)/my-beaver-data:/app/data \\\n  -e \"DATABASE=data/production.db\" \\\n  ghcr.io/syalia-srl/beaver:latest\nThis command:\n\nRuns the server on port 8000.\nMounts a local folder named my-beaver-data into the container.\nTells the server to create its database file at /app/data/production.db (which will persist in your my-beaver-data folder)."
  },
  {
    "objectID": "quickstart.html#a-step-by-step-example",
    "href": "quickstart.html#a-step-by-step-example",
    "title": "Quickstart Guide",
    "section": "",
    "text": "Let’s create a single Python script that shows off BeaverDB’s “multi-modal” power.\nCreate a new file named quickstart.py.\n\n\nFirst, import BeaverDB and Document. The BeaverDB class is your main entry point, and Document is the object we’ll use for storing rich data. This line creates a single file, my_data.db, if it doesn’t already exist.\nfrom beaver import BeaverDB, Document\n\n# This creates a single file \"my_data.db\" if it doesn't exist\n# and sets it up for safe, concurrent access.\ndb = BeaverDB(\"my_data.db\")\n\n\n\nNow let’s use a namespaced dictionary. This is perfect for storing app configuration or user settings. We get it by calling db.dict(\"app_config\"). The object it returns behaves just like a standard Python dict.\n# This is perfect for storing app configuration or user settings.\nconfig = db.dict(\"app_config\")\n\n# Assigning a value saves it instantly to the database file.\nconfig[\"theme\"] = \"dark\"\nconfig[\"user_id\"] = 123\n\n# You can read the value back just as easily:\nprint(f\"App theme is: {config['theme']}\")\n\n\n\nNext, let’s store some ordered data. A persistent list (db.list()) is great for a to-do list, a job queue, or a chat history. It supports methods like push, pop, and standard index access.\n# This is great for a to-do list, a job queue, or a chat history.\ntasks = db.list(\"daily_tasks\")\n\n# Use .push() to append items\ntasks.push({\"id\": \"task-001\", \"desc\": \"Write project report\"})\ntasks.push({\"id\": \"task-002\", \"desc\": \"Deploy new feature\"})\n\n# You can access items by index, just like a normal list:\nprint(f\"First task is: {tasks[0]['desc']}\")\n\n\n\nThe “collection” is the most powerful feature. It stores rich Document objects and allows you to search them using vectors, text, or graph relationships.\nLet’s get a collection and create a Document to store. The body field holds all of our text and metadata.\n# This is the most powerful feature, combining data and search.\narticles = db.collection(\"articles\")\n\n# Create a Document to store.\n# We give it a unique ID and some text content in the 'body'.\ndoc = Document(\n    id=\"sqlite-001\",\n    body=\"SQLite is a powerful embedded database ideal for local apps.\"\n)\n\n\n\nNow, we’ll save the document using .index(). By setting fts=True, we tell BeaverDB to also automatically add the text in body to a Full-Text Search index. We’ll add fuzzy=True to tolerate typos.\n# This not only saves the document but also automatically\n# makes its text content searchable via a Full-Text Search (FTS) index\n# with optional fuzzy matching.\narticles.index(doc, fts=True, fuzzy=True)\n\n\n\nFinally, let’s query our collection. We’ll use .match() to perform a text search. Notice the intentional typo in “datbase”. Because we indexed with fuzziness=1, BeaverDB finds the correct document anyway!\n# This isn't a simple string find; it's a real search engine!\n# Note the typo in \"datbase\"\nresults = articles.match(query=\"datbase\", fuzziness=1)\n\n# The result is a list of tuples: (document, score)\ntop_doc, rank = results[0]\nprint(f\"Search found: '{top_doc.body}' (Score: {rank:.2f})\")\nWhen you run your quickstart.py script, you’ll have a single my_data.db file containing your config, your task list, and your searchable articles."
  },
  {
    "objectID": "quickstart.html#using-the-cli",
    "href": "quickstart.html#using-the-cli",
    "title": "Quickstart Guide",
    "section": "",
    "text": "The beaver CLI is included in the core installation and is a great way to inspect or manage your database from the terminal.\nLet’s interact with the my_data.db file we just created.\n# Get the 'theme' key from the 'app_config' dictionary\n$ beaver --database my_data.db dict app_config get theme\n\"dark\"\n\n# Run the same fuzzy search from our script\n$ beaver --database my_data.db collection articles match \"datbase\" --fuzziness 1\n[\n  {\n    \"document\": {\n      \"id\": \"sqlite-001\",\n      \"embedding\": null,\n      \"body\": \"SQLite is a powerful embedded database ideal for local apps.\"\n    },\n    \"score\": 0.0\n  }\n]"
  },
  {
    "objectID": "quickstart.html#running-the-rest-api-server",
    "href": "quickstart.html#running-the-rest-api-server",
    "title": "Quickstart Guide",
    "section": "",
    "text": "You can start a REST API server for your database using the beaver serve command. This allows you to interact with your BeaverDB instance over HTTP, making it easy to build web or mobile applications.\n$ beaver serve --database my_data.db --port 8000\nINFO:     Started server process [12345]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nNow you can access your database via HTTP requests. For example, to get the theme from the app_config dictionary:\n$ curl http://localhost:8000/dict/app_config/theme\n\"dark\"\nThat’s it! You’ve successfully installed BeaverDB, created a multi-modal database, and interacted with it both programmatically and via the CLI and REST API.\nHappy coding!"
  },
  {
    "objectID": "guide-deployment.html",
    "href": "guide-deployment.html",
    "title": "Deployment & Interfaces",
    "section": "",
    "text": "Deploying BeaverDB is fundamentally different from deploying client-server databases like Postgres or Redis. Because BeaverDB is embedded, there is no separate “Database Server” to install, configure, or maintain.\nYour database is just a file (app.db) that lives alongside your code.\n\n\nIn a traditional stack, you have:\n\nApp Server: Python code (Stateless)\nDB Server: Postgres/MySQL (Stateful)\nCache: Redis (Stateful)\n\nIn a BeaverDB stack, you have:\n\nApp Server: Python code + app.db (Stateful)\n\n\n\n\nZero Latency: No network round-trips. Data access is instant.\nAtomic Backups: Backing up your entire state (Users + Search Index + Logs) is just copying one file.\nSimplified CI/CD: You don’t need to spin up DB containers in your GitHub Actions. Just run your tests.\n\n\n\n\n\nBeaverDB comes with a rich CLI tool to manage your database files directly from the terminal. This is useful for inspection, debugging, and quick modifications without writing Python scripts.\n\n\nThe CLI is installed automatically with the core package.\npip install beaver-db\n\n\n\nThe main command is beaver. You can inspect the database or interact with specific data structures.\n# General Information\nbeaver info --database my_app.db\n\n# Dictionary Operations\nbeaver dicts list                   # List all dictionaries\nbeaver dicts get my_dict my_key     # Retrieve a value\nbeaver dicts set my_dict my_key val # Set a value\n\n# Queue Inspection\nbeaver queues peek my_queue         # See the next job\nUse beaver --help to see all available commands.\n\n\n\n\nIf you need to access the database from multiple distinct machines, non-Python environments, or from a browser, you can run BeaverDB as a standalone REST API server.\nThis exposes all features (Vectors, Logs, Pub/Sub, etc.) over a high-performance FastAPI interface.\n\n\nThe server requires additional dependencies. Install them using the remote extra:\npip install \"beaver-db[remote]\"\n\n\n\nUse the beaver serve command to start the process.\n# Start server on port 8080\nbeaver serve --database ./my_app.db --host 0.0.0.0 --port 8080\n\n\n\nYou can connect to this server using the BeaverClient in Python, which provides an API identical to the local BeaverDB class. This allows you to switch between local (embedded) and remote (server) modes by changing only one line of code.\nfrom beaver import BeaverClient\n\n# Connect to the remote server\ndb = BeaverClient(\"http://localhost:8080\")\n\n# Use exactly like the local version\nusers = db.dict(\"users\")\nusers[\"alice\"] = {\"role\": \"admin\"}\n\n\n\n\n\n\n\nThe database runs inside your application process.\n\nUse Case: CLI tools, Desktop apps, Single-instance web apps (e.g., a VPS running one FastAPI service).\nSetup: Just ensure the process has Write Permissions to the directory containing the .db file.\n\n\n\n\nYou run multiple instances of your app (e.g., gunicorn -w 4 or multiple background workers).\n\nSetup: All processes must share access to the same filesystem volume.\nConcurrency: BeaverDB handles the locking. You don’t need to do anything special.\nLimitation: This works on a single machine (VPS/Dedicated Server). It does not work across different servers (e.g., AWS Lambda or Heroku Dynos) unless you mount a shared network file system like EFS.\n\n\n\n\n\n\nSince the database is a file, you must ensure it persists when the container restarts. Use a Docker Volume.\n\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY pyproject.toml .\nRUN pip install \"beaver-db[all]\"\n\n# Copy app code\nCOPY . .\n\n# Create a directory for the database\nRUN mkdir /data\n\n# Run the app (pointing DB to the volume)\nCMD [\"python\", \"main.py\", \"--db-path\", \"/data/app.db\"]\n\n\n\nservices:\n  app:\n    build: .\n    volumes:\n      - beaver_data:/data\n    ports:\n      - \"8000:8000\"\n\nvolumes:\n  beaver_data:\n\n\n\n\n\n\n\nYou cannot just copy the .db file while the app is writing to it (you might get a corrupted “torn page”). Use the SQLite Online Backup API.\nUsing Python:\nimport sqlite3\n\ndef backup(source_db_path, backup_path):\n    src = sqlite3.connect(source_db_path)\n    dst = sqlite3.connect(backup_path)\n    with dst:\n        src.backup(dst)\n    dst.close()\n    src.close()\n\n\n\nTo restore, simply stop your application and replace the .db file with your backup copy.\n\n\n\n\n\nFor high-throughput production environments, consider these settings:\n\n\nBeaverDB enables WAL mode by default. This allows concurrent readers and writers. Do not disable this.\n\n\n\nIf your database fits within your server’s RAM, enabling memory mapping can significantly boost read performance. BeaverDB enables this by default with a 256MB limit.\nYou can adjust this limit during initialization:\n# Enable mmap for a 2GB database\ndb = BeaverDB(\n    \"app.db\",\n    pragma_mmap_size=2 * 1024 * 1024 * 1024  # 2GB\n)\n\n\n\nFor bulk ingestion (ETL jobs), always use .batched().\n\nBad: 10,000 db.dict[\"key\"] = val calls = 10,000 transactions (~20s).\nGood: with db.dict.batched()... = 1 transaction (~0.1s)."
  },
  {
    "objectID": "guide-deployment.html#the-single-file-philosophy",
    "href": "guide-deployment.html#the-single-file-philosophy",
    "title": "Deployment & Interfaces",
    "section": "",
    "text": "In a traditional stack, you have:\n\nApp Server: Python code (Stateless)\nDB Server: Postgres/MySQL (Stateful)\nCache: Redis (Stateful)\n\nIn a BeaverDB stack, you have:\n\nApp Server: Python code + app.db (Stateful)\n\n\n\n\nZero Latency: No network round-trips. Data access is instant.\nAtomic Backups: Backing up your entire state (Users + Search Index + Logs) is just copying one file.\nSimplified CI/CD: You don’t need to spin up DB containers in your GitHub Actions. Just run your tests."
  },
  {
    "objectID": "guide-deployment.html#command-line-interface-cli",
    "href": "guide-deployment.html#command-line-interface-cli",
    "title": "Deployment & Interfaces",
    "section": "",
    "text": "BeaverDB comes with a rich CLI tool to manage your database files directly from the terminal. This is useful for inspection, debugging, and quick modifications without writing Python scripts.\n\n\nThe CLI is installed automatically with the core package.\npip install beaver-db\n\n\n\nThe main command is beaver. You can inspect the database or interact with specific data structures.\n# General Information\nbeaver info --database my_app.db\n\n# Dictionary Operations\nbeaver dicts list                   # List all dictionaries\nbeaver dicts get my_dict my_key     # Retrieve a value\nbeaver dicts set my_dict my_key val # Set a value\n\n# Queue Inspection\nbeaver queues peek my_queue         # See the next job\nUse beaver --help to see all available commands."
  },
  {
    "objectID": "guide-deployment.html#rest-api-server-beaver-serve",
    "href": "guide-deployment.html#rest-api-server-beaver-serve",
    "title": "Deployment & Interfaces",
    "section": "",
    "text": "If you need to access the database from multiple distinct machines, non-Python environments, or from a browser, you can run BeaverDB as a standalone REST API server.\nThis exposes all features (Vectors, Logs, Pub/Sub, etc.) over a high-performance FastAPI interface.\n\n\nThe server requires additional dependencies. Install them using the remote extra:\npip install \"beaver-db[remote]\"\n\n\n\nUse the beaver serve command to start the process.\n# Start server on port 8080\nbeaver serve --database ./my_app.db --host 0.0.0.0 --port 8080\n\n\n\nYou can connect to this server using the BeaverClient in Python, which provides an API identical to the local BeaverDB class. This allows you to switch between local (embedded) and remote (server) modes by changing only one line of code.\nfrom beaver import BeaverClient\n\n# Connect to the remote server\ndb = BeaverClient(\"http://localhost:8080\")\n\n# Use exactly like the local version\nusers = db.dict(\"users\")\nusers[\"alice\"] = {\"role\": \"admin\"}"
  },
  {
    "objectID": "guide-deployment.html#deployment-strategies",
    "href": "guide-deployment.html#deployment-strategies",
    "title": "Deployment & Interfaces",
    "section": "",
    "text": "The database runs inside your application process.\n\nUse Case: CLI tools, Desktop apps, Single-instance web apps (e.g., a VPS running one FastAPI service).\nSetup: Just ensure the process has Write Permissions to the directory containing the .db file.\n\n\n\n\nYou run multiple instances of your app (e.g., gunicorn -w 4 or multiple background workers).\n\nSetup: All processes must share access to the same filesystem volume.\nConcurrency: BeaverDB handles the locking. You don’t need to do anything special.\nLimitation: This works on a single machine (VPS/Dedicated Server). It does not work across different servers (e.g., AWS Lambda or Heroku Dynos) unless you mount a shared network file system like EFS."
  },
  {
    "objectID": "guide-deployment.html#dockerizing-beaverdb",
    "href": "guide-deployment.html#dockerizing-beaverdb",
    "title": "Deployment & Interfaces",
    "section": "",
    "text": "Since the database is a file, you must ensure it persists when the container restarts. Use a Docker Volume.\n\n\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY pyproject.toml .\nRUN pip install \"beaver-db[all]\"\n\n# Copy app code\nCOPY . .\n\n# Create a directory for the database\nRUN mkdir /data\n\n# Run the app (pointing DB to the volume)\nCMD [\"python\", \"main.py\", \"--db-path\", \"/data/app.db\"]\n\n\n\nservices:\n  app:\n    build: .\n    volumes:\n      - beaver_data:/data\n    ports:\n      - \"8000:8000\"\n\nvolumes:\n  beaver_data:"
  },
  {
    "objectID": "guide-deployment.html#backups-restore",
    "href": "guide-deployment.html#backups-restore",
    "title": "Deployment & Interfaces",
    "section": "",
    "text": "You cannot just copy the .db file while the app is writing to it (you might get a corrupted “torn page”). Use the SQLite Online Backup API.\nUsing Python:\nimport sqlite3\n\ndef backup(source_db_path, backup_path):\n    src = sqlite3.connect(source_db_path)\n    dst = sqlite3.connect(backup_path)\n    with dst:\n        src.backup(dst)\n    dst.close()\n    src.close()\n\n\n\nTo restore, simply stop your application and replace the .db file with your backup copy."
  },
  {
    "objectID": "guide-deployment.html#performance-tuning",
    "href": "guide-deployment.html#performance-tuning",
    "title": "Deployment & Interfaces",
    "section": "",
    "text": "For high-throughput production environments, consider these settings:\n\n\nBeaverDB enables WAL mode by default. This allows concurrent readers and writers. Do not disable this.\n\n\n\nIf your database fits within your server’s RAM, enabling memory mapping can significantly boost read performance. BeaverDB enables this by default with a 256MB limit.\nYou can adjust this limit during initialization:\n# Enable mmap for a 2GB database\ndb = BeaverDB(\n    \"app.db\",\n    pragma_mmap_size=2 * 1024 * 1024 * 1024  # 2GB\n)\n\n\n\nFor bulk ingestion (ETL jobs), always use .batched().\n\nBad: 10,000 db.dict[\"key\"] = val calls = 10,000 transactions (~20s).\nGood: with db.dict.batched()... = 1 transaction (~0.1s)."
  },
  {
    "objectID": "guide-locks.html",
    "href": "guide-locks.html",
    "title": "Locks (Concurrency Control)",
    "section": "",
    "text": "BeaverDB is designed to be Process-Safe. Multiple Python processes (e.g., web workers, scripts, background jobs) can read and write to the same database file simultaneously without corrupting it.\nHowever, sometimes you need to synchronize logic across these processes. For example: * “Only one worker should run the daily report generation.” * “Only one process should perform database compaction.” * “Wait until the current job is finished before starting the next.”\nThe LockManager provides a simple, persistent, dead-lock safe locking primitive to handle these cases.\n\n\nInitialize a lock using db.lock().\nfrom beaver import BeaverDB\nimport time\n\ndb = BeaverDB(\"app.db\")\n\n# 1. Define the critical section\n# The lock name is global across all processes using this database file.\nwith db.lock(\"report_generation\", timeout=10.0):\n    print(\"Lock acquired! I am the only process running this block.\")\n\n    # Simulate work\n    time.sleep(5)\n\n    print(\"Work done. Releasing lock.\")\n\n# 2. Another process trying to acquire \"report_generation\"\n# will wait here until the first process finishes or the timeout expires.\n\n\n\nThe lock is implemented using a persistent table in SQLite. It is:\n\nFair (FIFO): Processes are served in the order they requested the lock.\nDeadlock-Proof: Every lock has a Time-To-Live (TTL). If a process crashes while holding a lock, the lock will automatically expire after the TTL, allowing other processes to proceed.\nProcess-Aware: It works across different scripts, terminals, or containers sharing the same .db file.\n\n\n\n\nUnlike standard Python threading.Lock, a BeaverDB Lock instance is re-entrant. This means if you try to acquire the lock again using the same object instance, it will succeed immediately without blocking.\nThis design prevents self-deadlocks in complex code paths where a locked function calls another function that requires the same lock.\nlock = db.lock(\"resource\")\n\nwith lock:\n    print(\"Lock acquired.\")\n\n    # This works perfectly!\n    # It detects that this specific 'lock' instance already holds it.\n    with lock:\n        print(\"Still holding the lock (Nested block).\")\n\n\nRe-entrancy applies to the Lock Object Instance, not just the process.\nIf you create two different lock objects with the same name in the same process, they act as distinct waiters. The second one will block waiting for the first one to release.\n# Two distinct instances for the SAME resource\nlock_a = db.lock(\"my_resource\")\nlock_b = db.lock(\"my_resource\")\n\nwith lock_a:\n    # This WILL BLOCK (and deadlock if no timeout is set)\n    # because lock_b sees that \"my_resource\" is held by lock_a.\n    with lock_b:\n        pass\n\n\n\n\n\n\nThe default behavior is to block (wait) until the lock is available.\n# Waits forever until the lock is free\nwith db.lock(\"critical_resource\"):\n    process_resource()\n\n\n\nYou can specify a maximum wait time. If the lock isn’t acquired by then, it raises a TimeoutError.\ntry:\n    # Wait up to 5 seconds\n    with db.lock(\"resource\", timeout=5.0):\n        process()\nexcept TimeoutError:\n    print(\"Could not acquire lock. Resource is busy.\")\n\n\n\nIf you want to “try” to get the lock and fail immediately if it’s taken (e.g., for a cron job that shouldn’t overlap), use block=False.\nlock = db.lock(\"maintenance\")\n\nif lock.acquire(block=False):\n    try:\n        run_maintenance()\n    finally:\n        lock.release()\nelse:\n    print(\"Maintenance is already running. Skipping.\")\n\n\n\n\n\n\nIf you have a long-running task (longer than the default TTL), you must periodically renew the lock to prevent it from expiring and letting another process in.\nlock = db.lock(\"long_task\", lock_ttl=60) # 1 minute TTL\n\nif lock.acquire():\n    try:\n        for i in range(10):\n            do_step(i)\n            # Extend the lock for another 60 seconds\n            lock.renew(lock_ttl=60)\n    finally:\n        lock.release()\n\n\n\nIn rare cases (e.g., testing or admin intervention), you might want to forcibly break a lock held by another process.\n# Force-release the lock, kicking out any current holder\ndb.lock(\"stuck_job\").clear()"
  },
  {
    "objectID": "guide-locks.html#quick-start",
    "href": "guide-locks.html#quick-start",
    "title": "Locks (Concurrency Control)",
    "section": "",
    "text": "Initialize a lock using db.lock().\nfrom beaver import BeaverDB\nimport time\n\ndb = BeaverDB(\"app.db\")\n\n# 1. Define the critical section\n# The lock name is global across all processes using this database file.\nwith db.lock(\"report_generation\", timeout=10.0):\n    print(\"Lock acquired! I am the only process running this block.\")\n\n    # Simulate work\n    time.sleep(5)\n\n    print(\"Work done. Releasing lock.\")\n\n# 2. Another process trying to acquire \"report_generation\"\n# will wait here until the first process finishes or the timeout expires."
  },
  {
    "objectID": "guide-locks.html#how-it-works",
    "href": "guide-locks.html#how-it-works",
    "title": "Locks (Concurrency Control)",
    "section": "",
    "text": "The lock is implemented using a persistent table in SQLite. It is:\n\nFair (FIFO): Processes are served in the order they requested the lock.\nDeadlock-Proof: Every lock has a Time-To-Live (TTL). If a process crashes while holding a lock, the lock will automatically expire after the TTL, allowing other processes to proceed.\nProcess-Aware: It works across different scripts, terminals, or containers sharing the same .db file."
  },
  {
    "objectID": "guide-locks.html#re-entrancy-recursive-locking",
    "href": "guide-locks.html#re-entrancy-recursive-locking",
    "title": "Locks (Concurrency Control)",
    "section": "",
    "text": "Unlike standard Python threading.Lock, a BeaverDB Lock instance is re-entrant. This means if you try to acquire the lock again using the same object instance, it will succeed immediately without blocking.\nThis design prevents self-deadlocks in complex code paths where a locked function calls another function that requires the same lock.\nlock = db.lock(\"resource\")\n\nwith lock:\n    print(\"Lock acquired.\")\n\n    # This works perfectly!\n    # It detects that this specific 'lock' instance already holds it.\n    with lock:\n        print(\"Still holding the lock (Nested block).\")\n\n\nRe-entrancy applies to the Lock Object Instance, not just the process.\nIf you create two different lock objects with the same name in the same process, they act as distinct waiters. The second one will block waiting for the first one to release.\n# Two distinct instances for the SAME resource\nlock_a = db.lock(\"my_resource\")\nlock_b = db.lock(\"my_resource\")\n\nwith lock_a:\n    # This WILL BLOCK (and deadlock if no timeout is set)\n    # because lock_b sees that \"my_resource\" is held by lock_a.\n    with lock_b:\n        pass"
  },
  {
    "objectID": "guide-locks.html#basic-operations",
    "href": "guide-locks.html#basic-operations",
    "title": "Locks (Concurrency Control)",
    "section": "",
    "text": "The default behavior is to block (wait) until the lock is available.\n# Waits forever until the lock is free\nwith db.lock(\"critical_resource\"):\n    process_resource()\n\n\n\nYou can specify a maximum wait time. If the lock isn’t acquired by then, it raises a TimeoutError.\ntry:\n    # Wait up to 5 seconds\n    with db.lock(\"resource\", timeout=5.0):\n        process()\nexcept TimeoutError:\n    print(\"Could not acquire lock. Resource is busy.\")\n\n\n\nIf you want to “try” to get the lock and fail immediately if it’s taken (e.g., for a cron job that shouldn’t overlap), use block=False.\nlock = db.lock(\"maintenance\")\n\nif lock.acquire(block=False):\n    try:\n        run_maintenance()\n    finally:\n        lock.release()\nelse:\n    print(\"Maintenance is already running. Skipping.\")"
  },
  {
    "objectID": "guide-locks.html#advanced-configuration",
    "href": "guide-locks.html#advanced-configuration",
    "title": "Locks (Concurrency Control)",
    "section": "",
    "text": "If you have a long-running task (longer than the default TTL), you must periodically renew the lock to prevent it from expiring and letting another process in.\nlock = db.lock(\"long_task\", lock_ttl=60) # 1 minute TTL\n\nif lock.acquire():\n    try:\n        for i in range(10):\n            do_step(i)\n            # Extend the lock for another 60 seconds\n            lock.renew(lock_ttl=60)\n    finally:\n        lock.release()\n\n\n\nIn rare cases (e.g., testing or admin intervention), you might want to forcibly break a lock held by another process.\n# Force-release the lock, kicking out any current holder\ndb.lock(\"stuck_job\").clear()"
  },
  {
    "objectID": "guide-channels.html",
    "href": "guide-channels.html",
    "title": "Channels (Pub/Sub)",
    "section": "",
    "text": "The ChannelManager provides a high-performance Publish-Subscribe messaging system.\nUnlike Queues (where one item is processed by one worker), Channels implement a Fan-Out pattern: a message sent by one publisher is received by all active subscribers. This is perfect for event notifications, chat systems, or broadcasting updates to multiple services.\n\n\nInitialize a channel using db.channel().\nimport threading\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"chat.db\")\nchat = db.channel(\"general_chat\")\n\n# 1. Subscriber (Listener)\n# Runs in a background thread for this demo\ndef listen():\n    # The 'with' block handles subscription cleanup automatically\n    with chat.subscribe() as sub:\n        print(\"Listening for messages...\")\n        # Blocks until a message arrives\n        for message in sub.listen():\n            print(f\"Received: {message}\")\n\nt = threading.Thread(target=listen, daemon=True)\nt.start()\n\n# 2. Publisher (Sender)\n# Messages are serialized to JSON and persisted to disk\nchat.publish({\"user\": \"alice\", \"text\": \"Hello World!\"})\nchat.publish({\"user\": \"bob\", \"text\": \"Hi Alice!\"})\n\n\n\n\n\nUse .publish(payload) to broadcast a message. The payload can be any JSON-serializable object (dict, list, string).\n# Broadcast a system event\ndb.channel(\"system\").publish({\n    \"event\": \"maintenance_started\",\n    \"duration_minutes\": 60\n})\n\n\n\nTo receive messages, call .subscribe(). This returns a Subscriber object. It is highly recommended to use it as a Context Manager (with statement) to ensure you strictly unsubscribe when done.\nchannel = db.channel(\"system\")\n\nwith channel.subscribe() as subscriber:\n    # ... listen for messages ...\n    pass\n# Automatically unsubscribed here\n\n\n\nThe .listen() method returns a blocking iterator. It yields messages as soon as they are written to the database.\n# Infinite loop (blocks forever)\nfor msg in subscriber.listen():\n    process(msg)\n\n# Timeout (blocks for N seconds, then stops if no message)\n# Note: It raises queue.Empty or TimeoutError depending on implementation\ntry:\n    for msg in subscriber.listen(timeout=5.0):\n        print(msg)\nexcept TimeoutError:\n    print(\"No messages received.\")\n\n\n\n\n\n\nBeaverDB channels work seamlessly with asyncio. Use .as_async() to get an awaitable interface.\nimport asyncio\n\nasync def watch_events():\n    async_channel = db.channel(\"events\").as_async()\n\n    # Async Context Manager\n    async with async_channel.subscribe() as sub:\n        # Async Iterator\n        async for message in sub.listen():\n            print(f\"Got event: {message}\")\n\n\n\nChannels are Process-Safe. You can have a publisher script running in one terminal and multiple subscriber scripts running in other terminals. They will all receive the messages in real-time via the shared SQLite file.\n\n\n\nBeaverDB uses a smart polling architecture. Regardless of how many subscribers you have in a single process (e.g., 100 WebSocket connections listening to a channel), BeaverDB runs only one background thread to poll the database. It then “fans out” the data to the 100 listeners in memory. This keeps CPU usage extremely low.\n\n\n\nMessages are persisted to the beaver_pubsub_log table. By default, new subscribers only see messages sent after they subscribed. To reclaim disk space, you can clear the channel history.\n# Delete all old messages for this channel\ndb.channel(\"general_chat\").clear()"
  },
  {
    "objectID": "guide-channels.html#quick-start",
    "href": "guide-channels.html#quick-start",
    "title": "Channels (Pub/Sub)",
    "section": "",
    "text": "Initialize a channel using db.channel().\nimport threading\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"chat.db\")\nchat = db.channel(\"general_chat\")\n\n# 1. Subscriber (Listener)\n# Runs in a background thread for this demo\ndef listen():\n    # The 'with' block handles subscription cleanup automatically\n    with chat.subscribe() as sub:\n        print(\"Listening for messages...\")\n        # Blocks until a message arrives\n        for message in sub.listen():\n            print(f\"Received: {message}\")\n\nt = threading.Thread(target=listen, daemon=True)\nt.start()\n\n# 2. Publisher (Sender)\n# Messages are serialized to JSON and persisted to disk\nchat.publish({\"user\": \"alice\", \"text\": \"Hello World!\"})\nchat.publish({\"user\": \"bob\", \"text\": \"Hi Alice!\"})"
  },
  {
    "objectID": "guide-channels.html#basic-operations",
    "href": "guide-channels.html#basic-operations",
    "title": "Channels (Pub/Sub)",
    "section": "",
    "text": "Use .publish(payload) to broadcast a message. The payload can be any JSON-serializable object (dict, list, string).\n# Broadcast a system event\ndb.channel(\"system\").publish({\n    \"event\": \"maintenance_started\",\n    \"duration_minutes\": 60\n})\n\n\n\nTo receive messages, call .subscribe(). This returns a Subscriber object. It is highly recommended to use it as a Context Manager (with statement) to ensure you strictly unsubscribe when done.\nchannel = db.channel(\"system\")\n\nwith channel.subscribe() as subscriber:\n    # ... listen for messages ...\n    pass\n# Automatically unsubscribed here\n\n\n\nThe .listen() method returns a blocking iterator. It yields messages as soon as they are written to the database.\n# Infinite loop (blocks forever)\nfor msg in subscriber.listen():\n    process(msg)\n\n# Timeout (blocks for N seconds, then stops if no message)\n# Note: It raises queue.Empty or TimeoutError depending on implementation\ntry:\n    for msg in subscriber.listen(timeout=5.0):\n        print(msg)\nexcept TimeoutError:\n    print(\"No messages received.\")"
  },
  {
    "objectID": "guide-channels.html#advanced-features",
    "href": "guide-channels.html#advanced-features",
    "title": "Channels (Pub/Sub)",
    "section": "",
    "text": "BeaverDB channels work seamlessly with asyncio. Use .as_async() to get an awaitable interface.\nimport asyncio\n\nasync def watch_events():\n    async_channel = db.channel(\"events\").as_async()\n\n    # Async Context Manager\n    async with async_channel.subscribe() as sub:\n        # Async Iterator\n        async for message in sub.listen():\n            print(f\"Got event: {message}\")\n\n\n\nChannels are Process-Safe. You can have a publisher script running in one terminal and multiple subscriber scripts running in other terminals. They will all receive the messages in real-time via the shared SQLite file.\n\n\n\nBeaverDB uses a smart polling architecture. Regardless of how many subscribers you have in a single process (e.g., 100 WebSocket connections listening to a channel), BeaverDB runs only one background thread to poll the database. It then “fans out” the data to the 100 listeners in memory. This keeps CPU usage extremely low.\n\n\n\nMessages are persisted to the beaver_pubsub_log table. By default, new subscribers only see messages sent after they subscribed. To reclaim disk space, you can clear the channel history.\n# Delete all old messages for this channel\ndb.channel(\"general_chat\").clear()"
  },
  {
    "objectID": "guide-dicts.html",
    "href": "guide-dicts.html",
    "title": "Dictionaries (Key-Value Store)",
    "section": "",
    "text": "BeaverDB provides a robust, persistent Key-Value store that behaves almost exactly like a standard Python dictionary. The DictManager allows you to store JSON-serializable data (strings, numbers, lists, dictionaries) that persists to disk and is safe to access across multiple processes.\nThis is the “Swiss Army Knife” for storing configuration, user sessions, application state, or caching API responses.\n\n\nInitialize a dictionary using db.dict(). If the dictionary doesn’t exist, it is created automatically.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"app.db\")\nsettings = db.dict(\"app_settings\")\n\n# 1. Write data (Persists immediately to SQLite)\nsettings[\"theme\"] = \"dark\"\nsettings[\"max_retries\"] = 5\nsettings[\"features\"] = {\"beta\": True, \"logging\": \"verbose\"}\n\n# 2. Read data\nprint(settings[\"theme\"])  # -&gt; \"dark\"\n\n# 3. Process-Safe\n# You can open this same dict in another script/process safely.\n\n\n\n\n\nYou can use standard bracket notation [] or the .get() method.\n# Set value\nsettings[\"user_1\"] = {\"name\": \"Alice\", \"score\": 42}\n\n# Get value (Raises KeyError if missing)\nuser = settings[\"user_1\"]\n\n# Get with default (Safe)\nuser = settings.get(\"user_99\", default={\"name\": \"Guest\"})\n\n\n\nUse the in operator to check if a key exists efficiently (O(1)).\nif \"user_1\" in settings:\n    print(\"User exists!\")\n\n\n\nRemove items using del or .pop().\n# Remove and return value\nold_val = settings.pop(\"user_1\", None)\n\n# Delete key (Raises KeyError if missing)\ndel settings[\"max_retries\"]\n\n# Clear entire dictionary\nsettings.clear()\n\n\n\nYou can iterate over keys, values, or items just like a standard dict. Note that for very large dictionaries (millions of items), this streams data from the database to avoid loading everything into memory at once.\nfor key in settings:\n    print(key)\n\nfor key, val in settings.items():\n    print(f\"{key}: {val}\")\n\n\n\n\n\n\nYou can set keys that automatically expire after a certain duration. This is perfect for caching or temporary session tokens.\n# Set a key that expires in 300 seconds (5 minutes)\nsettings.set(\"session_id\", \"xyz_123\", ttl=300)\n\n# Retrieving it after 5 minutes will return None (or raise KeyError)\n\n\n\nIf you need to insert thousands of items (e.g., an initial data migration or bulk import), inserting them one-by-one is slow because each write is a separate database transaction.\nUse .batched() to buffer writes in memory and commit them in a single, high-speed transaction.\n# Insert 10,000 items efficiently\nwith settings.batched() as batch:\n    for i in range(10000):\n        batch[f\"key_{i}\"] = i\n\n\n\n\nBeaverDB offers a built-in Security Suite for Dictionaries, allowing you to store sensitive application secrets and user credentials securely.\n\n\nPass a secret (passphrase) when initializing the dictionary. This enables Encryption-at-Rest using AES-128 (Fernet). The keys remain visible (for lookup speed), but the values are completely encrypted on disk.\n# Initialize securely\n# If you lose the secret, the data is unrecoverable!\nvault = db.dict(\"api_keys\", secret=\"my-master-passphrase\")\n\n# Writes are encrypted before hitting the disk\nvault[\"openai\"] = \"sk-...\"\n\n# Reads are automatically decrypted\nprint(vault[\"openai\"])\n\n\n\nFor user passwords or authentication tokens, you should never store the plain text (even if encrypted). Use the Secret wrapper to store a One-Way Hash.\nBeaverDB automatically handles salting and PBKDF2 hashing for you.\nfrom beaver import Secret\n\nusers = db.dict(\"users\", secret=\"master-key\")\n\n# Create a user with a hashed password\n# The plain text \"correct-horse\" is hashed and discarded immediately.\nusers[\"alice\"] = {\n    \"role\": \"admin\",\n    \"password\": Secret(\"correct-horse\")\n}\n\n# Verify login\nstored_user = users[\"alice\"]\nif stored_user[\"password\"] == \"correct-horse\":\n    print(\"Login Successful\")"
  },
  {
    "objectID": "guide-dicts.html#quick-start",
    "href": "guide-dicts.html#quick-start",
    "title": "Dictionaries (Key-Value Store)",
    "section": "",
    "text": "Initialize a dictionary using db.dict(). If the dictionary doesn’t exist, it is created automatically.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"app.db\")\nsettings = db.dict(\"app_settings\")\n\n# 1. Write data (Persists immediately to SQLite)\nsettings[\"theme\"] = \"dark\"\nsettings[\"max_retries\"] = 5\nsettings[\"features\"] = {\"beta\": True, \"logging\": \"verbose\"}\n\n# 2. Read data\nprint(settings[\"theme\"])  # -&gt; \"dark\"\n\n# 3. Process-Safe\n# You can open this same dict in another script/process safely."
  },
  {
    "objectID": "guide-dicts.html#basic-operations",
    "href": "guide-dicts.html#basic-operations",
    "title": "Dictionaries (Key-Value Store)",
    "section": "",
    "text": "You can use standard bracket notation [] or the .get() method.\n# Set value\nsettings[\"user_1\"] = {\"name\": \"Alice\", \"score\": 42}\n\n# Get value (Raises KeyError if missing)\nuser = settings[\"user_1\"]\n\n# Get with default (Safe)\nuser = settings.get(\"user_99\", default={\"name\": \"Guest\"})\n\n\n\nUse the in operator to check if a key exists efficiently (O(1)).\nif \"user_1\" in settings:\n    print(\"User exists!\")\n\n\n\nRemove items using del or .pop().\n# Remove and return value\nold_val = settings.pop(\"user_1\", None)\n\n# Delete key (Raises KeyError if missing)\ndel settings[\"max_retries\"]\n\n# Clear entire dictionary\nsettings.clear()\n\n\n\nYou can iterate over keys, values, or items just like a standard dict. Note that for very large dictionaries (millions of items), this streams data from the database to avoid loading everything into memory at once.\nfor key in settings:\n    print(key)\n\nfor key, val in settings.items():\n    print(f\"{key}: {val}\")"
  },
  {
    "objectID": "guide-dicts.html#advanced-features",
    "href": "guide-dicts.html#advanced-features",
    "title": "Dictionaries (Key-Value Store)",
    "section": "",
    "text": "You can set keys that automatically expire after a certain duration. This is perfect for caching or temporary session tokens.\n# Set a key that expires in 300 seconds (5 minutes)\nsettings.set(\"session_id\", \"xyz_123\", ttl=300)\n\n# Retrieving it after 5 minutes will return None (or raise KeyError)\n\n\n\nIf you need to insert thousands of items (e.g., an initial data migration or bulk import), inserting them one-by-one is slow because each write is a separate database transaction.\nUse .batched() to buffer writes in memory and commit them in a single, high-speed transaction.\n# Insert 10,000 items efficiently\nwith settings.batched() as batch:\n    for i in range(10000):\n        batch[f\"key_{i}\"] = i"
  },
  {
    "objectID": "guide-dicts.html#security-encryption",
    "href": "guide-dicts.html#security-encryption",
    "title": "Dictionaries (Key-Value Store)",
    "section": "",
    "text": "BeaverDB offers a built-in Security Suite for Dictionaries, allowing you to store sensitive application secrets and user credentials securely.\n\n\nPass a secret (passphrase) when initializing the dictionary. This enables Encryption-at-Rest using AES-128 (Fernet). The keys remain visible (for lookup speed), but the values are completely encrypted on disk.\n# Initialize securely\n# If you lose the secret, the data is unrecoverable!\nvault = db.dict(\"api_keys\", secret=\"my-master-passphrase\")\n\n# Writes are encrypted before hitting the disk\nvault[\"openai\"] = \"sk-...\"\n\n# Reads are automatically decrypted\nprint(vault[\"openai\"])\n\n\n\nFor user passwords or authentication tokens, you should never store the plain text (even if encrypted). Use the Secret wrapper to store a One-Way Hash.\nBeaverDB automatically handles salting and PBKDF2 hashing for you.\nfrom beaver import Secret\n\nusers = db.dict(\"users\", secret=\"master-key\")\n\n# Create a user with a hashed password\n# The plain text \"correct-horse\" is hashed and discarded immediately.\nusers[\"alice\"] = {\n    \"role\": \"admin\",\n    \"password\": Secret(\"correct-horse\")\n}\n\n# Verify login\nstored_user = users[\"alice\"]\nif stored_user[\"password\"] == \"correct-horse\":\n    print(\"Login Successful\")"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to BeaverDB!\nIf you’ve ever found yourself building a Python application that needs to save some data, but setting up a full-blown database server felt like massive overkill, you’re in the right place. BeaverDB is designed to be the “Swiss Army knife” for embedded Python data. It’s built for those exact “just right” scenarios: more powerful than a simple pickle file, but far less complex than a networked server like PostgreSQL or MySQL.\n\n\nAt its heart, BeaverDB is a multi-modal database in a single SQLite file.\nIt’s a Python library that lets you manage modern, complex data types without ever leaving the comfort of a local file. The name itself tells the story:\nB.E.A.V.E.R. stands for Backend for mbedded, All-in-one Vector, Entity, and Relationship storage.\nThis means that inside that one .db file, you can seamlessly store and query:\n\nKey-Value Pairs (like a dictionary for your app’s configuration)\nLists (like a persistent to-do list)\nVector Embeddings (for AI and semantic search)\nDocuments & Text (for full-text search)\nGraph Relationships (to connect your data together)\n…and much more.\n\nAll this power comes from building on top of the world’s most deployed database engine: SQLite.\n\n\n\nBeaverDB is built on a few core principles. Understanding these will help you know when and why to choose it for your project.\nRobust, Safe, and Durable\nYour data should be safe, period. BeaverDB is built to be resilient. Thanks to SQLite’s atomic transactions and Write-Ahead Logging (WAL) mode, your database is crash-safe. If your program crashes mid-operation, your data is never lost or corrupted; the database simply rolls back the incomplete transaction.\nFurthermore, it’s designed for concurrency. It’s both thread-safe (different threads can share one BeaverDB object) and process-safe (multiple, independent Python scripts can read from and write to the same database file at the same time). For tasks that require true coordination, it even provides a simple, built-in distributed lock.\nPerformant by Default\nBeaverDB is fast. It’s not just “fast for a small database”–it’s genuinely fast for the vast majority of medium-sized projects. Because it’s an embedded library, there is zero network latency for any query.\nLet’s be clear: if you’re building the next X (formerly Twitter) and need to handle millions of documents and thousands of queries per second, you’ll need a distributed, networked database. But for almost everything else? BeaverDB is more than fast enough. If your project is in the thousand to tens-of-thousands of documents range, you’ll find it’s incredibly responsive.\nLocal-First & Embedded\nThe default, primary way to use BeaverDB is as a single file right next to your code. This means your entire database—users, vectors, chat logs, and all—is contained in one portable file (e.g., my_app.db). You can copy it, email it, or back it up. This “local-first” approach is what makes it so fast and simple to deploy.\nMinimal & Optional Dependencies\nThe core BeaverDB library has zero external dependencies. You can get started with key-value stores, lists, and queues right away.\nWant to add vector search? Great! Install the [vector] extra, and BeaverDB will activate its faiss integration. Need a web server? Install the [server] extra, and it unlocks a fastapi-based REST API. This “pay-as-you-go” approach keeps your project lightweight.\nPythonic API\nBeaverDB is designed to feel like you’re just using standard Python data structures. You shouldn’t have to write complex SQL queries just to save a Python dict or list. The goal is to make the database feel like a natural extension of your code.\nStandard SQLite Compatibility\nThis is the “no-magic” rule. The my_app.db file that BeaverDB creates is a 100% standard, valid SQLite file. You can open it with any database tool (like DB Browser for SQLite) and see your data in regular tables. This ensures your data is never locked into a proprietary format.\nSynchronous Core with Async Potential\nThe core library is synchronous, which makes it simple and robust for multi-threaded applications. However, BeaverDB is fully aware of the modern asyncio world. For every data structure, you can call .as_async() to get a fully awaitable version that runs its blocking operations in a background thread, keeping your asyncio event loop from getting blocked.\n\n\n\nBeaverDB shines in scenarios where simplicity, robustness, and local performance are more important than massive, web-scale concurrency.\n\nLocal AI & RAG: Perfect for building Retrieval-Augmented Generation (RAG) applications that run on your local machine. You can store your vector embeddings and their corresponding text right next to each other.\nDesktop Utilities & CLI Tools: The ideal companion for a custom tool that needs to remember user preferences, manage a history, or cache results.\nChatbots: A persistent list is a perfect, simple way to store a chatbot’s conversation history for a user.\nRapid Prototyping: Get your idea up and running in minutes. Start with a local .db file, and if your project grows, you can deploy it as a REST API without changing your application logic.\n\n\n\n\nWe’ve designed this documentation to get you the information you need, whether you’re building your first script or contributing to the core.\nThis guide is split into two main parts:\n\nPart 1: The User Guide\nThis is your starting point. After the Quickstart, this is where you’ll find an in-depth guide that walks you through how to use every single feature of BeaverDB. We’ll explore each modality one by one with practical examples.\nPart 2: The Developer Guide\nThis part is for power users and contributors. We’ll go under the hood to look at the why behind the design. This is where we do deep dives into the core architecture, the concurrency model (threading and locking), and the internals of how features like vector search are implemented."
  },
  {
    "objectID": "intro.html#what-is-beaverdb",
    "href": "intro.html#what-is-beaverdb",
    "title": "Introduction",
    "section": "",
    "text": "At its heart, BeaverDB is a multi-modal database in a single SQLite file.\nIt’s a Python library that lets you manage modern, complex data types without ever leaving the comfort of a local file. The name itself tells the story:\nB.E.A.V.E.R. stands for Backend for mbedded, All-in-one Vector, Entity, and Relationship storage.\nThis means that inside that one .db file, you can seamlessly store and query:\n\nKey-Value Pairs (like a dictionary for your app’s configuration)\nLists (like a persistent to-do list)\nVector Embeddings (for AI and semantic search)\nDocuments & Text (for full-text search)\nGraph Relationships (to connect your data together)\n…and much more.\n\nAll this power comes from building on top of the world’s most deployed database engine: SQLite."
  },
  {
    "objectID": "intro.html#the-beaverdb-philosophy",
    "href": "intro.html#the-beaverdb-philosophy",
    "title": "Introduction",
    "section": "",
    "text": "BeaverDB is built on a few core principles. Understanding these will help you know when and why to choose it for your project.\nRobust, Safe, and Durable\nYour data should be safe, period. BeaverDB is built to be resilient. Thanks to SQLite’s atomic transactions and Write-Ahead Logging (WAL) mode, your database is crash-safe. If your program crashes mid-operation, your data is never lost or corrupted; the database simply rolls back the incomplete transaction.\nFurthermore, it’s designed for concurrency. It’s both thread-safe (different threads can share one BeaverDB object) and process-safe (multiple, independent Python scripts can read from and write to the same database file at the same time). For tasks that require true coordination, it even provides a simple, built-in distributed lock.\nPerformant by Default\nBeaverDB is fast. It’s not just “fast for a small database”–it’s genuinely fast for the vast majority of medium-sized projects. Because it’s an embedded library, there is zero network latency for any query.\nLet’s be clear: if you’re building the next X (formerly Twitter) and need to handle millions of documents and thousands of queries per second, you’ll need a distributed, networked database. But for almost everything else? BeaverDB is more than fast enough. If your project is in the thousand to tens-of-thousands of documents range, you’ll find it’s incredibly responsive.\nLocal-First & Embedded\nThe default, primary way to use BeaverDB is as a single file right next to your code. This means your entire database—users, vectors, chat logs, and all—is contained in one portable file (e.g., my_app.db). You can copy it, email it, or back it up. This “local-first” approach is what makes it so fast and simple to deploy.\nMinimal & Optional Dependencies\nThe core BeaverDB library has zero external dependencies. You can get started with key-value stores, lists, and queues right away.\nWant to add vector search? Great! Install the [vector] extra, and BeaverDB will activate its faiss integration. Need a web server? Install the [server] extra, and it unlocks a fastapi-based REST API. This “pay-as-you-go” approach keeps your project lightweight.\nPythonic API\nBeaverDB is designed to feel like you’re just using standard Python data structures. You shouldn’t have to write complex SQL queries just to save a Python dict or list. The goal is to make the database feel like a natural extension of your code.\nStandard SQLite Compatibility\nThis is the “no-magic” rule. The my_app.db file that BeaverDB creates is a 100% standard, valid SQLite file. You can open it with any database tool (like DB Browser for SQLite) and see your data in regular tables. This ensures your data is never locked into a proprietary format.\nSynchronous Core with Async Potential\nThe core library is synchronous, which makes it simple and robust for multi-threaded applications. However, BeaverDB is fully aware of the modern asyncio world. For every data structure, you can call .as_async() to get a fully awaitable version that runs its blocking operations in a background thread, keeping your asyncio event loop from getting blocked."
  },
  {
    "objectID": "intro.html#ideal-use-cases",
    "href": "intro.html#ideal-use-cases",
    "title": "Introduction",
    "section": "",
    "text": "BeaverDB shines in scenarios where simplicity, robustness, and local performance are more important than massive, web-scale concurrency.\n\nLocal AI & RAG: Perfect for building Retrieval-Augmented Generation (RAG) applications that run on your local machine. You can store your vector embeddings and their corresponding text right next to each other.\nDesktop Utilities & CLI Tools: The ideal companion for a custom tool that needs to remember user preferences, manage a history, or cache results.\nChatbots: A persistent list is a perfect, simple way to store a chatbot’s conversation history for a user.\nRapid Prototyping: Get your idea up and running in minutes. Start with a local .db file, and if your project grows, you can deploy it as a REST API without changing your application logic."
  },
  {
    "objectID": "intro.html#how-this-guide-is-structured",
    "href": "intro.html#how-this-guide-is-structured",
    "title": "Introduction",
    "section": "",
    "text": "We’ve designed this documentation to get you the information you need, whether you’re building your first script or contributing to the core.\nThis guide is split into two main parts:\n\nPart 1: The User Guide\nThis is your starting point. After the Quickstart, this is where you’ll find an in-depth guide that walks you through how to use every single feature of BeaverDB. We’ll explore each modality one by one with practical examples.\nPart 2: The Developer Guide\nThis part is for power users and contributors. We’ll go under the hood to look at the why behind the design. This is where we do deep dives into the core architecture, the concurrency model (threading and locking), and the internals of how features like vector search are implemented."
  },
  {
    "objectID": "guide-sketches.html",
    "href": "guide-sketches.html",
    "title": "Sketches (Probabilistic Sets)",
    "section": "",
    "text": "The SketchManager introduces a powerful “Big Data” capability to BeaverDB: Probabilistic Data Structures.\nUnlike a standard set which stores every item you add (consuming more memory as it grows), a Sketch stores a mathematical summary of the data. It allows you to track the Presence and Cardinality (Count) of millions of items using a tiny, constant amount of memory (often just a few kilobytes).\nThe trade-off is that it provides approximate answers rather than perfect ones, but for many analytics and caching use cases, this is a perfect compromise.\n\n\nInitialize a sketch using db.sketch(). You define the expected capacity and acceptable error rate, and BeaverDB calculates the optimal storage size.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"analytics.db\")\n\n# Create a sketch for tracking unique visitors\n# Capacity: 1 million items\n# Error Rate: 1% (0.01)\n# Storage: ~1MB fixed size\nvisitors = db.sketch(\"daily_visitors\", capacity=1_000_000, error_rate=0.01)\n\n# 1. Add Data\nvisitors.add(\"192.168.1.1\")\nvisitors.add(\"10.0.0.5\")\n\n# 2. Check Membership (Bloom Filter)\nif \"192.168.1.1\" in visitors:\n    print(\"Returning visitor\")\n\n# 3. Count Uniques (HyperLogLog)\nprint(f\"Total unique visitors: {len(visitors)}\")\n\n\n\nBeaverDB implements a unified ApproximateSet that combines two best-in-class algorithms:\n\nBloom Filter: Used for __contains__ checks. It never gives false negatives (if it says “No”, the item is definitely not there). It might give false positives (saying “Yes” when the item wasn’t added) at the rate you configured (e.g., 1%).\nHyperLogLog (HLL): Used for __len__ counts. It estimates the number of unique elements with a standard error of roughly 0.8% - 2%, regardless of how many billions of items you add.\n\nThese are packed into a single binary BLOB in the database, making them extremely efficient to load and save.\n\n\n\n\n\nUse .add(item) to insert a string or bytes. This updates both the Bloom Filter and the HLL counters.\nvisitors.add(\"user_123\")\n\n\n\nUse the in operator. This queries the Bloom Filter part of the sketch.\n# \"Have we processed this URL before?\"\nif url in crawler_history:\n    skip_url(url)\n\n\n\nUse len() to get the approximate count of unique items added. This queries the HyperLogLog part.\n# \"How many distinct words are in this book?\"\ncount = len(word_sketch)\n\n\n\n\n\n\nUpdating a sketch requires a Read-Modify-Write cycle on the BLOB. Doing this for every single item in a loop is slow due to locking overhead.\nFor high-throughput scenarios (like ingesting server logs), always use .batched(). This buffers updates in memory and performs a single atomic merge with the database.\n# Ingest 10,000 log entries efficiently\nwith visitors.batched() as batch:\n    for ip in access_logs:\n        batch.add(ip)\n\n\n\nBeaverDB enforces strict configuration matching. If you create a sketch with capacity=1M and later try to open it with capacity=500k, it will raise a ValueError to prevent data corruption.\n# Valid: Open with same parameters\nv = db.sketch(\"visitors\", capacity=1_000_000, error_rate=0.01)\n\n# Invalid: Raises ValueError\nv = db.sketch(\"visitors\", capacity=500_000)"
  },
  {
    "objectID": "guide-sketches.html#quick-start",
    "href": "guide-sketches.html#quick-start",
    "title": "Sketches (Probabilistic Sets)",
    "section": "",
    "text": "Initialize a sketch using db.sketch(). You define the expected capacity and acceptable error rate, and BeaverDB calculates the optimal storage size.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"analytics.db\")\n\n# Create a sketch for tracking unique visitors\n# Capacity: 1 million items\n# Error Rate: 1% (0.01)\n# Storage: ~1MB fixed size\nvisitors = db.sketch(\"daily_visitors\", capacity=1_000_000, error_rate=0.01)\n\n# 1. Add Data\nvisitors.add(\"192.168.1.1\")\nvisitors.add(\"10.0.0.5\")\n\n# 2. Check Membership (Bloom Filter)\nif \"192.168.1.1\" in visitors:\n    print(\"Returning visitor\")\n\n# 3. Count Uniques (HyperLogLog)\nprint(f\"Total unique visitors: {len(visitors)}\")"
  },
  {
    "objectID": "guide-sketches.html#how-it-works",
    "href": "guide-sketches.html#how-it-works",
    "title": "Sketches (Probabilistic Sets)",
    "section": "",
    "text": "BeaverDB implements a unified ApproximateSet that combines two best-in-class algorithms:\n\nBloom Filter: Used for __contains__ checks. It never gives false negatives (if it says “No”, the item is definitely not there). It might give false positives (saying “Yes” when the item wasn’t added) at the rate you configured (e.g., 1%).\nHyperLogLog (HLL): Used for __len__ counts. It estimates the number of unique elements with a standard error of roughly 0.8% - 2%, regardless of how many billions of items you add.\n\nThese are packed into a single binary BLOB in the database, making them extremely efficient to load and save."
  },
  {
    "objectID": "guide-sketches.html#basic-operations",
    "href": "guide-sketches.html#basic-operations",
    "title": "Sketches (Probabilistic Sets)",
    "section": "",
    "text": "Use .add(item) to insert a string or bytes. This updates both the Bloom Filter and the HLL counters.\nvisitors.add(\"user_123\")\n\n\n\nUse the in operator. This queries the Bloom Filter part of the sketch.\n# \"Have we processed this URL before?\"\nif url in crawler_history:\n    skip_url(url)\n\n\n\nUse len() to get the approximate count of unique items added. This queries the HyperLogLog part.\n# \"How many distinct words are in this book?\"\ncount = len(word_sketch)"
  },
  {
    "objectID": "guide-sketches.html#advanced-features",
    "href": "guide-sketches.html#advanced-features",
    "title": "Sketches (Probabilistic Sets)",
    "section": "",
    "text": "Updating a sketch requires a Read-Modify-Write cycle on the BLOB. Doing this for every single item in a loop is slow due to locking overhead.\nFor high-throughput scenarios (like ingesting server logs), always use .batched(). This buffers updates in memory and performs a single atomic merge with the database.\n# Ingest 10,000 log entries efficiently\nwith visitors.batched() as batch:\n    for ip in access_logs:\n        batch.add(ip)\n\n\n\nBeaverDB enforces strict configuration matching. If you create a sketch with capacity=1M and later try to open it with capacity=500k, it will raise a ValueError to prevent data corruption.\n# Valid: Open with same parameters\nv = db.sketch(\"visitors\", capacity=1_000_000, error_rate=0.01)\n\n# Invalid: Raises ValueError\nv = db.sketch(\"visitors\", capacity=500_000)"
  },
  {
    "objectID": "guide-blobs.html",
    "href": "guide-blobs.html",
    "title": "Blobs (File Store)",
    "section": "",
    "text": "The BlobManager is designed for storing binary large objects (BLOBs) like images, PDFs, audio files, or zipped archives directly within the database.\nWhile storing large files in a database is often discouraged in server-based systems (due to backup bloat), for a local-first application, it is often the correct choice. It keeps your data portable: a single .db file contains your entire application state, including assets, making backup and migration trivial.\n\n\nInitialize a blob store using db.blobs().\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"app.db\")\nassets = db.blobs(\"user_assets\")\n\n# 1. Store a file\n# You must pass 'bytes' to the put method.\nwith open(\"avatar.png\", \"rb\") as f:\n    assets.put(\"user_1_avatar.png\", f.read())\n\n# 2. Retrieve a file\ndata = assets.get(\"user_1_avatar.png\")\n\nif data:\n    with open(\"downloaded_avatar.png\", \"wb\") as f:\n        f.write(data)\n\n\n\n\n\nUse .put(key, data) to save content. The data argument must be of type bytes.\n# Storing text as a blob (encode first)\nassets.put(\"notes.txt\", \"Hello World\".encode(\"utf-8\"))\n\n\n\nUse .get(key) to retrieve the data. It returns bytes or None if the key does not exist.\ndata = assets.get(\"notes.txt\")\nif data:\n    print(data.decode(\"utf-8\"))\n\n\n\nYou can attach a JSON-serializable dictionary to any blob. This is useful for storing MIME types, original filenames, or upload timestamps without creating a separate dictionary entry.\n# Store image with metadata\nassets.put(\n    \"photo_001.jpg\",\n    image_bytes,\n    metadata={\"content_type\": \"image/jpeg\", \"owner\": \"alice\"}\n)\n\n# Retrieve metadata only (Fast)\n# This avoids loading the full file content into memory.\nmeta = assets.metadata(\"photo_001.jpg\")\nprint(meta[\"content_type\"]) # -&gt; \"image/jpeg\"\n\n\n\nRemove blobs using .delete(key).\nassets.delete(\"photo_001.jpg\")\n\n\n\nYou can iterate over all keys in the blob store.\n# List all files\nfor filename in assets:\n    print(f\"Found file: {filename}\")\n\n\n\n\n\n\nInserting many small files one by one can be slow due to transaction overhead. Use .batched() to upload a directory of files in a single transaction.\n\nWarning: The batch is buffered in memory. Do not batch 100 video files at once, or you will run out of RAM. Use this for groups of small icons, documents, or config files.\n\nimport os\n\n# Bulk upload all icons\nwith assets.batched() as batch:\n    for filename in os.listdir(\"./icons\"):\n        with open(f\"./icons/{filename}\", \"rb\") as f:\n            batch.put(filename, f.read())"
  },
  {
    "objectID": "guide-blobs.html#quick-start",
    "href": "guide-blobs.html#quick-start",
    "title": "Blobs (File Store)",
    "section": "",
    "text": "Initialize a blob store using db.blobs().\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"app.db\")\nassets = db.blobs(\"user_assets\")\n\n# 1. Store a file\n# You must pass 'bytes' to the put method.\nwith open(\"avatar.png\", \"rb\") as f:\n    assets.put(\"user_1_avatar.png\", f.read())\n\n# 2. Retrieve a file\ndata = assets.get(\"user_1_avatar.png\")\n\nif data:\n    with open(\"downloaded_avatar.png\", \"wb\") as f:\n        f.write(data)"
  },
  {
    "objectID": "guide-blobs.html#basic-operations",
    "href": "guide-blobs.html#basic-operations",
    "title": "Blobs (File Store)",
    "section": "",
    "text": "Use .put(key, data) to save content. The data argument must be of type bytes.\n# Storing text as a blob (encode first)\nassets.put(\"notes.txt\", \"Hello World\".encode(\"utf-8\"))\n\n\n\nUse .get(key) to retrieve the data. It returns bytes or None if the key does not exist.\ndata = assets.get(\"notes.txt\")\nif data:\n    print(data.decode(\"utf-8\"))\n\n\n\nYou can attach a JSON-serializable dictionary to any blob. This is useful for storing MIME types, original filenames, or upload timestamps without creating a separate dictionary entry.\n# Store image with metadata\nassets.put(\n    \"photo_001.jpg\",\n    image_bytes,\n    metadata={\"content_type\": \"image/jpeg\", \"owner\": \"alice\"}\n)\n\n# Retrieve metadata only (Fast)\n# This avoids loading the full file content into memory.\nmeta = assets.metadata(\"photo_001.jpg\")\nprint(meta[\"content_type\"]) # -&gt; \"image/jpeg\"\n\n\n\nRemove blobs using .delete(key).\nassets.delete(\"photo_001.jpg\")\n\n\n\nYou can iterate over all keys in the blob store.\n# List all files\nfor filename in assets:\n    print(f\"Found file: {filename}\")"
  },
  {
    "objectID": "guide-blobs.html#advanced-features",
    "href": "guide-blobs.html#advanced-features",
    "title": "Blobs (File Store)",
    "section": "",
    "text": "Inserting many small files one by one can be slow due to transaction overhead. Use .batched() to upload a directory of files in a single transaction.\n\nWarning: The batch is buffered in memory. Do not batch 100 video files at once, or you will run out of RAM. Use this for groups of small icons, documents, or config files.\n\nimport os\n\n# Bulk upload all icons\nwith assets.batched() as batch:\n    for filename in os.listdir(\"./icons\"):\n        with open(f\"./icons/{filename}\", \"rb\") as f:\n            batch.put(filename, f.read())"
  },
  {
    "objectID": "guide-queues.html",
    "href": "guide-queues.html",
    "title": "Queues (Priority & Jobs)",
    "section": "",
    "text": "The QueueManager provides a persistent, multi-process Priority Queue.\nUnlike a standard list or FIFO queue, items in a BeaverDB queue are retrieved based on their Priority Score. This makes it ideal for building job queues, task schedulers, or message brokers where some tasks (like “send urgent email”) must be processed before others (like “generate weekly report”).\n\n\nInitialize a queue using db.queue().\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"jobs.db\")\nqueue = db.queue(\"email_jobs\")\n\n# 1. Producer: Add tasks with priority\n# Priority 1.0 (High) comes before Priority 10.0 (Low)\nqueue.put({\"to\": \"admin@example.com\", \"subject\": \"System Alert\"}, priority=1.0)\nqueue.put({\"to\": \"user@example.com\", \"subject\": \"Weekly Update\"}, priority=10.0)\n\n# 2. Consumer: Process tasks\n# This blocks until an item is available\njob = queue.get()\n\nprint(f\"Processing: {job.data['subject']}\")\n# -&gt; \"System Alert\" (because 1.0 &lt; 10.0)\n\n\n\n\n\nUse .put(data, priority) to add an item.\n\nData: Any JSON-serializable object (dict, list, string, int).\nPriority: A float. Lower numbers = Higher priority.\nFIFO Behavior: Items with the same priority are retrieved in FIFO order (First-In, First-Out).\n\n\nqueue.put(\"Task A\", priority=5)\nqueue.put(\"Task B\", priority=5)\n# Task A will be retrieved before Task B\n\n\n\nThe .get() method is atomic and process-safe. It retrieves and removes the highest-priority item.\n# Blocking Get (Waits forever until item arrives)\nitem = queue.get()\n\n# Non-Blocking Get (Raises IndexError if empty)\ntry:\n    item = queue.get(block=False)\nexcept IndexError:\n    print(\"Queue is empty\")\n\n# Timeout (Waits 5 seconds, then raises TimeoutError)\ntry:\n    item = queue.get(timeout=5.0)\nexcept TimeoutError:\n    print(\"No jobs arrived in 5 seconds\")\n\n\n\nUse .peek() to see the highest-priority item without removing it.\nnext_job = queue.peek()\nif next_job:\n    print(f\"Next up: {next_job.data}\")\n\n\n\nprint(f\"Pending jobs: {len(queue)}\")\n\n\n\n\n\n\nYou can run multiple Python scripts (workers) consuming from the same queue concurrently. BeaverDB uses SQLite locking to ensure that each job is delivered to exactly one worker.\n# worker.py\nwhile True:\n    try:\n        # Wait for work\n        job = queue.get()\n        process(job.data)\n    except Exception as e:\n        print(f\"Worker crashed: {e}\")\n\n\n\nWhen you retrieve an item, you get a QueueItem named tuple containing metadata:\n\ndata: The payload you stored.\npriority: The float priority.\ntimestamp: When the item was added (used for FIFO tie-breaking).\n\n\nitem = queue.get()\nprint(f\"Processing {item.data} (Priority: {item.priority}, Added at: {item.timestamp})\")"
  },
  {
    "objectID": "guide-queues.html#quick-start",
    "href": "guide-queues.html#quick-start",
    "title": "Queues (Priority & Jobs)",
    "section": "",
    "text": "Initialize a queue using db.queue().\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"jobs.db\")\nqueue = db.queue(\"email_jobs\")\n\n# 1. Producer: Add tasks with priority\n# Priority 1.0 (High) comes before Priority 10.0 (Low)\nqueue.put({\"to\": \"admin@example.com\", \"subject\": \"System Alert\"}, priority=1.0)\nqueue.put({\"to\": \"user@example.com\", \"subject\": \"Weekly Update\"}, priority=10.0)\n\n# 2. Consumer: Process tasks\n# This blocks until an item is available\njob = queue.get()\n\nprint(f\"Processing: {job.data['subject']}\")\n# -&gt; \"System Alert\" (because 1.0 &lt; 10.0)"
  },
  {
    "objectID": "guide-queues.html#basic-operations",
    "href": "guide-queues.html#basic-operations",
    "title": "Queues (Priority & Jobs)",
    "section": "",
    "text": "Use .put(data, priority) to add an item.\n\nData: Any JSON-serializable object (dict, list, string, int).\nPriority: A float. Lower numbers = Higher priority.\nFIFO Behavior: Items with the same priority are retrieved in FIFO order (First-In, First-Out).\n\n\nqueue.put(\"Task A\", priority=5)\nqueue.put(\"Task B\", priority=5)\n# Task A will be retrieved before Task B\n\n\n\nThe .get() method is atomic and process-safe. It retrieves and removes the highest-priority item.\n# Blocking Get (Waits forever until item arrives)\nitem = queue.get()\n\n# Non-Blocking Get (Raises IndexError if empty)\ntry:\n    item = queue.get(block=False)\nexcept IndexError:\n    print(\"Queue is empty\")\n\n# Timeout (Waits 5 seconds, then raises TimeoutError)\ntry:\n    item = queue.get(timeout=5.0)\nexcept TimeoutError:\n    print(\"No jobs arrived in 5 seconds\")\n\n\n\nUse .peek() to see the highest-priority item without removing it.\nnext_job = queue.peek()\nif next_job:\n    print(f\"Next up: {next_job.data}\")\n\n\n\nprint(f\"Pending jobs: {len(queue)}\")"
  },
  {
    "objectID": "guide-queues.html#advanced-patterns",
    "href": "guide-queues.html#advanced-patterns",
    "title": "Queues (Priority & Jobs)",
    "section": "",
    "text": "You can run multiple Python scripts (workers) consuming from the same queue concurrently. BeaverDB uses SQLite locking to ensure that each job is delivered to exactly one worker.\n# worker.py\nwhile True:\n    try:\n        # Wait for work\n        job = queue.get()\n        process(job.data)\n    except Exception as e:\n        print(f\"Worker crashed: {e}\")\n\n\n\nWhen you retrieve an item, you get a QueueItem named tuple containing metadata:\n\ndata: The payload you stored.\npriority: The float priority.\ntimestamp: When the item was added (used for FIFO tie-breaking).\n\n\nitem = queue.get()\nprint(f\"Processing {item.data} (Priority: {item.priority}, Added at: {item.timestamp})\")"
  },
  {
    "objectID": "dev-search.html",
    "href": "dev-search.html",
    "title": "Search Internals",
    "section": "",
    "text": "The CollectionManager provides a unified search interface, but under the hood, it orchestrates three distinct query engines: 1. Vector Engine: For semantic similarity (dense retrieval). 2. FTS Engine: For keyword matching (sparse retrieval). 3. Graph Engine: For structural traversal (relational retrieval).\nThis document details how each engine is implemented.\n\n\nThe vector search implementation (beaver.vectors.NumpyVectorIndex) is designed to be Crash-Safe, Persistent, and Fast.\nTo achieve this without an external vector database, BeaverDB uses a Hybrid Snapshot + Delta Log architecture.\n\n\n\nNaive Approach: Store vectors in SQLite rows. SELECT + compute distance in Python.\n\nProblem: Too slow. Serialization overhead for 100k vectors is massive.\n\nMemory Approach: Load all vectors into a numpy matrix at startup.\n\nProblem: Startup time is slow. Syncing writes across processes is hard.\n\n\n\n\n\nWe maintain two structures: 1. Base Index (Snapshot): A serialized, memory-mapped binary blob containing a pre-computed numpy matrix of vectors up to time \\(T\\). 2. Delta Log (Append-Only): A standard SQLite table (beaver_vector_change_log) recording every insertion and deletion since time \\(T\\).\n\n\n\n\nStartup:\n\nThe manager loads the Base Index (fast binary load).\nIt queries the Delta Log for changes after the snapshot’s version.\nIt applies these changes to an in-memory “Delta Buffer”.\n\nSearch (O(N)):\n\nThe query vector is compared against the Base Matrix (using BLAS/SIMD).\nThe query vector is compared against the Delta Buffer.\nResults are merged and sorted.\nDeleted items (Tombstones) are filtered out.\n\nWrite:\n\nNew vectors are simply INSERTed into the Delta Log. This is atomic and fast.\n\nCompaction:\n\nWhen the Delta Log grows too large (&gt;100 items), a background thread merges the log into the Base Matrix and creates a new Snapshot.\n\n\n\n\n\n\nBeaverDB provides robust text search using a two-layered approach.\n\n\nWe use SQLite’s built-in FTS5 virtual table (beaver_fts_index). * Tokenization: Uses the porter stemmer (e.g., “running” -&gt; “run”). * Storage: Maps (collection, item_id) to a bag-of-words index. * Querying: Supports fast boolean logic (python AND NOT java).\n\n\n\nFTS5 fails on typos (“pytohn” won’t match “python”). To solve this, we implement a Trigram Index (beaver_trigrams). * Ingestion: The string “hello” is broken into hel, ell, llo. * Storage: Rows in beaver_trigrams map these 3-char chunks to document IDs. * Retrieval: 1. Break query “helo” -&gt; hel, elo. 2. Find IDs that contain a high percentage of these trigrams (Candidate Generation). 3. Compute Levenshtein Distance on these candidates only (Verification).\n\n\n\n\nThe graph capabilities rely on the beaver_edges table, which stores directed, labeled edges.\nTo avoid the “N+1 Select Problem” (fetching a node, then fetching its neighbors in a loop), BeaverDB uses SQL-Native Recursion.\n\n\nThe .walk() and .expand() methods generate a single SQL query that executes the Breadth-First Search (BFS) entirely within the database engine.\nWITH RECURSIVE bfs(item_id, current_depth) AS (\n    -- 1. Start at the source node\n    SELECT target_item_id, 1\n    FROM beaver_edges\n    WHERE source_item_id = ?\n\n    UNION ALL\n\n    -- 2. Recursively find neighbors\n    SELECT edges.target_item_id, bfs.current_depth + 1\n    FROM beaver_edges AS edges\n    JOIN bfs ON edges.source_item_id = bfs.item_id\n    WHERE bfs.current_depth &lt; ?\n)\nSELECT DISTINCT item_id FROM bfs;\nThis approach allows retrieving thousands of connected nodes in milliseconds without round-tripping data to Python."
  },
  {
    "objectID": "dev-search.html#vector-engine-hybrid-architecture",
    "href": "dev-search.html#vector-engine-hybrid-architecture",
    "title": "Search Internals",
    "section": "",
    "text": "The vector search implementation (beaver.vectors.NumpyVectorIndex) is designed to be Crash-Safe, Persistent, and Fast.\nTo achieve this without an external vector database, BeaverDB uses a Hybrid Snapshot + Delta Log architecture.\n\n\n\nNaive Approach: Store vectors in SQLite rows. SELECT + compute distance in Python.\n\nProblem: Too slow. Serialization overhead for 100k vectors is massive.\n\nMemory Approach: Load all vectors into a numpy matrix at startup.\n\nProblem: Startup time is slow. Syncing writes across processes is hard.\n\n\n\n\n\nWe maintain two structures: 1. Base Index (Snapshot): A serialized, memory-mapped binary blob containing a pre-computed numpy matrix of vectors up to time \\(T\\). 2. Delta Log (Append-Only): A standard SQLite table (beaver_vector_change_log) recording every insertion and deletion since time \\(T\\).\n\n\n\n\nStartup:\n\nThe manager loads the Base Index (fast binary load).\nIt queries the Delta Log for changes after the snapshot’s version.\nIt applies these changes to an in-memory “Delta Buffer”.\n\nSearch (O(N)):\n\nThe query vector is compared against the Base Matrix (using BLAS/SIMD).\nThe query vector is compared against the Delta Buffer.\nResults are merged and sorted.\nDeleted items (Tombstones) are filtered out.\n\nWrite:\n\nNew vectors are simply INSERTed into the Delta Log. This is atomic and fast.\n\nCompaction:\n\nWhen the Delta Log grows too large (&gt;100 items), a background thread merges the log into the Base Matrix and creates a new Snapshot."
  },
  {
    "objectID": "dev-search.html#text-search-engine-fts5-trigrams",
    "href": "dev-search.html#text-search-engine-fts5-trigrams",
    "title": "Search Internals",
    "section": "",
    "text": "BeaverDB provides robust text search using a two-layered approach.\n\n\nWe use SQLite’s built-in FTS5 virtual table (beaver_fts_index). * Tokenization: Uses the porter stemmer (e.g., “running” -&gt; “run”). * Storage: Maps (collection, item_id) to a bag-of-words index. * Querying: Supports fast boolean logic (python AND NOT java).\n\n\n\nFTS5 fails on typos (“pytohn” won’t match “python”). To solve this, we implement a Trigram Index (beaver_trigrams). * Ingestion: The string “hello” is broken into hel, ell, llo. * Storage: Rows in beaver_trigrams map these 3-char chunks to document IDs. * Retrieval: 1. Break query “helo” -&gt; hel, elo. 2. Find IDs that contain a high percentage of these trigrams (Candidate Generation). 3. Compute Levenshtein Distance on these candidates only (Verification)."
  },
  {
    "objectID": "dev-search.html#graph-engine-recursive-ctes",
    "href": "dev-search.html#graph-engine-recursive-ctes",
    "title": "Search Internals",
    "section": "",
    "text": "The graph capabilities rely on the beaver_edges table, which stores directed, labeled edges.\nTo avoid the “N+1 Select Problem” (fetching a node, then fetching its neighbors in a loop), BeaverDB uses SQL-Native Recursion.\n\n\nThe .walk() and .expand() methods generate a single SQL query that executes the Breadth-First Search (BFS) entirely within the database engine.\nWITH RECURSIVE bfs(item_id, current_depth) AS (\n    -- 1. Start at the source node\n    SELECT target_item_id, 1\n    FROM beaver_edges\n    WHERE source_item_id = ?\n\n    UNION ALL\n\n    -- 2. Recursively find neighbors\n    SELECT edges.target_item_id, bfs.current_depth + 1\n    FROM beaver_edges AS edges\n    JOIN bfs ON edges.source_item_id = bfs.item_id\n    WHERE bfs.current_depth &lt; ?\n)\nSELECT DISTINCT item_id FROM bfs;\nThis approach allows retrieving thousands of connected nodes in milliseconds without round-tripping data to Python."
  },
  {
    "objectID": "dev-contributing.html",
    "href": "dev-contributing.html",
    "title": "Contributing to BeaverDB",
    "section": "",
    "text": "BeaverDB follows a “Simplicity First” philosophy not just in its API, but also in its development workflow. We minimize tooling overhead and prefer standard Python ecosystem tools.\n\n\nTo contribute, you’ll need Python 3.13+. We recommend using a virtual environment.\n# 1. Clone the repository\ngit clone https://github.com/syalia-srl/beaver.git\ncd beaver\n\n# 2. Create a virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\n\n# 3. Install in editable mode with ALL dependencies (dev, server, vector, graph)\nuv sync\n\n\n\nWe use a makefile to automate common tasks. You don’t need to memorize complex commands.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nmake test-unit\nRuns fast unit tests (Default). Use this during local dev.\n\n\nmake test-all\nRuns all tests, including slow integration and concurrency tests.\n\n\nmake format\nRuns black to format code and commits the changes.\n\n\nmake docs\nPublishes the documentation to GitHub Pages (requires permissions).\n\n\nmake issues\nSyncs local markdown issues with GitHub Issues (see below).\n\n\n\n\n\n\nBeaverDB uses a unique “Docs-as-Code” approach for issue tracking using the gh-md-issues extension.\n\nIssues are Files: Every issue exists as a markdown file in the issues/ directory (e.g., issues/31-composable-graph-rag.md).\nSyncing: We edit these files locally to draft feature plans or update status. Running make issues pushes these changes to GitHub Issues and pulls any comments/updates back to your local files.\n\nWorkflow:\n\nCreate a new file issues/New-Feature.md.\nWrite the frontmatter (title, labels, state: open).\nRun make issues to create it on GitHub.\n\n\n\n\nOur test suite is divided into strict categories to keep the “Feedback Loop” fast. We use pytest markers defined in pytest.ini.\n\n\n\nGoal: Instant feedback (&lt; 1 second).\nScope: In-memory logic, single-process SQLite, schema validation.\nRule: NEVER sleep, NEVER perform network I/O, NEVER spawn subprocesses.\n\n\n\n\n\nGoal: Verification of complex interactions.\nScope: Client-Server communication (FastAPI), File I/O (Blobs), Persistence checks.\nRule: Can be slower, but must be robust/deterministic.\n\n\n\n\n\nGoal: Stress testing locks and race conditions.\nScope: Multiprocessing, locking correctness, high-contention scenarios.\nRule: These are slow and resource-intensive. Run them only before a PR.\n\n\n\n\n\nBeaverDB is feature-complete for its core mission: a multi-modal embedded database. The future roadmap focuses on Architecture and Stability rather than adding new data structures.\n\n\nThe single biggest remaining milestone is refactoring the core to be Async-First.\n\nCurrent: Synchronous core with async wrappers.\nFuture: AsyncBeaverDB (using aiosqlite) as the engine, with a synchronous BeaverDB facade on top. This will allow handling thousands of concurrent Pub/Sub listeners and Locks on a single event loop.\n\n\n\n\nWe aim for the BeaverClient (HTTP) to be a 100% drop-in replacement for BeaverDB (Local). Code written for one should run on the other without modification. This allows developers to start local (embedded) and scale to a server deployment effortlessly.\n\n\n\nWe are committed to a stable, Pythonic API. We resist “feature creep.” If a feature can be implemented as a plugin or helper function on top of existing primitives (like DocumentStream built on walk), it belongs there, not in the core."
  },
  {
    "objectID": "dev-contributing.html#environment-setup",
    "href": "dev-contributing.html#environment-setup",
    "title": "Contributing to BeaverDB",
    "section": "",
    "text": "To contribute, you’ll need Python 3.13+. We recommend using a virtual environment.\n# 1. Clone the repository\ngit clone https://github.com/syalia-srl/beaver.git\ncd beaver\n\n# 2. Create a virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\n\n# 3. Install in editable mode with ALL dependencies (dev, server, vector, graph)\nuv sync"
  },
  {
    "objectID": "dev-contributing.html#development-workflow-makefile",
    "href": "dev-contributing.html#development-workflow-makefile",
    "title": "Contributing to BeaverDB",
    "section": "",
    "text": "We use a makefile to automate common tasks. You don’t need to memorize complex commands.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nmake test-unit\nRuns fast unit tests (Default). Use this during local dev.\n\n\nmake test-all\nRuns all tests, including slow integration and concurrency tests.\n\n\nmake format\nRuns black to format code and commits the changes.\n\n\nmake docs\nPublishes the documentation to GitHub Pages (requires permissions).\n\n\nmake issues\nSyncs local markdown issues with GitHub Issues (see below)."
  },
  {
    "objectID": "dev-contributing.html#issue-tracking-file-based",
    "href": "dev-contributing.html#issue-tracking-file-based",
    "title": "Contributing to BeaverDB",
    "section": "",
    "text": "BeaverDB uses a unique “Docs-as-Code” approach for issue tracking using the gh-md-issues extension.\n\nIssues are Files: Every issue exists as a markdown file in the issues/ directory (e.g., issues/31-composable-graph-rag.md).\nSyncing: We edit these files locally to draft feature plans or update status. Running make issues pushes these changes to GitHub Issues and pulls any comments/updates back to your local files.\n\nWorkflow:\n\nCreate a new file issues/New-Feature.md.\nWrite the frontmatter (title, labels, state: open).\nRun make issues to create it on GitHub."
  },
  {
    "objectID": "dev-contributing.html#testing-philosophy",
    "href": "dev-contributing.html#testing-philosophy",
    "title": "Contributing to BeaverDB",
    "section": "",
    "text": "Our test suite is divided into strict categories to keep the “Feedback Loop” fast. We use pytest markers defined in pytest.ini.\n\n\n\nGoal: Instant feedback (&lt; 1 second).\nScope: In-memory logic, single-process SQLite, schema validation.\nRule: NEVER sleep, NEVER perform network I/O, NEVER spawn subprocesses.\n\n\n\n\n\nGoal: Verification of complex interactions.\nScope: Client-Server communication (FastAPI), File I/O (Blobs), Persistence checks.\nRule: Can be slower, but must be robust/deterministic.\n\n\n\n\n\nGoal: Stress testing locks and race conditions.\nScope: Multiprocessing, locking correctness, high-contention scenarios.\nRule: These are slow and resource-intensive. Run them only before a PR."
  },
  {
    "objectID": "dev-contributing.html#the-future-of-beaverdb",
    "href": "dev-contributing.html#the-future-of-beaverdb",
    "title": "Contributing to BeaverDB",
    "section": "",
    "text": "BeaverDB is feature-complete for its core mission: a multi-modal embedded database. The future roadmap focuses on Architecture and Stability rather than adding new data structures.\n\n\nThe single biggest remaining milestone is refactoring the core to be Async-First.\n\nCurrent: Synchronous core with async wrappers.\nFuture: AsyncBeaverDB (using aiosqlite) as the engine, with a synchronous BeaverDB facade on top. This will allow handling thousands of concurrent Pub/Sub listeners and Locks on a single event loop.\n\n\n\n\nWe aim for the BeaverClient (HTTP) to be a 100% drop-in replacement for BeaverDB (Local). Code written for one should run on the other without modification. This allows developers to start local (embedded) and scale to a server deployment effortlessly.\n\n\n\nWe are committed to a stable, Pythonic API. We resist “feature creep.” If a feature can be implemented as a plugin or helper function on top of existing primitives (like DocumentStream built on walk), it belongs there, not in the core."
  },
  {
    "objectID": "guide.html",
    "href": "guide.html",
    "title": "Feature Summary",
    "section": "",
    "text": "This guide provides a comprehensive set of practical, code-first examples for every major feature of BeaverDB.\n\n\n\n\nUse a namespaced dictionary for storing simple key-value data like application configuration or user profiles.\nFirst, let’s import BeaverDB and get a handle to a namespaced dictionary called \"app_config\".\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\n\n# Get a handle to a namespaced dictionary\nconfig = db.dict(\"app_config\")\nNow you can use the config object just like a standard Python dictionary. Operations are saved to the database file instantly.\n# --- 1. Setting Values ---\nconfig[\"theme\"] = \"dark\"\nconfig[\"retries\"] = 3\n\n# You can also use the .set() method\nconfig.set(\"user_ids\", [101, 205, 301])\n\nprint(f\"Configuration dictionary has {len(config)} items.\")\n\n# --- 2. Retrieving Values ---\n# Use standard dict access\ntheme = config[\"theme\"]\nprint(f\"Retrieved theme: {theme}\")\n\n# Or use .get() with a default value\nnon_existent = config.get(\"non_existent_key\", \"default_value\")\nprint(f\"Result for non_existent_key: {non_existent}\")\nIterating and deleting items also follows standard Python syntax.\n# --- 3. Iterating Over the Dictionary ---\nprint(\"\\nIterating over config items:\")\nfor key, value in config.items():\n    print(f\"  - {key}: {value}\")\n\n# --- 4. Deleting an Item ---\ndel config[\"retries\"]\nprint(f\"\\nAfter deleting 'retries', config has {len(config)} items.\")\n\ndb.close()\n\n\n\nLeverage a dictionary with a Time-To-Live (TTL) to cache the results of slow network requests.\nLet’s define a mock function that simulates a slow API call and get a handle to a dictionary we’ll use as our cache.\nimport time\nfrom beaver import BeaverDB\n\ndef expensive_api_call(prompt: str):\n    \"\"\"A mock function that simulates a slow API call.\"\"\"\n    print(f\"--- Making expensive API call for: '{prompt}' ---\")\n    time.sleep(2)  # Simulate network latency\n    return \"Quito\"\n\ndb = BeaverDB(\"demo.db\")\napi_cache = db.dict(\"api_cache\")\nprompt = \"capital of Ecuador\"\nOn our first attempt, the cache is empty, so we get a “cache miss.” We call our expensive function and then use .set() with ttl_seconds=10 to store the result for 10 seconds.\n# --- 1. First Call (Cache Miss) ---\nprint(\"\\nAttempt 1: Key is not in cache.\")\nresponse = api_cache.get(prompt)\nif response is None:\n    print(\"Cache miss.\")\n    response = expensive_api_call(prompt)\n    # Set the value in the cache with a 10-second TTL\n    api_cache.set(prompt, response, ttl_seconds=10)\n\nprint(f\"Response: {response}\")\nNow, if we request the same data within the 10-second window, we get a “cache hit.” The data is returned instantly from the cache, and our expensive function is never called.\n# --- 2. Second Call (Cache Hit) ---\nprint(\"\\nAttempt 2: Making the same request within 5 seconds.\")\ntime.sleep(5)\nresponse = api_cache.get(prompt)\nif response is None:\n    # ... (this won't be called) ...\n    pass\nelse:\n    print(\"Cache hit!\")\n\nprint(f\"Response: {response}\")\nFinally, we wait 12 seconds, which is longer than our TTL. The cache key has automatically expired. When we request the data, it’s a “cache miss” again, forcing us to re-run the expensive function and update the cache for another 10 seconds.\n# --- 3. Third Call (Cache Expired) ---\nprint(\"\\nAttempt 3: Waiting for 12 seconds for the cache to expire.\")\ntime.sleep(12)\nresponse = api_cache.get(prompt)\nif response is None:\n    print(\"Cache miss (key expired).\")\n    response = expensive_api_call(prompt)\n    api_cache.set(prompt, response, ttl_seconds=10)\nelse:\n    print(\"Cache hit!\")\n\nprint(f\"Response: {response}\")\ndb.close()\n\n\n\nA persistent list is perfect for storing ordered data, like the history of a conversation. It supports a full Python list API.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\ntasks = db.list(\"project_tasks\")\nYou can add items to the beginning or end using push (like append) and prepend.\n# --- 1. Pushing and Prepending Items ---\ntasks.push({\"id\": \"task-002\", \"desc\": \"Write documentation\"})\ntasks.push({\"id\": \"task-003\", \"desc\": \"Deploy to production\"})\ntasks.prepend({\"id\": \"task-001\", \"desc\": \"Design the feature\"})\nAccessing, iterating, and slicing work exactly as you’d expect.\n# --- 2. Iterating Over the List ---\nprint(\"\\nCurrent tasks in order:\")\nfor task in tasks:\n    print(f\"  - {task['id']}: {task['desc']}\")\n\n# --- 3. Accessing and Slicing ---\nprint(f\"\\nThe first task is: {tasks[0]}\")\nprint(f\"The last task is: {tasks[-1]}\")\nprint(f\"A slice of the first two tasks: {tasks[0:2]}\")\nYou can also modify the list in place with standard __setitem__ and __delitem__ syntax, or use stack/queue methods like pop and deque.\n# --- 4. Updating an Item in Place ---\nprint(\"\\nUpdating the second task...\")\ntasks[1] = {\"id\": \"task-002\", \"desc\": \"Write and review documentation\"}\nprint(f\"Updated second task: {tasks[1]}\")\n\n# --- 5. Deleting an Item by Index ---\nprint(\"\\nDeleting the first task ('task-001')...\")\ndel tasks[0]\nprint(f\"List length after deletion: {len(tasks)}\")\n\n# --- 6. Popping the Last Item ---\nlast_item = tasks.pop()\nprint(f\"\\nPopped the last task: {last_item}\")\n\ndb.close()\n\n\n\nUse a persistent priority queue to manage tasks for an AI agent or any worker system. This ensures the most important tasks are always processed first.\nItems are added with a priority (lower numbers are processed first). The .get() method is atomic and blocks until an item is available, making it perfect for worker processes.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\ntasks = db.queue(\"agent_tasks\")\n\n# Tasks are added with a priority (lower is higher)\ntasks.put({\"action\": \"summarize_news\", \"topic\": \"AI\"}, priority=10)\ntasks.put({\"action\": \"respond_to_user\", \"user_id\": \"alice\"}, priority=1)\ntasks.put({\"action\": \"run_backup\", \"target\": \"all\"}, priority=20)\ntasks.put({\"action\": \"send_alert\", \"message\": \"CPU at 90%\"}, priority=1)\n\n# The agent retrieves the highest-priority task\n# This is a blocking call that waits for an item\nitem1 = tasks.get() # -&gt; Returns \"respond_to_user\" (priority 1, added first)\nitem2 = tasks.get() # -&gt; Returns \"send_alert\" (priority 1, added second)\nitem3 = tasks.get() # -&gt; Returns \"summarize_news\" (priority 10)\n\nprint(f\"Agent's first task: {item1.data['action']}\")\nprint(f\"Agent's second task: {item2.data['action']}\")\n\ndb.close()\n\n\n\nUse the blob store to save binary data like user avatars, attachments, or generated reports directly in the database.\nFirst, we’ll get a handle to our blob store and prepare some sample binary data.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\nattachments = db.blobs(\"user_uploads\")\n\n# Create some sample binary data\nfile_content = \"This is the content of a virtual text file.\"\nfile_bytes = file_content.encode(\"utf-8\")\nfile_key = \"emails/user123/attachment_01.txt\"\nNow, we store the data using .put() with a unique key and some JSON metadata.\n# Store a user's avatar with metadata\nattachments.put(\n    key=file_key,\n    data=file_bytes,\n    metadata={\"mimetype\": \"text/plain\", \"sender\": \"alice@example.com\"}\n)\nWe can retrieve the blob using .get(), which returns a Blob object containing the key, data, and metadata.\n# Retrieve it later\nblob = attachments.get(file_key)\nif blob:\n    print(f\"Retrieved Blob: {blob.key}\")\n    print(f\"Metadata: {blob.metadata}\")\n    print(f\"Data (decoded): '{blob.data.decode('utf-8')}'\")\nFinally, we can clean up the blob using .delete().\n# Delete the blob\nattachments.delete(file_key)\nprint(f\"\\nVerified deletion: {file_key not in attachments}\")\n\ndb.close()\n\n\n\n\nThe collection is the most powerful data structure, combining document storage with vector, text, and graph search.\n\n\nThis example shows how to combine keyword and vector search for a simple RAG pipeline.\nfrom beaver import BeaverDB, Document\nfrom beaver.collections import rerank\n\ndb = BeaverDB(\"rag_demo.db\")\narticles = db.collection(\"articles\")\nFirst, we index our documents. Each Document contains text for FTS (in body) and a vector (embedding). We enable FTS by setting fts=True.\n# Index documents with both text and embeddings\ndocs_to_index = [\n    Document(\n        id=\"py-fast\",\n        embedding=[0.1, 0.9, 0.2],  # Vector leans towards \"speed\"\n        body=\"Python is a great language for fast prototyping.\",\n    ),\n    Document(\n        id=\"py-data\",\n        embedding=[0.8, 0.2, 0.9],  # Vector leans towards \"data science\"\n        body=\"The Python ecosystem is ideal for data science.\",\n    ),\n    Document(\n        id=\"js-fast\",\n        embedding=[0.2, 0.8, 0.1],  # Vector similar to \"py-fast\"\n        body=\"JavaScript engines are optimized for fast execution.\",\n    ),\n]\nfor doc in docs_to_index:\n    articles.index(doc, fts=True) # Enable FTS\nNow, we perform two separate searches. One is a keyword search for “python,” and the other is a semantic vector search for a query representing “high-performance code.”\n# 1. Vector Search for \"high-performance code\"\nquery_vector = [0.15, 0.85, 0.15] # A vector close to \"fast\"\nvector_results = [doc for doc, _ in articles.search(vector=query_vector)]\n\n# 2. Full-Text Search for \"python\"\ntext_results = [doc for doc, _ in articles.match(query=\"python\")]\nFinally, we use the rerank helper to merge these two lists. It promotes documents that appear high in both result sets, giving us the most relevant answer.\n# 3. Combine and rerank to get the best context\nbest_context = rerank(text_results, vector_results)\n\nprint(\"--- Final Reranked Results ---\")\nfor doc in best_context:\n    print(f\"  - {doc.id}: {doc.body}\")\n\ndb.close()\n\n\n\nA collection also serves as a graph. We can connect documents to build a network of relationships.\nfrom beaver import BeaverDB, Document\n\ndb = BeaverDB(\"graph_demo.db\")\nnet = db.collection(\"social_network\")\nFirst, we index our documents, which will act as the nodes in our graph.\n# 1. Create Documents (nodes)\nalice = Document(id=\"alice\", body={\"name\": \"Alice\"})\nbob = Document(id=\"bob\", body={\"name\": \"Bob\"})\ncharlie = Document(id=\"charlie\", body={\"name\": \"Charlie\"})\ndiana = Document(id=\"diana\", body={\"name\": \"Diana\"})\nnet.index(alice); net.index(bob); net.index(charlie); net.index(diana)\nNext, we create relationships (or edges) between them using the .connect() method. We can give each connection a label to define the relationship type.\n# 2. Create Edges (relationships)\nnet.connect(alice, bob, label=\"FOLLOWS\")\nnet.connect(alice, charlie, label=\"FOLLOWS\")\nnet.connect(bob, diana, label=\"FOLLOWS\")\nnet.connect(charlie, bob, label=\"FOLLOWS\")\nWe can now query these relationships. .neighbors() finds all nodes one hop away.\n# 3. Find 1-hop neighbors\nprint(\"--- Testing `neighbors` (1-hop) ---\")\nfollowing = net.neighbors(alice, label=\"FOLLOWS\")\nprint(f\"Alice follows: {[p.id for p in following]}\")\nFor multi-hop traversals (like “friends of friends”), we use the .walk() method.\n# 4. Find multi-hop connections (e.g., \"friends of friends\")\nprint(\"\\n--- Testing `walk` (multi-hop) ---\")\nfoaf = net.walk(\n    source=alice,\n    labels=[\"FOLLOWS\"],\n    depth=2,\n)\nprint(f\"Alice's extended network (friends of friends): {[p.id for p in foaf]}\")\n\ndb.close()\n\n\n\n\n\n\nRun multiple scripts in parallel and use db.lock() to coordinate them. This example simulates two scrapers trying to refresh a shared sitemap. The db.lock() ensures that only one process can enter the critical section, preventing a “thundering herd” race condition. The timeout=1 causes other processes to skip rather than wait.\nimport time\nimport os\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"scraper_state.db\")\nscrapers_state = db.dict(\"scraper_state\")\n\nlast_refresh = scrapers_state.get(\"last_sitemap_refresh\", 0)\nif time.time() - last_refresh &gt; 3600: # Only refresh once per hour\n    try:\n        # Try to get a lock, but don't wait long\n        with db.lock(\"refresh_sitemap\", timeout=1):\n            # We got the lock.\n            print(f\"PID {os.getpid()} is refreshing the sitemap...\")\n            scrapers_state[\"sitemap\"] = [\"/page1\", \"/page2\"] # Your fetch_sitemap()\n            scrapers_state[\"last_sitemap_refresh\"] = time.time()\n\n    except TimeoutError:\n        # Another process is already refreshing\n        print(f\"PID {os.getpid()} letting other process handle refresh.\")\n\nsitemap = scrapers_state.get(\"sitemap\")\nprint(f\"PID {os.getpid()} proceeding with sitemap: {sitemap}\")\ndb.close()\n\n\n\nEnsure a worker process can safely pull a batch of items from a queue without another worker interfering, using the built-in manager lock. By wrapping the operation in with db.queue(...).acquire(), we guarantee that this process can pull 10 items from the queue atomically, without another worker process stealing items in the middle of the batch.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"tasks.db\")\ntasks_to_process = []\ntry:\n    # This lock guarantees no other process can access 'agent_tasks'\n    # while this block is running.\n    with db.queue('agent_tasks').acquire(timeout=5) as q:\n        for _ in range(10): # Get a batch of 10\n            item = q.get(block=False)\n            tasks_to_process.append(item.data)\nexcept (TimeoutError, IndexError):\n    # Lock timed out or queue was empty\n    print(\"Could not get 10 items.\")\n    pass\n\n# Now process the batch outside the lock\nprint(f\"Processing batch of {len(tasks_to_process)} items.\")\ndb.close()\n\n\n\nBeaverDB’s pub/sub system allows for real-time, multi-process communication. Here is how two separate processes can communicate.\nIn one process (or thread), a publisher sends a message to a named channel. This is a fast, single INSERT operation.\n# --- In one process or thread (e.g., a monitoring service) ---\n#\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\nsystem_events = db.channel(\"system_events\")\nprint(\"[Publisher] Publishing message...\")\nsystem_events.publish({\"event\": \"user_login\", \"user_id\": \"alice\"})\ndb.close()\nIn another process, a subscriber “listens” to that same channel. The with ...subscribe() block handles registration, and listener.listen() blocks until a message arrives.\n# --- In another process or thread (e.g., a UI updater or logger) ---\n#\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\n# The 'with' block handles the subscription lifecycle.\nwith db.channel(\"system_events\").subscribe() as listener:\n    for message in listener.listen():\n        print(f\"Event received: {message}\")\n        # &gt;&gt; Event received: {'event': 'user_login', 'user_id': 'alice'}\n        break # Exit after one message for this example\ndb.close()\n\n\n\nMonitor your application’s health in real-time. The .live() method provides a continuously updating, aggregated view of your log data, perfect for terminal dashboards.\nFirst, we define an ‘aggregator’ function that takes a list of log entries and returns a single summary dictionary.\nfrom datetime import timedelta\nimport statistics\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"live_log_demo.db\")\nlogs = db.log(\"system_metrics\")\n\ndef summarize(window: list[dict]) -&gt; dict:\n    \"\"\"Aggregator that calculates stats from a window of log data.\"\"\"\n    if not window:\n        return {\"mean\": 0.0, \"count\": 0}\n    values = [log.get(\"value\", 0) for log in window]\n    return {\"mean\": statistics.mean(values), \"count\": len(values)}\n\n# Start a background thread to write logs (see examples/logs.py for full code)\n# ...\nNext, we get the live iterator. We tell it to maintain a 5-second rolling window of data and to compute a new summary every 1 second using our function.\n# Get the live iterator over a 5-second rolling window, updating every 1 sec\nlive_summary = logs.live(\n    window=timedelta(seconds=5),\n    period=timedelta(seconds=1),\n    aggregator=summarize\n)\nFinally, we just iterate over live_summary. This loop will block and yield a new summary object every 1 second, giving us a real-time view of our data.\nprint(\"[Main Thread] Starting live view. Press Ctrl+C to stop.\")\ntry:\n    for summary in live_summary:\n        print(f\"Live Stats (5s window): Count={summary['count']}, Mean={summary['mean']:.2f}\")\n        time.sleep(1) # In a real app, this loop just blocks\nexcept KeyboardInterrupt:\n    print(\"\\nShutting down.\")\nfinally:\n    db.close()\n\n\n\nListen for database changes in real-time. You can subscribe to events on specific managers (e.g., db.dict(\"config\").on(\"set\", ...) to trigger workflows or update UIs).\nFirst, let’s define a callback function that will be triggered when a change occurs.\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"events_demo.db\")\nconfig = db.dict(\"app_config\")\nconfig.clear()\n\ndef on_config_change(payload):\n    \"\"\"This callback is triggered when a key is set or deleted.\"\"\"\n    print(f\"EVENT RECEIVED: Key '{payload['key']}' was changed!\")\nNow, we subscribe to the ‘set’ and ‘del’ events on our config dictionary using the .on() method. This returns a handle that we can use to unsubscribe later.\n# Subscribe to 'set' events on this specific dict\nset_handle = config.on(\"set\", on_config_change)\ndel_handle = config.on(\"del\", on_config_change)\n\n# Give the listener thread time to start\ntime.sleep(0.1)\nWhen we modify the dictionary, the callback is triggered automatically in a background thread.\nprint(\"Setting 'theme'...\")\nconfig[\"theme\"] = \"dark\"  # Triggers the 'on_config_change' callback\ntime.sleep(0.1) # Wait for event to process\n\nprint(\"Deleting 'theme'...\")\ndel config[\"theme\"] # Triggers the 'on_config_change' callback\ntime.sleep(0.1)\nTo stop listening, we call .off() on the handles. Subsequent changes will be silent.\n# Clean up the listeners\nset_handle.off()\ndel_handle.off()\n\nprint(\"Listeners are off. This change will be silent.\")\nconfig[\"theme\"] = \"light\" # No event will be printed\ntime.sleep(0.1)\n\ndb.close()\n\n\n\n\n\n\nBeaverDB has first-class support for Pydantic. By associating a BaseModel with a data structure, you get automatic, recursive (de)serialization and data validation.\nBy passing model=User to the db.dict() factory, BeaverDB will automatically validate data on write and, more importantly, deserialize the stored JSON back into a full User object on read, giving you type safety and editor autocompletion.\nfrom pydantic import BaseModel\nfrom beaver import BeaverDB\n\n# Define your Pydantic model\nclass User(BaseModel):\n    name: str\n    email: str\n    permissions: list[str]\n\ndb = BeaverDB(\"user_data.db\")\n\n# Associate the User model with a dictionary\nusers = db.dict(\"user_profiles\", model=User)\n\n# BeaverDB now handles serialization automatically\nusers[\"alice\"] = User(\n    name=\"Alice\",\n    email=\"alice@example.com\",\n    permissions=[\"read\", \"write\"]\n)\n\n# The retrieved object is a proper, validated User instance\nretrieved_user = users[\"alice\"]\n\n# Your editor will provide autocompletion here\nprint(f\"Retrieved: {retrieved_user.name}\")\nprint(f\"Permissions: {retrieved_user.permissions}\")\n\ndb.close()\n\n\n\nYou can instantly expose your database over a RESTful API and interact with it from the command line.\n1. Start the Server\n# Start the server for your database file\nbeaver serve --database data.db --port 8000\n2. Interact with the API (e.g., from curl)\n# Set a value in the 'app_config' dictionary\ncurl -X PUT [http://127.0.0.1:8000/dicts/app_config/api_key](http://127.0.0.1:8000/dicts/app_config/api_key) \\\n     -H \"Content-Type: application/json\" \\\n     -d '\"your-secret-api-key\"'\n\n# Get the value back\ncurl [http://127.0.0.1:8000/dicts/app_config/api_key](http://127.0.0.1:8000/dicts/app_config/api_key)\n# Output: \"your-secret-api-key\"\n3. Interact with the CLI Client\nThe beaver CLI lets you call any method directly from your terminal.\n# Get a value from a dictionary\nbeaver --database data.db dict app_config get theme\n\n# Set a value (JSON is automatically parsed)\nbeaver --database data.db dict app_config set user '{\"name\": \"Alice\", \"id\": 123}'\n\n# Push an item to a list\nbeaver --database data.db list daily_tasks push \"Review PRs\"\n\n# Run a script protected by a distributed lock\nbeaver --database data.db lock my-cron-job run bash -c 'run_daily_report.sh'\n\n\n\nAll data structures support a .dump() method for easy backups and migration to a JSON file.\nimport json\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"my_app.db\")\nconfig = db.dict(\"app_config\")\nconfig[\"theme\"] = \"dark\"\nconfig[\"user_id\"] = 456\n\n# Dump the dictionary's contents to a JSON file\nwith open(\"config_backup.json\", \"w\") as f:\n    config.dump(f)\n\n# You can also get the dump as a Python object\ndump_data = config.dump()\nprint(dump_data['metadata'])\n\ndb.close()\nYou can also use the CLI to dump data:\nbeaver --database data.db collection my_documents dump &gt; my_documents.json"
  },
  {
    "objectID": "guide.html#core-data-structures",
    "href": "guide.html#core-data-structures",
    "title": "Feature Summary",
    "section": "",
    "text": "Use a namespaced dictionary for storing simple key-value data like application configuration or user profiles.\nFirst, let’s import BeaverDB and get a handle to a namespaced dictionary called \"app_config\".\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\n\n# Get a handle to a namespaced dictionary\nconfig = db.dict(\"app_config\")\nNow you can use the config object just like a standard Python dictionary. Operations are saved to the database file instantly.\n# --- 1. Setting Values ---\nconfig[\"theme\"] = \"dark\"\nconfig[\"retries\"] = 3\n\n# You can also use the .set() method\nconfig.set(\"user_ids\", [101, 205, 301])\n\nprint(f\"Configuration dictionary has {len(config)} items.\")\n\n# --- 2. Retrieving Values ---\n# Use standard dict access\ntheme = config[\"theme\"]\nprint(f\"Retrieved theme: {theme}\")\n\n# Or use .get() with a default value\nnon_existent = config.get(\"non_existent_key\", \"default_value\")\nprint(f\"Result for non_existent_key: {non_existent}\")\nIterating and deleting items also follows standard Python syntax.\n# --- 3. Iterating Over the Dictionary ---\nprint(\"\\nIterating over config items:\")\nfor key, value in config.items():\n    print(f\"  - {key}: {value}\")\n\n# --- 4. Deleting an Item ---\ndel config[\"retries\"]\nprint(f\"\\nAfter deleting 'retries', config has {len(config)} items.\")\n\ndb.close()\n\n\n\nLeverage a dictionary with a Time-To-Live (TTL) to cache the results of slow network requests.\nLet’s define a mock function that simulates a slow API call and get a handle to a dictionary we’ll use as our cache.\nimport time\nfrom beaver import BeaverDB\n\ndef expensive_api_call(prompt: str):\n    \"\"\"A mock function that simulates a slow API call.\"\"\"\n    print(f\"--- Making expensive API call for: '{prompt}' ---\")\n    time.sleep(2)  # Simulate network latency\n    return \"Quito\"\n\ndb = BeaverDB(\"demo.db\")\napi_cache = db.dict(\"api_cache\")\nprompt = \"capital of Ecuador\"\nOn our first attempt, the cache is empty, so we get a “cache miss.” We call our expensive function and then use .set() with ttl_seconds=10 to store the result for 10 seconds.\n# --- 1. First Call (Cache Miss) ---\nprint(\"\\nAttempt 1: Key is not in cache.\")\nresponse = api_cache.get(prompt)\nif response is None:\n    print(\"Cache miss.\")\n    response = expensive_api_call(prompt)\n    # Set the value in the cache with a 10-second TTL\n    api_cache.set(prompt, response, ttl_seconds=10)\n\nprint(f\"Response: {response}\")\nNow, if we request the same data within the 10-second window, we get a “cache hit.” The data is returned instantly from the cache, and our expensive function is never called.\n# --- 2. Second Call (Cache Hit) ---\nprint(\"\\nAttempt 2: Making the same request within 5 seconds.\")\ntime.sleep(5)\nresponse = api_cache.get(prompt)\nif response is None:\n    # ... (this won't be called) ...\n    pass\nelse:\n    print(\"Cache hit!\")\n\nprint(f\"Response: {response}\")\nFinally, we wait 12 seconds, which is longer than our TTL. The cache key has automatically expired. When we request the data, it’s a “cache miss” again, forcing us to re-run the expensive function and update the cache for another 10 seconds.\n# --- 3. Third Call (Cache Expired) ---\nprint(\"\\nAttempt 3: Waiting for 12 seconds for the cache to expire.\")\ntime.sleep(12)\nresponse = api_cache.get(prompt)\nif response is None:\n    print(\"Cache miss (key expired).\")\n    response = expensive_api_call(prompt)\n    api_cache.set(prompt, response, ttl_seconds=10)\nelse:\n    print(\"Cache hit!\")\n\nprint(f\"Response: {response}\")\ndb.close()\n\n\n\nA persistent list is perfect for storing ordered data, like the history of a conversation. It supports a full Python list API.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\ntasks = db.list(\"project_tasks\")\nYou can add items to the beginning or end using push (like append) and prepend.\n# --- 1. Pushing and Prepending Items ---\ntasks.push({\"id\": \"task-002\", \"desc\": \"Write documentation\"})\ntasks.push({\"id\": \"task-003\", \"desc\": \"Deploy to production\"})\ntasks.prepend({\"id\": \"task-001\", \"desc\": \"Design the feature\"})\nAccessing, iterating, and slicing work exactly as you’d expect.\n# --- 2. Iterating Over the List ---\nprint(\"\\nCurrent tasks in order:\")\nfor task in tasks:\n    print(f\"  - {task['id']}: {task['desc']}\")\n\n# --- 3. Accessing and Slicing ---\nprint(f\"\\nThe first task is: {tasks[0]}\")\nprint(f\"The last task is: {tasks[-1]}\")\nprint(f\"A slice of the first two tasks: {tasks[0:2]}\")\nYou can also modify the list in place with standard __setitem__ and __delitem__ syntax, or use stack/queue methods like pop and deque.\n# --- 4. Updating an Item in Place ---\nprint(\"\\nUpdating the second task...\")\ntasks[1] = {\"id\": \"task-002\", \"desc\": \"Write and review documentation\"}\nprint(f\"Updated second task: {tasks[1]}\")\n\n# --- 5. Deleting an Item by Index ---\nprint(\"\\nDeleting the first task ('task-001')...\")\ndel tasks[0]\nprint(f\"List length after deletion: {len(tasks)}\")\n\n# --- 6. Popping the Last Item ---\nlast_item = tasks.pop()\nprint(f\"\\nPopped the last task: {last_item}\")\n\ndb.close()\n\n\n\nUse a persistent priority queue to manage tasks for an AI agent or any worker system. This ensures the most important tasks are always processed first.\nItems are added with a priority (lower numbers are processed first). The .get() method is atomic and blocks until an item is available, making it perfect for worker processes.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\ntasks = db.queue(\"agent_tasks\")\n\n# Tasks are added with a priority (lower is higher)\ntasks.put({\"action\": \"summarize_news\", \"topic\": \"AI\"}, priority=10)\ntasks.put({\"action\": \"respond_to_user\", \"user_id\": \"alice\"}, priority=1)\ntasks.put({\"action\": \"run_backup\", \"target\": \"all\"}, priority=20)\ntasks.put({\"action\": \"send_alert\", \"message\": \"CPU at 90%\"}, priority=1)\n\n# The agent retrieves the highest-priority task\n# This is a blocking call that waits for an item\nitem1 = tasks.get() # -&gt; Returns \"respond_to_user\" (priority 1, added first)\nitem2 = tasks.get() # -&gt; Returns \"send_alert\" (priority 1, added second)\nitem3 = tasks.get() # -&gt; Returns \"summarize_news\" (priority 10)\n\nprint(f\"Agent's first task: {item1.data['action']}\")\nprint(f\"Agent's second task: {item2.data['action']}\")\n\ndb.close()\n\n\n\nUse the blob store to save binary data like user avatars, attachments, or generated reports directly in the database.\nFirst, we’ll get a handle to our blob store and prepare some sample binary data.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\nattachments = db.blobs(\"user_uploads\")\n\n# Create some sample binary data\nfile_content = \"This is the content of a virtual text file.\"\nfile_bytes = file_content.encode(\"utf-8\")\nfile_key = \"emails/user123/attachment_01.txt\"\nNow, we store the data using .put() with a unique key and some JSON metadata.\n# Store a user's avatar with metadata\nattachments.put(\n    key=file_key,\n    data=file_bytes,\n    metadata={\"mimetype\": \"text/plain\", \"sender\": \"alice@example.com\"}\n)\nWe can retrieve the blob using .get(), which returns a Blob object containing the key, data, and metadata.\n# Retrieve it later\nblob = attachments.get(file_key)\nif blob:\n    print(f\"Retrieved Blob: {blob.key}\")\n    print(f\"Metadata: {blob.metadata}\")\n    print(f\"Data (decoded): '{blob.data.decode('utf-8')}'\")\nFinally, we can clean up the blob using .delete().\n# Delete the blob\nattachments.delete(file_key)\nprint(f\"\\nVerified deletion: {file_key not in attachments}\")\n\ndb.close()"
  },
  {
    "objectID": "guide.html#the-document-collection-db.collection",
    "href": "guide.html#the-document-collection-db.collection",
    "title": "Feature Summary",
    "section": "",
    "text": "The collection is the most powerful data structure, combining document storage with vector, text, and graph search.\n\n\nThis example shows how to combine keyword and vector search for a simple RAG pipeline.\nfrom beaver import BeaverDB, Document\nfrom beaver.collections import rerank\n\ndb = BeaverDB(\"rag_demo.db\")\narticles = db.collection(\"articles\")\nFirst, we index our documents. Each Document contains text for FTS (in body) and a vector (embedding). We enable FTS by setting fts=True.\n# Index documents with both text and embeddings\ndocs_to_index = [\n    Document(\n        id=\"py-fast\",\n        embedding=[0.1, 0.9, 0.2],  # Vector leans towards \"speed\"\n        body=\"Python is a great language for fast prototyping.\",\n    ),\n    Document(\n        id=\"py-data\",\n        embedding=[0.8, 0.2, 0.9],  # Vector leans towards \"data science\"\n        body=\"The Python ecosystem is ideal for data science.\",\n    ),\n    Document(\n        id=\"js-fast\",\n        embedding=[0.2, 0.8, 0.1],  # Vector similar to \"py-fast\"\n        body=\"JavaScript engines are optimized for fast execution.\",\n    ),\n]\nfor doc in docs_to_index:\n    articles.index(doc, fts=True) # Enable FTS\nNow, we perform two separate searches. One is a keyword search for “python,” and the other is a semantic vector search for a query representing “high-performance code.”\n# 1. Vector Search for \"high-performance code\"\nquery_vector = [0.15, 0.85, 0.15] # A vector close to \"fast\"\nvector_results = [doc for doc, _ in articles.search(vector=query_vector)]\n\n# 2. Full-Text Search for \"python\"\ntext_results = [doc for doc, _ in articles.match(query=\"python\")]\nFinally, we use the rerank helper to merge these two lists. It promotes documents that appear high in both result sets, giving us the most relevant answer.\n# 3. Combine and rerank to get the best context\nbest_context = rerank(text_results, vector_results)\n\nprint(\"--- Final Reranked Results ---\")\nfor doc in best_context:\n    print(f\"  - {doc.id}: {doc.body}\")\n\ndb.close()\n\n\n\nA collection also serves as a graph. We can connect documents to build a network of relationships.\nfrom beaver import BeaverDB, Document\n\ndb = BeaverDB(\"graph_demo.db\")\nnet = db.collection(\"social_network\")\nFirst, we index our documents, which will act as the nodes in our graph.\n# 1. Create Documents (nodes)\nalice = Document(id=\"alice\", body={\"name\": \"Alice\"})\nbob = Document(id=\"bob\", body={\"name\": \"Bob\"})\ncharlie = Document(id=\"charlie\", body={\"name\": \"Charlie\"})\ndiana = Document(id=\"diana\", body={\"name\": \"Diana\"})\nnet.index(alice); net.index(bob); net.index(charlie); net.index(diana)\nNext, we create relationships (or edges) between them using the .connect() method. We can give each connection a label to define the relationship type.\n# 2. Create Edges (relationships)\nnet.connect(alice, bob, label=\"FOLLOWS\")\nnet.connect(alice, charlie, label=\"FOLLOWS\")\nnet.connect(bob, diana, label=\"FOLLOWS\")\nnet.connect(charlie, bob, label=\"FOLLOWS\")\nWe can now query these relationships. .neighbors() finds all nodes one hop away.\n# 3. Find 1-hop neighbors\nprint(\"--- Testing `neighbors` (1-hop) ---\")\nfollowing = net.neighbors(alice, label=\"FOLLOWS\")\nprint(f\"Alice follows: {[p.id for p in following]}\")\nFor multi-hop traversals (like “friends of friends”), we use the .walk() method.\n# 4. Find multi-hop connections (e.g., \"friends of friends\")\nprint(\"\\n--- Testing `walk` (multi-hop) ---\")\nfoaf = net.walk(\n    source=alice,\n    labels=[\"FOLLOWS\"],\n    depth=2,\n)\nprint(f\"Alice's extended network (friends of friends): {[p.id for p in foaf]}\")\n\ndb.close()"
  },
  {
    "objectID": "guide.html#real-time-concurrency",
    "href": "guide.html#real-time-concurrency",
    "title": "Feature Summary",
    "section": "",
    "text": "Run multiple scripts in parallel and use db.lock() to coordinate them. This example simulates two scrapers trying to refresh a shared sitemap. The db.lock() ensures that only one process can enter the critical section, preventing a “thundering herd” race condition. The timeout=1 causes other processes to skip rather than wait.\nimport time\nimport os\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"scraper_state.db\")\nscrapers_state = db.dict(\"scraper_state\")\n\nlast_refresh = scrapers_state.get(\"last_sitemap_refresh\", 0)\nif time.time() - last_refresh &gt; 3600: # Only refresh once per hour\n    try:\n        # Try to get a lock, but don't wait long\n        with db.lock(\"refresh_sitemap\", timeout=1):\n            # We got the lock.\n            print(f\"PID {os.getpid()} is refreshing the sitemap...\")\n            scrapers_state[\"sitemap\"] = [\"/page1\", \"/page2\"] # Your fetch_sitemap()\n            scrapers_state[\"last_sitemap_refresh\"] = time.time()\n\n    except TimeoutError:\n        # Another process is already refreshing\n        print(f\"PID {os.getpid()} letting other process handle refresh.\")\n\nsitemap = scrapers_state.get(\"sitemap\")\nprint(f\"PID {os.getpid()} proceeding with sitemap: {sitemap}\")\ndb.close()\n\n\n\nEnsure a worker process can safely pull a batch of items from a queue without another worker interfering, using the built-in manager lock. By wrapping the operation in with db.queue(...).acquire(), we guarantee that this process can pull 10 items from the queue atomically, without another worker process stealing items in the middle of the batch.\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"tasks.db\")\ntasks_to_process = []\ntry:\n    # This lock guarantees no other process can access 'agent_tasks'\n    # while this block is running.\n    with db.queue('agent_tasks').acquire(timeout=5) as q:\n        for _ in range(10): # Get a batch of 10\n            item = q.get(block=False)\n            tasks_to_process.append(item.data)\nexcept (TimeoutError, IndexError):\n    # Lock timed out or queue was empty\n    print(\"Could not get 10 items.\")\n    pass\n\n# Now process the batch outside the lock\nprint(f\"Processing batch of {len(tasks_to_process)} items.\")\ndb.close()\n\n\n\nBeaverDB’s pub/sub system allows for real-time, multi-process communication. Here is how two separate processes can communicate.\nIn one process (or thread), a publisher sends a message to a named channel. This is a fast, single INSERT operation.\n# --- In one process or thread (e.g., a monitoring service) ---\n#\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\nsystem_events = db.channel(\"system_events\")\nprint(\"[Publisher] Publishing message...\")\nsystem_events.publish({\"event\": \"user_login\", \"user_id\": \"alice\"})\ndb.close()\nIn another process, a subscriber “listens” to that same channel. The with ...subscribe() block handles registration, and listener.listen() blocks until a message arrives.\n# --- In another process or thread (e.g., a UI updater or logger) ---\n#\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"demo.db\")\n# The 'with' block handles the subscription lifecycle.\nwith db.channel(\"system_events\").subscribe() as listener:\n    for message in listener.listen():\n        print(f\"Event received: {message}\")\n        # &gt;&gt; Event received: {'event': 'user_login', 'user_id': 'alice'}\n        break # Exit after one message for this example\ndb.close()\n\n\n\nMonitor your application’s health in real-time. The .live() method provides a continuously updating, aggregated view of your log data, perfect for terminal dashboards.\nFirst, we define an ‘aggregator’ function that takes a list of log entries and returns a single summary dictionary.\nfrom datetime import timedelta\nimport statistics\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"live_log_demo.db\")\nlogs = db.log(\"system_metrics\")\n\ndef summarize(window: list[dict]) -&gt; dict:\n    \"\"\"Aggregator that calculates stats from a window of log data.\"\"\"\n    if not window:\n        return {\"mean\": 0.0, \"count\": 0}\n    values = [log.get(\"value\", 0) for log in window]\n    return {\"mean\": statistics.mean(values), \"count\": len(values)}\n\n# Start a background thread to write logs (see examples/logs.py for full code)\n# ...\nNext, we get the live iterator. We tell it to maintain a 5-second rolling window of data and to compute a new summary every 1 second using our function.\n# Get the live iterator over a 5-second rolling window, updating every 1 sec\nlive_summary = logs.live(\n    window=timedelta(seconds=5),\n    period=timedelta(seconds=1),\n    aggregator=summarize\n)\nFinally, we just iterate over live_summary. This loop will block and yield a new summary object every 1 second, giving us a real-time view of our data.\nprint(\"[Main Thread] Starting live view. Press Ctrl+C to stop.\")\ntry:\n    for summary in live_summary:\n        print(f\"Live Stats (5s window): Count={summary['count']}, Mean={summary['mean']:.2f}\")\n        time.sleep(1) # In a real app, this loop just blocks\nexcept KeyboardInterrupt:\n    print(\"\\nShutting down.\")\nfinally:\n    db.close()\n\n\n\nListen for database changes in real-time. You can subscribe to events on specific managers (e.g., db.dict(\"config\").on(\"set\", ...) to trigger workflows or update UIs).\nFirst, let’s define a callback function that will be triggered when a change occurs.\nimport time\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"events_demo.db\")\nconfig = db.dict(\"app_config\")\nconfig.clear()\n\ndef on_config_change(payload):\n    \"\"\"This callback is triggered when a key is set or deleted.\"\"\"\n    print(f\"EVENT RECEIVED: Key '{payload['key']}' was changed!\")\nNow, we subscribe to the ‘set’ and ‘del’ events on our config dictionary using the .on() method. This returns a handle that we can use to unsubscribe later.\n# Subscribe to 'set' events on this specific dict\nset_handle = config.on(\"set\", on_config_change)\ndel_handle = config.on(\"del\", on_config_change)\n\n# Give the listener thread time to start\ntime.sleep(0.1)\nWhen we modify the dictionary, the callback is triggered automatically in a background thread.\nprint(\"Setting 'theme'...\")\nconfig[\"theme\"] = \"dark\"  # Triggers the 'on_config_change' callback\ntime.sleep(0.1) # Wait for event to process\n\nprint(\"Deleting 'theme'...\")\ndel config[\"theme\"] # Triggers the 'on_config_change' callback\ntime.sleep(0.1)\nTo stop listening, we call .off() on the handles. Subsequent changes will be silent.\n# Clean up the listeners\nset_handle.off()\ndel_handle.off()\n\nprint(\"Listeners are off. This change will be silent.\")\nconfig[\"theme\"] = \"light\" # No event will be printed\ntime.sleep(0.1)\n\ndb.close()"
  },
  {
    "objectID": "guide.html#advanced-features",
    "href": "guide.html#advanced-features",
    "title": "Feature Summary",
    "section": "",
    "text": "BeaverDB has first-class support for Pydantic. By associating a BaseModel with a data structure, you get automatic, recursive (de)serialization and data validation.\nBy passing model=User to the db.dict() factory, BeaverDB will automatically validate data on write and, more importantly, deserialize the stored JSON back into a full User object on read, giving you type safety and editor autocompletion.\nfrom pydantic import BaseModel\nfrom beaver import BeaverDB\n\n# Define your Pydantic model\nclass User(BaseModel):\n    name: str\n    email: str\n    permissions: list[str]\n\ndb = BeaverDB(\"user_data.db\")\n\n# Associate the User model with a dictionary\nusers = db.dict(\"user_profiles\", model=User)\n\n# BeaverDB now handles serialization automatically\nusers[\"alice\"] = User(\n    name=\"Alice\",\n    email=\"alice@example.com\",\n    permissions=[\"read\", \"write\"]\n)\n\n# The retrieved object is a proper, validated User instance\nretrieved_user = users[\"alice\"]\n\n# Your editor will provide autocompletion here\nprint(f\"Retrieved: {retrieved_user.name}\")\nprint(f\"Permissions: {retrieved_user.permissions}\")\n\ndb.close()\n\n\n\nYou can instantly expose your database over a RESTful API and interact with it from the command line.\n1. Start the Server\n# Start the server for your database file\nbeaver serve --database data.db --port 8000\n2. Interact with the API (e.g., from curl)\n# Set a value in the 'app_config' dictionary\ncurl -X PUT [http://127.0.0.1:8000/dicts/app_config/api_key](http://127.0.0.1:8000/dicts/app_config/api_key) \\\n     -H \"Content-Type: application/json\" \\\n     -d '\"your-secret-api-key\"'\n\n# Get the value back\ncurl [http://127.0.0.1:8000/dicts/app_config/api_key](http://127.0.0.1:8000/dicts/app_config/api_key)\n# Output: \"your-secret-api-key\"\n3. Interact with the CLI Client\nThe beaver CLI lets you call any method directly from your terminal.\n# Get a value from a dictionary\nbeaver --database data.db dict app_config get theme\n\n# Set a value (JSON is automatically parsed)\nbeaver --database data.db dict app_config set user '{\"name\": \"Alice\", \"id\": 123}'\n\n# Push an item to a list\nbeaver --database data.db list daily_tasks push \"Review PRs\"\n\n# Run a script protected by a distributed lock\nbeaver --database data.db lock my-cron-job run bash -c 'run_daily_report.sh'\n\n\n\nAll data structures support a .dump() method for easy backups and migration to a JSON file.\nimport json\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"my_app.db\")\nconfig = db.dict(\"app_config\")\nconfig[\"theme\"] = \"dark\"\nconfig[\"user_id\"] = 456\n\n# Dump the dictionary's contents to a JSON file\nwith open(\"config_backup.json\", \"w\") as f:\n    config.dump(f)\n\n# You can also get the dump as a Python object\ndump_data = config.dump()\nprint(dump_data['metadata'])\n\ndb.close()\nYou can also use the CLI to dump data:\nbeaver --database data.db collection my_documents dump &gt; my_documents.json"
  },
  {
    "objectID": "guide-lists.html",
    "href": "guide-lists.html",
    "title": "Lists",
    "section": "",
    "text": "The ListManager provides a persistent, ordered collection of items that behaves like a standard Python list.\nUnlike simpler key-value stores that might require rewriting a whole array to add an item, BeaverDB lists are implemented using Fractional Indexing. This allows for efficient O(1) insertions at the beginning, end, or anywhere in the middle of the list without shifting other elements.\n\n\nInitialize a list using db.list().\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"app.db\")\ntasks = db.list(\"todo_list\")\n\n# 1. Add items\ntasks.push(\"Buy milk\")\ntasks.prepend(\"Urgent: Fix bug\")\n\n# 2. Access items\nprint(tasks[0])  # -&gt; \"Urgent: Fix bug\"\nprint(tasks[-1]) # -&gt; \"Buy milk\"\n\n# 3. Iterate\nfor task in tasks:\n    print(f\"- {task}\")\n\n\n\n\n\nYou can add items to the end, the beginning, or any specific position.\n# Add to the end (Append)\ntasks.push(\"Walk the dog\")\n\n# Add to the beginning\ntasks.prepend(\"Morning Meeting\")\n\n# Insert at a specific index\n# This is efficient and does not require rewriting adjacent rows.\ntasks.insert(1, \"Check emails\")\n\n\n\nAccess items by their integer index. Negative indexing is supported.\nfirst = tasks[0]\nlast = tasks[-1]\n\n# Slicing (Returns a generator/iterator to save memory)\n# Note: This queries the DB range, it's very fast.\nfor item in tasks[1:3]:\n    print(item)\n\n\n\nYou can remove items by index (pop) or by value (remove).\n# Remove and return the last item\nitem = tasks.pop()\n\n# Remove and return item at index 0\nfirst_item = tasks.pop(0)\n\n# Remove the first occurrence of a value\ntasks.remove(\"Check emails\")\n\n# Clear the entire list\ntasks.clear()\n\n\n\nStandard Python operators work as expected.\nif \"Buy milk\" in tasks:\n    print(\"Don't forget the milk!\")\n\nprint(f\"Tasks remaining: {len(tasks)}\")\n\n\n\n\n\n\nWhen adding many items at once (e.g., importing logs or a feed), use .batched(). This is significantly faster than calling .push() in a loop because it calculates the order keys in memory and performs a single bulk insert.\n\nNote: The batch interface supports .push() and .prepend(). Arbitrary .insert() is not supported in batch mode to keep order calculation efficient.\n\n# Efficiently add 5,000 items\nwith tasks.batched() as batch:\n    for i in range(5000):\n        batch.push(f\"Log entry {i}\")\n\n\n\nBeaverDB lists are process-safe.\n\nOrder Stability: Because items use floating-point keys for ordering, inserting an item between index 4 and 5 creates an item with order 4.5. This means indices are stable; inserting at index 0 essentially “shifts” logical indices but doesn’t require rewriting rows."
  },
  {
    "objectID": "guide-lists.html#quick-start",
    "href": "guide-lists.html#quick-start",
    "title": "Lists",
    "section": "",
    "text": "Initialize a list using db.list().\nfrom beaver import BeaverDB\n\ndb = BeaverDB(\"app.db\")\ntasks = db.list(\"todo_list\")\n\n# 1. Add items\ntasks.push(\"Buy milk\")\ntasks.prepend(\"Urgent: Fix bug\")\n\n# 2. Access items\nprint(tasks[0])  # -&gt; \"Urgent: Fix bug\"\nprint(tasks[-1]) # -&gt; \"Buy milk\"\n\n# 3. Iterate\nfor task in tasks:\n    print(f\"- {task}\")"
  },
  {
    "objectID": "guide-lists.html#basic-operations",
    "href": "guide-lists.html#basic-operations",
    "title": "Lists",
    "section": "",
    "text": "You can add items to the end, the beginning, or any specific position.\n# Add to the end (Append)\ntasks.push(\"Walk the dog\")\n\n# Add to the beginning\ntasks.prepend(\"Morning Meeting\")\n\n# Insert at a specific index\n# This is efficient and does not require rewriting adjacent rows.\ntasks.insert(1, \"Check emails\")\n\n\n\nAccess items by their integer index. Negative indexing is supported.\nfirst = tasks[0]\nlast = tasks[-1]\n\n# Slicing (Returns a generator/iterator to save memory)\n# Note: This queries the DB range, it's very fast.\nfor item in tasks[1:3]:\n    print(item)\n\n\n\nYou can remove items by index (pop) or by value (remove).\n# Remove and return the last item\nitem = tasks.pop()\n\n# Remove and return item at index 0\nfirst_item = tasks.pop(0)\n\n# Remove the first occurrence of a value\ntasks.remove(\"Check emails\")\n\n# Clear the entire list\ntasks.clear()\n\n\n\nStandard Python operators work as expected.\nif \"Buy milk\" in tasks:\n    print(\"Don't forget the milk!\")\n\nprint(f\"Tasks remaining: {len(tasks)}\")"
  },
  {
    "objectID": "guide-lists.html#advanced-features",
    "href": "guide-lists.html#advanced-features",
    "title": "Lists",
    "section": "",
    "text": "When adding many items at once (e.g., importing logs or a feed), use .batched(). This is significantly faster than calling .push() in a loop because it calculates the order keys in memory and performs a single bulk insert.\n\nNote: The batch interface supports .push() and .prepend(). Arbitrary .insert() is not supported in batch mode to keep order calculation efficient.\n\n# Efficiently add 5,000 items\nwith tasks.batched() as batch:\n    for i in range(5000):\n        batch.push(f\"Log entry {i}\")\n\n\n\nBeaverDB lists are process-safe.\n\nOrder Stability: Because items use floating-point keys for ordering, inserting an item between index 4 and 5 creates an item with order 4.5. This means indices are stable; inserting at index 0 essentially “shifts” logical indices but doesn’t require rewriting rows."
  },
  {
    "objectID": "dev-concurrency.html",
    "href": "dev-concurrency.html",
    "title": "Concurrency Model",
    "section": "",
    "text": "BeaverDB is designed to be Process-Safe and Thread-Safe.\n\nProcess-Safe: Multiple Python processes (workers, scripts) can access the same .db file simultaneously.\nThread-Safe: Multiple threads within a single process can share the BeaverDB instance.\n\nThis capability relies on two pillars: SQLite’s WAL Mode for data safety, and a custom Lock Manager for logical coordination.\n\n\nBy default, BeaverDB enables Write-Ahead Logging (WAL) mode (PRAGMA journal_mode=WAL).\n\n\nIn standard SQLite (DELETE mode), a write operation locks the entire file, blocking all readers. This kills concurrency.\nIn WAL mode: 1. Writers append changes to a separate -wal file. 2. Readers read from the main .db file + the -wal file. 3. Result: Readers do not block writers, and writers do not block readers.\nThis allows BeaverDB to handle high-throughput scenarios (e.g., a logger writing events while an API server queries them) without lock contention errors.\n\n\n\n\nSQLite connections cannot be shared across threads. If two threads try to write to the same sqlite3.Connection object, the application will crash or corrupt memory.\nBeaverDB manages this automatically using Thread-Local Storage.\n# Internal Logic\nclass BeaverDB:\n    def __init__(self):\n        self._thread_local = threading.local()\n\n    @property\n    def connection(self):\n        # If this thread has never asked for a connection, open one.\n        if not hasattr(self._thread_local, \"conn\"):\n            self._thread_local.conn = sqlite3.connect(...)\n\n        return self._thread_local.conn\n\nWhen Thread A calls db.dict(\"x\"), it gets a connection dedicated to Thread A.\nWhen Thread B calls db.dict(\"x\"), it gets a different connection dedicated to Thread B.\nBoth connections talk to the same file, mediated by SQLite’s file locking.\n\n\n\n\nSQLite handles data safety, but it doesn’t handle logical safety.\nThe Problem: Imagine two worker processes running a “Daily Cleanup” job. SQLite ensures they don’t corrupt the file, but it doesn’t stop them from running the cleanup logic twice, wasting CPU or sending duplicate emails.\nThe Solution: BeaverDB implements a custom Inter-Process Lock stored in the database itself (beaver_lock_waiters table).\n\n\nThe LockManager implements a Fair, Deadlock-Proof, FIFO Mutex.\n\nRequest: A process inserts a row into beaver_lock_waiters with a timestamp and a unique waiter_id.\nQueue: The table acts as a queue. The lock is “acquired” only if the process’s row is the oldest active row for that lock name.\nPoll: If not at the front, the process sleeps (poll_interval) and checks again.\nSafety (TTL): Every lock has a expires_at timestamp. If a process crashes while holding the lock, other waiters will eventually see the expired row and delete it (“steal” the lock), preventing deadlocks.\n\n-- Simplified Schema\nCREATE TABLE beaver_lock_waiters (\n    lock_name TEXT,\n    waiter_id TEXT,\n    requested_at REAL, -- Used for FIFO ordering\n    expires_at REAL    -- Used for Deadlock protection\n);\n\n\n\n\nBeaverDB follows a “Synchronous Core, Async Wrapper” architecture.\n\nThe Core: All logic (DictManager, LogManager) is written in standard synchronous Python using sqlite3. This ensures simplicity and stability.\nThe Async Layer: Classes like AsyncDictManager or AsyncLogManager wrap the synchronous methods using asyncio.to_thread().\n\n\n\nStandard sqlite3 is blocking. Even if you wrap it in async def, a query blocks the event loop. True non-blocking SQLite requires running the query in a separate thread.\nBeaverDB automates this. When you call await db.log(\"x\").as_async().log(...), the operation is offloaded to a thread pool, keeping your asyncio loop responsive."
  },
  {
    "objectID": "dev-concurrency.html#the-sqlite-foundation-wal-mode",
    "href": "dev-concurrency.html#the-sqlite-foundation-wal-mode",
    "title": "Concurrency Model",
    "section": "",
    "text": "By default, BeaverDB enables Write-Ahead Logging (WAL) mode (PRAGMA journal_mode=WAL).\n\n\nIn standard SQLite (DELETE mode), a write operation locks the entire file, blocking all readers. This kills concurrency.\nIn WAL mode: 1. Writers append changes to a separate -wal file. 2. Readers read from the main .db file + the -wal file. 3. Result: Readers do not block writers, and writers do not block readers.\nThis allows BeaverDB to handle high-throughput scenarios (e.g., a logger writing events while an API server queries them) without lock contention errors."
  },
  {
    "objectID": "dev-concurrency.html#thread-safety-threading.local",
    "href": "dev-concurrency.html#thread-safety-threading.local",
    "title": "Concurrency Model",
    "section": "",
    "text": "SQLite connections cannot be shared across threads. If two threads try to write to the same sqlite3.Connection object, the application will crash or corrupt memory.\nBeaverDB manages this automatically using Thread-Local Storage.\n# Internal Logic\nclass BeaverDB:\n    def __init__(self):\n        self._thread_local = threading.local()\n\n    @property\n    def connection(self):\n        # If this thread has never asked for a connection, open one.\n        if not hasattr(self._thread_local, \"conn\"):\n            self._thread_local.conn = sqlite3.connect(...)\n\n        return self._thread_local.conn\n\nWhen Thread A calls db.dict(\"x\"), it gets a connection dedicated to Thread A.\nWhen Thread B calls db.dict(\"x\"), it gets a different connection dedicated to Thread B.\nBoth connections talk to the same file, mediated by SQLite’s file locking."
  },
  {
    "objectID": "dev-concurrency.html#logical-concurrency-lockmanager",
    "href": "dev-concurrency.html#logical-concurrency-lockmanager",
    "title": "Concurrency Model",
    "section": "",
    "text": "SQLite handles data safety, but it doesn’t handle logical safety.\nThe Problem: Imagine two worker processes running a “Daily Cleanup” job. SQLite ensures they don’t corrupt the file, but it doesn’t stop them from running the cleanup logic twice, wasting CPU or sending duplicate emails.\nThe Solution: BeaverDB implements a custom Inter-Process Lock stored in the database itself (beaver_lock_waiters table).\n\n\nThe LockManager implements a Fair, Deadlock-Proof, FIFO Mutex.\n\nRequest: A process inserts a row into beaver_lock_waiters with a timestamp and a unique waiter_id.\nQueue: The table acts as a queue. The lock is “acquired” only if the process’s row is the oldest active row for that lock name.\nPoll: If not at the front, the process sleeps (poll_interval) and checks again.\nSafety (TTL): Every lock has a expires_at timestamp. If a process crashes while holding the lock, other waiters will eventually see the expired row and delete it (“steal” the lock), preventing deadlocks.\n\n-- Simplified Schema\nCREATE TABLE beaver_lock_waiters (\n    lock_name TEXT,\n    waiter_id TEXT,\n    requested_at REAL, -- Used for FIFO ordering\n    expires_at REAL    -- Used for Deadlock protection\n);"
  },
  {
    "objectID": "dev-concurrency.html#async-architecture",
    "href": "dev-concurrency.html#async-architecture",
    "title": "Concurrency Model",
    "section": "",
    "text": "BeaverDB follows a “Synchronous Core, Async Wrapper” architecture.\n\nThe Core: All logic (DictManager, LogManager) is written in standard synchronous Python using sqlite3. This ensures simplicity and stability.\nThe Async Layer: Classes like AsyncDictManager or AsyncLogManager wrap the synchronous methods using asyncio.to_thread().\n\n\n\nStandard sqlite3 is blocking. Even if you wrap it in async def, a query blocks the event loop. True non-blocking SQLite requires running the query in a separate thread.\nBeaverDB automates this. When you call await db.log(\"x\").as_async().log(...), the operation is offloaded to a thread pool, keeping your asyncio loop responsive."
  },
  {
    "objectID": "dev-architecture.html",
    "href": "dev-architecture.html",
    "title": "Core Architecture",
    "section": "",
    "text": "BeaverDB is built on a simple but powerful premise: One File, Multiple Modalities.\nInstead of being a simple key-value store or a pure vector database, it acts as a “meta-database” that manages multiple specialized data structures (Dictionaries, Lists, Logs, Vectors) within a single SQLite file.\n\n\nThe entry point for the entire system is the BeaverDB class in beaver.core. It acts as a Facade and a Resource Manager.\n\nFacade: It provides factory methods (.dict(), .list(), .collection()) that hide the complexity of initializing specific managers.\nResource Manager: It holds the persistent SQLite connection pool and manages thread safety.\n\n\n\nBeaverDB does not implement data logic itself. Instead, it delegates to Managers.\nWhen you call db.dict(\"settings\"), BeaverDB: 1. Checks its internal _manager_cache to see if a manager for “settings” already exists. 2. If not, it initializes a DictManager(name=\"settings\", db=self). 3. Returns the singleton instance.\nThis ensures that multiple calls to db.dict(\"settings\") in the same process return the same python object, sharing locks and caches.\n\n\n\n\nBeaverDB uses a fixed set of internal tables to store user data. These tables are created automatically on initialization.\n\n\n\n\n\n\n\n\n\n\nTable\nPurpose\nSchema\n\n\n\n\nbeaver_dicts\nStores all dictionaries\ndict_name, key, value, expires_at\n\n\nbeaver_lists\nStores all lists\nlist_name, item_order, item_value\n\n\nbeaver_blobs\nStores binary files\nstore_name, key, data, metadata\n\n\nbeaver_sketches\nProbabilistic structures\nname, type, config, data (BLOB)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\nPurpose\nSchema\n\n\n\n\nbeaver_collections\nMain document storage\ncollection, item_id, item_vector, metadata\n\n\nbeaver_fts_index\nVirtual table for Full-Text Search\n(FTS5 Virtual Table)\n\n\nbeaver_trigrams\nN-grams for fuzzy matching\ncollection, trigram, item_id\n\n\nbeaver_edges\nGraph connections\ncollection, source, target, label, weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\nPurpose\nSchema\n\n\n\n\nbeaver_logs\nTime-series logs\nlog_name, timestamp, data\n\n\nbeaver_priority_queues\nJob queues\nqueue_name, priority, timestamp, data\n\n\nbeaver_pubsub_log\nMessage bus history\nchannel_name, timestamp, message_payload\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\nPurpose\nSchema\n\n\n\n\nbeaver_lock_waiters\nInter-process locks\nlock_name, waiter_id, expires_at"
  },
  {
    "objectID": "dev-architecture.html#the-facade-pattern",
    "href": "dev-architecture.html#the-facade-pattern",
    "title": "Core Architecture",
    "section": "",
    "text": "The entry point for the entire system is the BeaverDB class in beaver.core. It acts as a Facade and a Resource Manager.\n\nFacade: It provides factory methods (.dict(), .list(), .collection()) that hide the complexity of initializing specific managers.\nResource Manager: It holds the persistent SQLite connection pool and manages thread safety.\n\n\n\nBeaverDB does not implement data logic itself. Instead, it delegates to Managers.\nWhen you call db.dict(\"settings\"), BeaverDB: 1. Checks its internal _manager_cache to see if a manager for “settings” already exists. 2. If not, it initializes a DictManager(name=\"settings\", db=self). 3. Returns the singleton instance.\nThis ensures that multiple calls to db.dict(\"settings\") in the same process return the same python object, sharing locks and caches."
  },
  {
    "objectID": "dev-architecture.html#schema-design",
    "href": "dev-architecture.html#schema-design",
    "title": "Core Architecture",
    "section": "",
    "text": "BeaverDB uses a fixed set of internal tables to store user data. These tables are created automatically on initialization.\n\n\n\n\n\n\n\n\n\n\nTable\nPurpose\nSchema\n\n\n\n\nbeaver_dicts\nStores all dictionaries\ndict_name, key, value, expires_at\n\n\nbeaver_lists\nStores all lists\nlist_name, item_order, item_value\n\n\nbeaver_blobs\nStores binary files\nstore_name, key, data, metadata\n\n\nbeaver_sketches\nProbabilistic structures\nname, type, config, data (BLOB)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\nPurpose\nSchema\n\n\n\n\nbeaver_collections\nMain document storage\ncollection, item_id, item_vector, metadata\n\n\nbeaver_fts_index\nVirtual table for Full-Text Search\n(FTS5 Virtual Table)\n\n\nbeaver_trigrams\nN-grams for fuzzy matching\ncollection, trigram, item_id\n\n\nbeaver_edges\nGraph connections\ncollection, source, target, label, weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\nPurpose\nSchema\n\n\n\n\nbeaver_logs\nTime-series logs\nlog_name, timestamp, data\n\n\nbeaver_priority_queues\nJob queues\nqueue_name, priority, timestamp, data\n\n\nbeaver_pubsub_log\nMessage bus history\nchannel_name, timestamp, message_payload\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\nPurpose\nSchema\n\n\n\n\nbeaver_lock_waiters\nInter-process locks\nlock_name, waiter_id, expires_at"
  },
  {
    "objectID": "guide-collections.html",
    "href": "guide-collections.html",
    "title": "Collections (Multi-Modal Search)",
    "section": "",
    "text": "The CollectionManager is the powerhouse of BeaverDB. It provides a unified interface for storing Documents and performing Vector Search, Full-Text Search (FTS), Fuzzy Matching, and Graph Traversal on them.\nThis is ideal for building RAG (Retrieval Augmented Generation) applications, search engines, knowledge graphs, or recommendation systems.\n\n\nA Collection stores Document objects. Each document has a unique id, an optional vector embedding, and a JSON-serializable body.\nfrom beaver import BeaverDB\nfrom beaver.collections import Document\n\ndb = BeaverDB(\"app.db\")\ndocs = db.collection(\"articles\")\n\n# 1. Index a Document (with embedding for semantic search)\ndoc = Document(\n    id=\"doc_1\",\n    body={\"title\": \"Introduction to AI\", \"content\": \"AI is changing the world...\"},\n    embedding=[0.1, 0.2, 0.8, ...] # 768-dim vector\n)\ndocs.index(doc)\n\n# 2. Semantic Search (Vector)\n# Finds documents semantically similar to the query vector\nresults = docs.search(query_vector, top_k=5)\n\n# 3. Keyword Search (FTS)\n# Finds documents containing specific words\nmatches = docs.match(\"artificial intelligence\")\n\n\n\n\n\nBeaverDB uses a strict Pydantic model for items.\n\nid: String (UUID by default).\nembedding: List of floats (or None).\nbody: Dictionary or Pydantic model (Metadata).\n\n\nfrom pydantic import BaseModel\n\nclass Article(BaseModel):\n    title: str\n    tags: list[str]\n\n# Typed Collection\narticles = db.collection(\"news\", model=Article)\n\n# Indexing\narticles.index(Document(body=Article(title=\"New Release\", tags=[\"tech\"])))\n\n\n\nThe .index() method performs an atomic upsert. It updates the main storage, vector index, FTS index, and fuzzy n-gram index simultaneously.\n# Index with specific FTS fields\ndocs.index(doc, fts=[\"title\", \"content\"])\n\n# Enable Fuzzy Search support (slower indexing, robust matching)\ndocs.index(doc, fts=[\"title\"], fuzzy=True)\n\n\n\n# Remove a single document\ndocs.drop(doc)\n\n# Wipe the entire collection (Vectors, Graph, FTS)\ndocs.clear()\n\n\n\n\n\n\nPerforms an Approximate Nearest Neighbor (ANN) search using Cosine Similarity.\n\nSpeed: Extremely fast (in-memory index).\nPersistence: The index is fully persisted to disk and crash-safe.\n\n\n# Returns list of (Document, score) tuples\nresults = docs.search(embedding_vector, top_k=10)\n\nfor doc, score in results:\n    print(f\"{score:.4f}: {doc.body['title']}\")\n\n\n\nUses SQLite’s FTS5 engine for powerful keyword matching. Supports boolean operators (AND, OR, NOT) and prefix matching.\n# Simple match\ndocs.match(\"python database\")\n\n# Specific fields\ndocs.match(\"tutorial\", on=[\"title\"])\n\n# Boolean query\ndocs.match(\"python AND NOT java\")\n\n\n\nIf you indexed with fuzzy=True, you can find documents even with typos. BeaverDB uses a hybrid Trigram + Levenshtein approach for high performance.\n# Finds \"BeaverDB\" even if user types \"BaverDB\"\ndocs.match(\"BaverDB\", fuzziness=1)\n\n\n\nTo get the best of both worlds (semantic understanding + keyword precision), use Reverse Rank Fusion (RRF). BeaverDB provides a built-in rerank helper.\nfrom beaver.collections import rerank\n\n# 1. Run searches in parallel\nvector_hits = [doc for doc, _ in docs.search(vec, top_k=50)]\nkeyword_hits = [doc for doc, _ in docs.match(\"query\", top_k=50)]\n\n# 2. Fuse results\n# Scores are normalized based on rank position\nfinal_results = rerank(vector_hits, keyword_hits, k=60)\n\n\n\n\nBeaverDB is also a Graph Database. You can link documents together and traverse the connections to find related context (e.g., for GraphRAG).\n\n\nCreate directed, labeled edges between documents.\n# d1 -&gt; d2 (d1 references d2)\ndocs.connect(d1, d2, label=\"references\")\n\n# d2 -&gt; d3 (d2 is the parent of d3)\ndocs.connect(d2, d3, label=\"parent\")\n\n\n\nRetrieve connected documents efficiently.\n# Get immediate neighbors\nrefs = docs.neighbors(d1, label=\"references\")\n\n# Multi-hop Traversal (BFS)\n# \"Find everything d1 connects to, up to 2 hops away\"\n# This runs a Recursive CTE in SQLite (Very Fast)\ncontext = docs.walk(d1, labels=[\"references\", \"parent\"], depth=2)\n\n\n\n\n\n\nThe vector index uses a Hybrid Architecture (Base Snapshot + Delta Log). Over time, the delta log grows.\n\nAuto-Compaction: Triggered automatically when the log gets too large.\nManual Compaction: You can force a merge of the log into the main index.\n\ndocs.compact()\n\n\n\nDump the entire collection (vectors and metadata) to JSON for backup.\ndata = docs.dump()\n# OR write directly to file\nwith open(\"backup.json\", \"w\") as f:\n    docs.dump(f)"
  },
  {
    "objectID": "guide-collections.html#quick-start",
    "href": "guide-collections.html#quick-start",
    "title": "Collections (Multi-Modal Search)",
    "section": "",
    "text": "A Collection stores Document objects. Each document has a unique id, an optional vector embedding, and a JSON-serializable body.\nfrom beaver import BeaverDB\nfrom beaver.collections import Document\n\ndb = BeaverDB(\"app.db\")\ndocs = db.collection(\"articles\")\n\n# 1. Index a Document (with embedding for semantic search)\ndoc = Document(\n    id=\"doc_1\",\n    body={\"title\": \"Introduction to AI\", \"content\": \"AI is changing the world...\"},\n    embedding=[0.1, 0.2, 0.8, ...] # 768-dim vector\n)\ndocs.index(doc)\n\n# 2. Semantic Search (Vector)\n# Finds documents semantically similar to the query vector\nresults = docs.search(query_vector, top_k=5)\n\n# 3. Keyword Search (FTS)\n# Finds documents containing specific words\nmatches = docs.match(\"artificial intelligence\")"
  },
  {
    "objectID": "guide-collections.html#managing-documents",
    "href": "guide-collections.html#managing-documents",
    "title": "Collections (Multi-Modal Search)",
    "section": "",
    "text": "BeaverDB uses a strict Pydantic model for items.\n\nid: String (UUID by default).\nembedding: List of floats (or None).\nbody: Dictionary or Pydantic model (Metadata).\n\n\nfrom pydantic import BaseModel\n\nclass Article(BaseModel):\n    title: str\n    tags: list[str]\n\n# Typed Collection\narticles = db.collection(\"news\", model=Article)\n\n# Indexing\narticles.index(Document(body=Article(title=\"New Release\", tags=[\"tech\"])))\n\n\n\nThe .index() method performs an atomic upsert. It updates the main storage, vector index, FTS index, and fuzzy n-gram index simultaneously.\n# Index with specific FTS fields\ndocs.index(doc, fts=[\"title\", \"content\"])\n\n# Enable Fuzzy Search support (slower indexing, robust matching)\ndocs.index(doc, fts=[\"title\"], fuzzy=True)\n\n\n\n# Remove a single document\ndocs.drop(doc)\n\n# Wipe the entire collection (Vectors, Graph, FTS)\ndocs.clear()"
  },
  {
    "objectID": "guide-collections.html#search-capabilities",
    "href": "guide-collections.html#search-capabilities",
    "title": "Collections (Multi-Modal Search)",
    "section": "",
    "text": "Performs an Approximate Nearest Neighbor (ANN) search using Cosine Similarity.\n\nSpeed: Extremely fast (in-memory index).\nPersistence: The index is fully persisted to disk and crash-safe.\n\n\n# Returns list of (Document, score) tuples\nresults = docs.search(embedding_vector, top_k=10)\n\nfor doc, score in results:\n    print(f\"{score:.4f}: {doc.body['title']}\")\n\n\n\nUses SQLite’s FTS5 engine for powerful keyword matching. Supports boolean operators (AND, OR, NOT) and prefix matching.\n# Simple match\ndocs.match(\"python database\")\n\n# Specific fields\ndocs.match(\"tutorial\", on=[\"title\"])\n\n# Boolean query\ndocs.match(\"python AND NOT java\")\n\n\n\nIf you indexed with fuzzy=True, you can find documents even with typos. BeaverDB uses a hybrid Trigram + Levenshtein approach for high performance.\n# Finds \"BeaverDB\" even if user types \"BaverDB\"\ndocs.match(\"BaverDB\", fuzziness=1)\n\n\n\nTo get the best of both worlds (semantic understanding + keyword precision), use Reverse Rank Fusion (RRF). BeaverDB provides a built-in rerank helper.\nfrom beaver.collections import rerank\n\n# 1. Run searches in parallel\nvector_hits = [doc for doc, _ in docs.search(vec, top_k=50)]\nkeyword_hits = [doc for doc, _ in docs.match(\"query\", top_k=50)]\n\n# 2. Fuse results\n# Scores are normalized based on rank position\nfinal_results = rerank(vector_hits, keyword_hits, k=60)"
  },
  {
    "objectID": "guide-collections.html#graph-relationships",
    "href": "guide-collections.html#graph-relationships",
    "title": "Collections (Multi-Modal Search)",
    "section": "",
    "text": "BeaverDB is also a Graph Database. You can link documents together and traverse the connections to find related context (e.g., for GraphRAG).\n\n\nCreate directed, labeled edges between documents.\n# d1 -&gt; d2 (d1 references d2)\ndocs.connect(d1, d2, label=\"references\")\n\n# d2 -&gt; d3 (d2 is the parent of d3)\ndocs.connect(d2, d3, label=\"parent\")\n\n\n\nRetrieve connected documents efficiently.\n# Get immediate neighbors\nrefs = docs.neighbors(d1, label=\"references\")\n\n# Multi-hop Traversal (BFS)\n# \"Find everything d1 connects to, up to 2 hops away\"\n# This runs a Recursive CTE in SQLite (Very Fast)\ncontext = docs.walk(d1, labels=[\"references\", \"parent\"], depth=2)"
  },
  {
    "objectID": "guide-collections.html#maintenance",
    "href": "guide-collections.html#maintenance",
    "title": "Collections (Multi-Modal Search)",
    "section": "",
    "text": "The vector index uses a Hybrid Architecture (Base Snapshot + Delta Log). Over time, the delta log grows.\n\nAuto-Compaction: Triggered automatically when the log gets too large.\nManual Compaction: You can force a merge of the log into the main index.\n\ndocs.compact()\n\n\n\nDump the entire collection (vectors and metadata) to JSON for backup.\ndata = docs.dump()\n# OR write directly to file\nwith open(\"backup.json\", \"w\") as f:\n    docs.dump(f)"
  },
  {
    "objectID": "guide-logs.html",
    "href": "guide-logs.html",
    "title": "Logs (Time-Series & Stream)",
    "section": "",
    "text": "The LogManager is a specialized data structure for Time-Series Data. Unlike a dictionary (where keys are unique) or a list (ordered by insertion), a Log is ordered by Timestamp.\nThis is ideal for:\n\nSystem Telemetry: CPU usage, memory stats, sensor readings.\nAudit Trails: “User X changed setting Y at Time Z”.\nChat History: Storing messages chronologically.\nEvent Sourcing: Storing a sequence of state-changing events.\n\n\n\nInitialize a log using db.log().\nfrom beaver import BeaverDB\nfrom datetime import datetime, timedelta\n\ndb = BeaverDB(\"telemetry.db\")\nmetrics = db.log(\"server_metrics\")\n\n# 1. Log Events\n# BeaverDB automatically records the current UTC timestamp.\nmetrics.log({\"cpu\": 12.5, \"ram\": 64})\nmetrics.log({\"cpu\": 45.0, \"ram\": 70})\n\n# 2. Log with explicit timestamp\n# You can backfill data by providing a datetime object.\nyesterday = datetime.now() - timedelta(days=1)\nmetrics.log({\"cpu\": 10.0, \"ram\": 60}, timestamp=yesterday)\n\n# 3. Iterate (Chronological Order)\nfor timestamp, data in metrics:\n    print(f\"[{timestamp}] CPU: {data['cpu']}%\")\n\n\n\n\n\nThe most common operation on logs is fetching data for a specific window (e.g., “Show me errors from the last hour”).\nnow = datetime.now()\none_hour_ago = now - timedelta(hours=1)\n\n# Efficiently queries the index for this range\nrecent_logs = metrics.range(start=one_hour_ago, end=now)\n\nprint(f\"Found {len(recent_logs)} entries in the last hour.\")\n\n\n\nYou can iterate over the entire history of the log.\nfor ts, entry in metrics:\n    process(entry)\n\n\n\n\nThe “Killer Feature” of the LogManager is the .live() iterator. It allows you to build Real-Time Dashboards or Monitoring Alerts without writing complex polling loops or maintaining state.\nIt works by maintaining a Rolling Window over the most recent data and applying an Aggregator Function to it periodically.\n\n\n\nWindow: “Look at the last 5 minutes of data.”\nPeriod: “Update the result every 1 second.”\nAggregator: “Calculate the average CPU usage.”\n\n\nimport statistics\n\n# Define how to summarize the window\ndef calculate_avg_cpu(entries: list[dict]) -&gt; float:\n    if not entries:\n        return 0.0\n    values = [e[\"cpu\"] for e in entries]\n    return statistics.mean(values)\n\n# Create the live view\n# This returns an infinite iterator that blocks until the next period\nlive_view = metrics.live(\n    window=timedelta(minutes=5),   # 5-minute rolling window\n    period=timedelta(seconds=1),   # Yield a new result every second\n    aggregator=calculate_avg_cpu\n)\n\nprint(\"Starting Live Dashboard...\")\nfor avg_cpu in live_view:\n    print(f\"Live CPU (5min avg): {avg_cpu:.2f}%\")\n    # This loop runs forever. Press Ctrl+C to stop.\n\n\n\nFor modern web applications (FastAPI, etc.), you can use the async version to stream updates via WebSockets/SSE.\nasync_logs = db.log(\"metrics\").as_async()\n\nasync for avg_cpu in async_logs.live(..., aggregator=calculate_avg_cpu):\n    await websocket.send_json({\"cpu\": avg_cpu})\n\n\n\n\n\n\nJust like other managers, use .batched() when ingesting high-frequency logs (e.g., 1000 requests/sec) to prevent database lock contention.\n# Ingest a batch of 1,000 sensor readings\nwith metrics.batched() as batch:\n    for reading in sensor_stream:\n        batch.log(reading)\n\n\n\nTo rotate logs or wipe history:\nmetrics.clear()"
  },
  {
    "objectID": "guide-logs.html#quick-start",
    "href": "guide-logs.html#quick-start",
    "title": "Logs (Time-Series & Stream)",
    "section": "",
    "text": "Initialize a log using db.log().\nfrom beaver import BeaverDB\nfrom datetime import datetime, timedelta\n\ndb = BeaverDB(\"telemetry.db\")\nmetrics = db.log(\"server_metrics\")\n\n# 1. Log Events\n# BeaverDB automatically records the current UTC timestamp.\nmetrics.log({\"cpu\": 12.5, \"ram\": 64})\nmetrics.log({\"cpu\": 45.0, \"ram\": 70})\n\n# 2. Log with explicit timestamp\n# You can backfill data by providing a datetime object.\nyesterday = datetime.now() - timedelta(days=1)\nmetrics.log({\"cpu\": 10.0, \"ram\": 60}, timestamp=yesterday)\n\n# 3. Iterate (Chronological Order)\nfor timestamp, data in metrics:\n    print(f\"[{timestamp}] CPU: {data['cpu']}%\")"
  },
  {
    "objectID": "guide-logs.html#reading-data",
    "href": "guide-logs.html#reading-data",
    "title": "Logs (Time-Series & Stream)",
    "section": "",
    "text": "The most common operation on logs is fetching data for a specific window (e.g., “Show me errors from the last hour”).\nnow = datetime.now()\none_hour_ago = now - timedelta(hours=1)\n\n# Efficiently queries the index for this range\nrecent_logs = metrics.range(start=one_hour_ago, end=now)\n\nprint(f\"Found {len(recent_logs)} entries in the last hour.\")\n\n\n\nYou can iterate over the entire history of the log.\nfor ts, entry in metrics:\n    process(entry)"
  },
  {
    "objectID": "guide-logs.html#real-time-analysis-live-views",
    "href": "guide-logs.html#real-time-analysis-live-views",
    "title": "Logs (Time-Series & Stream)",
    "section": "",
    "text": "The “Killer Feature” of the LogManager is the .live() iterator. It allows you to build Real-Time Dashboards or Monitoring Alerts without writing complex polling loops or maintaining state.\nIt works by maintaining a Rolling Window over the most recent data and applying an Aggregator Function to it periodically.\n\n\n\nWindow: “Look at the last 5 minutes of data.”\nPeriod: “Update the result every 1 second.”\nAggregator: “Calculate the average CPU usage.”\n\n\nimport statistics\n\n# Define how to summarize the window\ndef calculate_avg_cpu(entries: list[dict]) -&gt; float:\n    if not entries:\n        return 0.0\n    values = [e[\"cpu\"] for e in entries]\n    return statistics.mean(values)\n\n# Create the live view\n# This returns an infinite iterator that blocks until the next period\nlive_view = metrics.live(\n    window=timedelta(minutes=5),   # 5-minute rolling window\n    period=timedelta(seconds=1),   # Yield a new result every second\n    aggregator=calculate_avg_cpu\n)\n\nprint(\"Starting Live Dashboard...\")\nfor avg_cpu in live_view:\n    print(f\"Live CPU (5min avg): {avg_cpu:.2f}%\")\n    # This loop runs forever. Press Ctrl+C to stop.\n\n\n\nFor modern web applications (FastAPI, etc.), you can use the async version to stream updates via WebSockets/SSE.\nasync_logs = db.log(\"metrics\").as_async()\n\nasync for avg_cpu in async_logs.live(..., aggregator=calculate_avg_cpu):\n    await websocket.send_json({\"cpu\": avg_cpu})"
  },
  {
    "objectID": "guide-logs.html#maintenance",
    "href": "guide-logs.html#maintenance",
    "title": "Logs (Time-Series & Stream)",
    "section": "",
    "text": "Just like other managers, use .batched() when ingesting high-frequency logs (e.g., 1000 requests/sec) to prevent database lock contention.\n# Ingest a batch of 1,000 sensor readings\nwith metrics.batched() as batch:\n    for reading in sensor_stream:\n        batch.log(reading)\n\n\n\nTo rotate logs or wipe history:\nmetrics.clear()"
  }
]